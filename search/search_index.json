{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lambda Feedback!","text":"<p>A place to study, supported by automated feedback.</p> <p></p> Students: Teachers: <ul><li>Accessible content - in the browser and on PDF. </li><li>Feedback - express ideas naturally and get instant feedback, thousands of times per year. </li><li>Analytics - track your progress, manage your time.</li></ul> <ul><li>Curate content - edit and publish your content in one place.</li><li>Feedback - configure automation to give students timely support.</li><li>Analytics - data driven feedback to students, and teaching organisation.</li></ul> <p> Above: a screenshot from the web-app showing feedback on handwritten mathematics.</p> <p>Lambda Feedback is a project at Imperial College running since 2021. For more info see our list of publications.</p>"},{"location":"#calling-all-teachers","title":"Calling all teachers!","text":"<p>We are recruiting participants to collaborate with us on a pilot. Register your interest here.</p>"},{"location":"#overview-of-documentation","title":"Overview of documentation:","text":"<p>In these docs you can find:</p> <p><li> Student guide </li><li> Teacher guide </li><li> Advanced teachers. <p>Check out Lambda Feedback by clicking on the button below!</p> <p>Visit Lambda Feedback</p>"},{"location":"opportunities/","title":"Opportunities","text":"<p>We are looking for schools, universities, and tutoring agencies to pilot Lambda Feedback. Please register your interest at https://forms.office.com/e/qA1UndSHrs</p> <p>About the pilot:</p> <ul> <li> <p>Organisations can use the platform (without charge) to deliver self-study exercises to their students.</p> </li> <li> <p>All the features of the platform will be available, including curating content, automated feedback, and data analytics</p> </li> <li> <p>The team at Imperial will support your deployment</p> </li> <li> <p>We will ask for your feedback and suggestions before, during, and after the pilot</p> </li> <li> <p>We will initiate further collaborations after the pilot</p> </li> </ul> <p>Key points to consider:</p> <ul> <li> <p>The government has advised schools to use automation for low stakes feedback</p> </li> <li> <p>Lambda Feedback is automated formative feedback, perfect for this application</p> </li> <li> <p>We are a not-for-profit project funded by the Imperial College London Digital Innovation Fund (DiF)</p> </li> <li> <p>All automated feedback is provided by external services that are open source, tested, and can be used on other platforms</p> </li> <li> <p>This is a genuine pilot, and the future of this project will change based on feedback from pioneers who deploy it in practice</p> </li> <li> <p>As this is a pilot, it will require engaged participants who are willing to be adaptive and constructively react to challenges.</p> </li> </ul>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#2025a-formative-feedback-on-engineering-self-study-towards-1-million-times-per-year-per-cohort","title":"2025a: Formative feedback on engineering self-study: Towards 1 million times per year per cohort","text":"<ul> <li> <p> Download PDF or access via IEEE: https://ieeexplore.ieee.org/abstract/document/11016422</p> </li> <li> <p>Conference paper; 3-page summary of the Lambda Feedback system. Recommended for a general overview.</p> </li> </ul>"},{"location":"publications/#2025b-exploring-the-feasibility-of-using-generative-ai-to-emulate-teacher-feedback-for-gcse-english-language-assessments","title":"2025b: Exploring the feasibility of using Generative AI to emulate teacher feedback for GCSE English Language assessments","text":"<ul> <li> <p> Download PDF or access via BERA: https://www.bera.ac.uk/conference/bera-conference-2025/programme</p> </li> <li> <p>Pilot in a school giving students automated feedback on GCSE english essays. Led by the research team at AQA and presented at the BERA 2025 conference.</p> </li> </ul>"},{"location":"publications/#2025c-how-do-we-define-and-evaluate-good-automated-formative-feedback","title":"2025c: How do we define and evaluate \"good\" automated formative feedback?","text":"<ul> <li> <p> Download PDF. A workshop hosted at SEFI 2025.</p> </li> <li> <p>General overview (6 pages) of the literature and thinking behind automated formative feedback, with some community inputs on how we should evaluate automated feedback. Good for educators wanting a general framing of our approach.</p> </li> </ul>"},{"location":"publications/#2025d-ai-microservices-for-sustainable-innovation-in-education","title":"2025d: AI microservices for sustainable innovation in education","text":"<ul> <li> <p> Download PDF pre-print under review. Also available on OSF: https://osf.io/preprints/edarxiv/wq4bd_v3</p> </li> <li> <p>Journal pre-print (~30 pages). Extended manifesto on why the education sector should use microservices for automated judgement in education. Literature review, conceptual arguments on innovation and ethics, and results from deployments of Lambda Feedback. This paper is a long read, providing the detailed thinking behind Lambda Feedback.</p> </li> </ul>"},{"location":"publications/#2025e-chatbots-for-dialogic-feedback-during-self-study-the-importance-of-contextual-information","title":"2025e: Chatbots for Dialogic Feedback during Self-Study: The Importance of Contextual Information","text":"<ul> <li> <p> Download PDF or access via the Engineering Education Research Network (EERN) Conference Proceedings</p> </li> <li> <p>Conference Extended Abstract (8-pages). Using a framework of impasse-driven learning we explore the role of chatbots, and focus on whether the bot has access to context. Empirical results show students benefitting from chatbots accessing context. Valuable thinking behind the benefits and risks of AI in education, combining theory and practice.</p> </li> </ul>"},{"location":"publications/#2024-automated-feedback-on-student-attempts-to-produce-a-set-of-dimensionless-power-products-from-a-set-of-physical-quantities-that-describe-a-physical-problem","title":"2024: Automated Feedback on Student Attempts to Produce a Set of Dimensionless Power Products from a Set of Physical Quantities that Describe a Physical Problem","text":"<ul> <li> <p> Download PDF or access via International Journal for Technology in Mathematics Education</p> </li> <li> <p>Journal paper. Deeply technical paper (8 pages) about a specialist evaluation function for the Buckingham Pi theorem. Only recommended for specialists.</p> </li> </ul>"},{"location":"publications/#blog-articles","title":"Blog articles","text":"<ul> <li>Value proposition: Computers make us human</li> <li>Student experience of self-study: Getting stuck</li> <li>The role of challenge in self-study: Friction in the ideal learning process</li> <li>Configuring online learning: Worked solutions: when and how to use them?</li> </ul>"},{"location":"terminology/","title":"Terminology","text":""},{"location":"terminology/#general-information","title":"General Information","text":"<p>LambdaFeedback is a place to study online. Teachers curate content that students can access. Content is available in the browser and in PDF. Automated feedback on final answers and detailed worked solutions are provided.</p>"},{"location":"terminology/#terminology-used-and-definitions","title":"Terminology Used and Definitions","text":"<p>Here, the fundamental structure and terminology will be laid out.</p>"},{"location":"terminology/#evaluation-functions","title":"Evaluation Functions","text":"<p>An evaluation function is an algorithm that is applied to a response area, whereby the student's response is evaluated and checked. The types of evaluation functions correspond to the types of response areas.</p>"},{"location":"terminology/#final-answer","title":"Final Answer","text":"<p>This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. It serves as a simple container for the final answer, so that the student may compare results.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#modules","title":"Modules","text":"<p>A module is a set programme of taught material. In a university setting, this would correspond to a course module.</p>"},{"location":"terminology/#questions","title":"Questions","text":"<p>A question is a problem within a set, and it may contain any number of parts.</p>"},{"location":"terminology/#response-areas","title":"Response Areas","text":"<p>A response area is an interactive element. Student enters a response and receives feedback. There are different types of response area (text, numerical, array etc.), and these correspond to the different types of information the student is required to input.</p>"},{"location":"terminology/#sets","title":"Sets","text":"<p>We use the word 'Set' to refer to a group of questions content. In a university setting, a Set typically corresponds to an individual homework/tutorial sheet.</p>"},{"location":"terminology/#structured-tutorial","title":"Structured Tutorial","text":"<p>Content providing a structure with which to approach a problem - but not to give the full details away. It is encouraged to be the first piece of guidance for the student before they look at the \"Worked Solution\", or \"Final Answer\".</p> <p>This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#student-learner","title":"Student (learner)","text":"<p>From the perspective of Lambda Feedback, a student is someone who accesses and responds to problem sets. A student only has permissions to view and respond to problem sets (not to edit them).</p>"},{"location":"terminology/#teacher","title":"Teacher","text":"<p>From the perspective of Lambda Feedback, a teacher is someone who creates and manages content. A teacher's account has permissions to create, edit, and delete content within a Module.</p>"},{"location":"terminology/#worked-solution","title":"Worked Solution","text":"<p>The stages of working that lead to a final answer. It may be split into multiple steps which the student can reveal sequentially. This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#workspace","title":"Workspace","text":"<p>On the Question page, the students has access to their own workspace tab. Here they can find the \"Canvas\", for handwriting notes, and the \"Chat\", for conversing with an LLM-driven Chatbot on the question materials.</p>"},{"location":"advanced/","title":"Advanced users","text":""},{"location":"advanced/#microservices","title":"Microservices","text":"<p>The fundamental idea of Lambda Feedback is that it calls external microservices to provide interaction with users. There are two types of microservice: </p> <p>Evaluate a student response and provide feedback:  Evaluation functions - Quickstart Guide</p> <p>LLM-driven chatbots to converse with students: Chat functions - Quickstart guide </p> <p>All microservices are called over http. There is complete freedom in their implementation subject to the expected API schema. Lambda Feedback also provides families of deployed microservices, using open source code available in our public GitHub repositories.</p> <p>This section of documentation is to help developers of microservices. The documentation is written assuming you have basic developer skills.</p> <p>In addition to microservices, this 'Advanced' section caters to administrators of Lambda Feedback tenants; and to developers of interactive response areas, that can be published within our web-stack via discussion with the developers. </p>"},{"location":"advanced/#response-areas","title":"Response areas","text":"<p>Response areas are components in the frontend where student users can enter a response. The response is sent to the evaluation function, which returns feedback to the response area. In the alpha version response areas are built into the software (rather than being modular) so are not straightforward to redevelop. This website catalogues the basic behaviour of response areas, to inform developers of evaluation functions.</p> <p>Response areas - overview</p>"},{"location":"advanced/#administrators","title":"Administrators","text":"<p>Content to be added.</p>"},{"location":"advanced/placeholder/","title":"coming soon","text":""},{"location":"advanced/chat_functions/info/","title":"Chat Functions - More information","text":"<p>Chat functions are the microservices that Lambda Feedback calls to provide the underlying functionality of a chatbot. Students can chat with the chatbots and ask for help or further explanations regarding the Question that they are working on. Each chatbot has its own personality and approach to assisting the students.</p> <p>The chatbots have at their basis a Large Language Model (LLM) which received information regarding:</p> <ul> <li>The raw markdown content of the question the student is on currently, including:<ul> <li>The question name, number and content</li> <li>The final answer, structured tutorial, and worked solutions of the question</li> <li>The guidance (blurb and time estimate) from the teacher for the question</li> <li>The set name, number and description</li> <li>All parts with their number, content and done status (current part emphasised)</li> <li>All response areas and their respective expected answers</li> </ul> </li> <li>The progress of the student on all parts of the Question, including:<ul> <li>The total number of responses and the number of wrong responses the student has made for each response area</li> <li>The last responses the student has made for each response area and the received feedback</li> <li>The time duration the student has spent on the respective question and current part on that day</li> </ul> </li> </ul>"},{"location":"advanced/chat_functions/info/#available-chat-functions","title":"Available Chat functions","text":"<p>Currently the students have access to the following chat functions that host their own specific chatbot. Many others are in development.</p> <p>Click on the links below for information on each chatbot:</p> <p>1. Informational Chatbot</p> <p>2. Concise Chatbot</p> <p>3. Reflective Chatbot</p>"},{"location":"advanced/chat_functions/info/#chat-function-development","title":"Chat Function Development","text":"<p>Are you interested in developing your own chatbot? Then check out the Quickstart guide to develop and deploy your own AI chat function for Lambda Feedback.</p>"},{"location":"advanced/chat_functions/local/","title":"Running and Testing Chat function Locally","text":"<p>You can run the Python function for your chat function itself by writing a <code>main()</code> function, or you can call the <code>testbench_prompts.py</code> script that runs a similar pipeline to the <code>module.py</code>.</p> <pre><code>python src/agents/utils/testbench_prompts.py\n</code></pre> <p>You can also use the <code>test_prompts.py</code> script to test the chat function with example inputs from Lambda Feedback questions and synthetic conversations. <pre><code>python src/agents/utils/test_prompts.py\n</code></pre></p>"},{"location":"advanced/chat_functions/local/#testing-using-the-docker-image","title":"Testing using the Docker Image","text":"<p>You can also build and run the docker pipeline for the chat function. The chatbot associated with the chat function is deployed onto a AWS Lambda serverless cloud function using the docker image. Hence, for final testing of your chatbot, we recommend completing those steps.</p>"},{"location":"advanced/chat_functions/local/#build-the-docker-image","title":"Build the Docker Image","text":"<p>To build the Docker image, run the following command in the root folder of the project (where the Dockerfile is located):</p> <pre><code>docker build -t llm_chat .\n</code></pre>"},{"location":"advanced/chat_functions/local/#running-the-docker-image","title":"Running the Docker Image","text":"<p>To run the Docker image, use the following command:</p>"},{"location":"advanced/chat_functions/local/#without-env-file","title":"Without .env file:","text":"<pre><code>docker run -e OPENAI_API_KEY={your key} -e OPENAI_MODEL={your LLM chosen model name} -p 8080:8080 llm_chat\n</code></pre>"},{"location":"advanced/chat_functions/local/#with-container-name-for-interaction-eg-copying-file-from-inside-the-docker-container","title":"With container name (for interaction, e.g. copying file from inside the docker container):","text":"<pre><code>docker run --env-file .env -it --name my-lambda-container -p 8080:8080 llm_chat\n</code></pre> <p>This will start the evaluation function and expose it on port <code>8080</code> and it will be open to be curl:</p> <pre><code>curl --location 'http://localhost:8080/2015-03-31/functions/function/invocations' \\\n--header 'Content-Type: application/json' \\\n--data '{\"body\":\"{\\\"message\\\": \\\"hi\\\", \\\"params\\\": {\\\"conversation_id\\\": \\\"12345Test\\\", \\\"conversation_history\\\": [{\\\"type\\\": \\\"user\\\", \\\"content\\\": \\\"hi\\\"}]}}\"}'\n</code></pre>"},{"location":"advanced/chat_functions/local/#call-docker-container-from-postman","title":"Call Docker Container From Postman","text":"<p>POST URL:</p> <pre><code>http://localhost:8080/2015-03-31/functions/function/invocations\n</code></pre> <p>Body:</p> <pre><code>{\"body\":\"{\\\"message\\\": \\\"hi\\\", \\\"params\\\": {\\\"conversation_id\\\": \\\"12345Test\\\", \\\"conversation_history\\\": [{\\\"type\\\": \\\"user\\\", \\\"content\\\": \\\"hi\\\"}]}}\"}\n</code></pre> <p>Body with optional Params: <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n        \"conversation_id\":\"12345Test\",\n        \"conversation_history\":[{\"type\":\"user\",\"content\":\"hi\"}],\n        \"summary\":\" \",\n        \"conversational_style\":\" \",\n        \"question_response_details\": \"\",\n        \"include_test_data\": true,\n        \"agent_type\": {agent_name}\n    }\n}\n</code></pre></p>"},{"location":"advanced/chat_functions/quickstart/","title":"Developing Chat Functions: Getting Started","text":""},{"location":"advanced/chat_functions/quickstart/#what-is-a-chat-function","title":"What is a Chat Function?","text":"<p>A chat function is a function which calls Large Language Models (LLMs) to respond to the messages of students given contextual data:</p> <ul> <li>question data</li> <li>user data such as past responses to the problem</li> </ul> <p>Chat functions host a chatbot. Chatbots capture and automate the process of assisting students during their learning process when outside of classroom.</p>"},{"location":"advanced/chat_functions/quickstart/#getting-setup-for-development","title":"Getting Setup for Development","text":"<ol> <li> <p>Get the code on your local machine (Using github desktop or the <code>git</code> cli)</p> <ul> <li>For new functions: clone the template repo for chat-function-boilerplate. Make sure the new repository is set to public (it needs access to organisation secrets). </li> <li>For existing functions: please make your changes on a new separate branch</li> </ul> </li> <li> <p>If you are creating a new chatbot, you can either edit the <code>src/agents/base_agent</code> or copy it and rename it based on the name of your chatbot.</p> </li> <li> <p>You are now ready to start making changes and implementing features by editing each of the main function-logic files:</p> <ol> <li> <p><code>src/agents/{base_agent}/{base}_agent.py</code>: This file contains the main LLM pipeline using LangGraph and LangChain.</p> </li> <li> <p>the chat function expects the following arguments when it being called:</p> </li> </ol> <p>Body with necessary Params:</p> <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n            \"conversation_id\":\"12345Test\",\n            \"conversation_history\": [{\"type\":\"user\",\"content\":\"hi\"}]\n    }\n}\n</code></pre> <p>Body with optional Params:</p> <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n            \"conversation_id\":\"12345Test\",\n            \"conversation_history\":[{\"type\":\"user\",\"content\":\"hi\"}],\n            \"summary\":\" \",\n            \"conversational_style\":\" \",\n            \"question_response_details\": \"\",\n            \"include_test_data\": true,\n            \"agent_type\": {agent_name}\n    }\n}\n</code></pre> </li> <li> <p><code>src/agents/{base_agent}/{base}_prompts.py</code>: This is where you can write the system prompts that describe how your AI Assistant should behave and respond to the user.</p> </li> <li> <p>If you edited the chatbot agent file name, make sure to add your chatbot <code>invoke()</code> function to the <code>module.py</code> file.</p> <ol> <li>Update the <code>config.json</code> file with the name of the chat function.</li> </ol> </li> <li> <p>Please add a <code>README.md</code> file to describe the use and behaviour of your chatbot.</p> </li> <li> <p>Changes can be tested locally by running the pipeline tests using:     <pre><code>pytest src/module_test.py\n</code></pre> Running and Testing Chat Functions Locally</p> </li> <li> <p>Merge commits into dev branch will trigger the <code>dev.yml</code> workflow, which will build the docker image, push it to a shared <code>dev</code> ECR repository and deploy an AWS Lambda function available to any http requests. In order to make your new chatbot available on the <code>dev</code> environment of the Lambda Feedback platform, you will have to get in contact with the ADMINS on the platform.</p> </li> <li> <p>You can now test the deployed chat function using your preferred request client (such as Insomnia or Postman or simply <code>curl</code> from a terminal). <code>DEV</code> Functions are made available at:     <pre><code>https://&lt;***&gt;.execute-api.eu-west-2.amazonaws.com/default/chat/&lt;function name as defined in config.json&gt;\n</code></pre></p> <p>Example Request to chatFunctionBoilerplate-dev</p> <pre><code>curl --location 'https://&lt;***&gt;.execute-api.eu-west-2.amazonaws.com/default/chat/chatFunctionBoilerplate-dev' \\\n--header 'Content-Type: application/json' \\\n--data '{\n        \"message\": \"hi\",\n        \"params\": {\n                \"conversation_id\": \"12345Test\",\n                \"conversation_history\": [\n                        {\n                                \"type\": \"user\",\n                                \"content\": \"hi\"\n                        }\n                ]\n        }\n}'\n</code></pre> </li> <li> <p>Once the <code>dev</code> chat function is fully tested, you can merge the code to the default branch (<code>main</code>). This will trigger the <code>main.yml</code> workflow, which will deploy the <code>staging</code> and <code>prod</code> versions of your chat function. Please contact the ADMIN to provide you the URLS for the <code>staging</code> and <code>prod</code> versions of your chat function.</p> </li> <li> <p>In order to make your new chat function available on any of the environments of the Lambda Feedback platform, you will have to get in contact with the ADMINS on the platform.</p> </li> </ol>"},{"location":"advanced/evaluation_functions/","title":"Deployed Evaluation Functions","text":"<p>Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader.</p> <p>If you can't see any documentation files as subsections here, please contact an admin.</p>"},{"location":"advanced/evaluation_functions/alternate_languages/","title":"Alternate Evaluation Function Languages","text":""},{"location":"advanced/evaluation_functions/alternate_languages/#lambda-compatible-images","title":"Lambda-Compatible Images","text":""},{"location":"advanced/evaluation_functions/alternate_languages/#extending-a-pre-built-lambda-image","title":"Extending a pre-built Lambda image","text":"<ul> <li>Available for: Node.js, Python, Java, .NET, Go, Ruby</li> <li>Docs</li> <li>Repo</li> <li>These base images are regularly updated, and the most widely used (more docs)</li> <li>They also come with pre-packaged runtime interface clients - a HTTP interface for runtimes to receive invocation events and respond<ul> <li>Good for local development</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#creating-custom-base-images","title":"Creating custom base images","text":"<ul> <li>Using the lambda/provided image<ul> <li>This \"contains all the required components to run functions packaged as container images on Lambda\"</li> </ul> </li> <li>Building a custom runtime from scratch <ul> <li>Custom AWS Lambda runtimes</li> <li>Runtimes walkthrough tutorial</li> </ul> </li> <li>Emulate execution locally? <p>Lambda provides a runtime interface emulator (RIE) for you to test your function locally. The AWS base images for Lambda and base images for custom runtimes include the RIE. For other base images, you can download the\u00a0Runtime interface emulator\u00a0from the AWS GitHub repository.</p> </li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#misc-notessources","title":"Misc Notes/Sources","text":"<ul> <li>The Lambda Execution Environment</li> <li>Create Images from Alternative base images</li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#development-philosophy","title":"Development Philosophy","text":"<p>Ultimately we want to call a function made by a user in any language. Two ways to do this:</p> <ul> <li> <p>We write and provide runtime in all the different languages. This means that all the logic happens in that language. We write the code that actually receives the requests from lambda function events. In this case, the user function can be imported from those handlers.</p> <ul> <li>Writing handlers in each of those languages requires time and extensive knowledge (in order to write robust code)</li> <li>Handler code needs to:<ul> <li>Have clean and reliable error catching</li> </ul> </li> </ul> </li> <li> <p>We write a global runtime, which makes a call to their function via a sub-process. We call their script, which must recieve the payload as a commandline argument.</p> <ul> <li>User has to write more code <ul> <li>For allowing cmdline arguments, and parsing of inputs</li> </ul> </li> <li>Might be slower than in other languages. Since another script has to be executed.</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/feedback/","title":"Base Layer Feedback Implementation","text":"<p>Input structure:</p> <pre><code>{\n    \"response\": \"user input\",\n    \"answer\": \"original answer\",\n    \"params\": {\n        \"cases\": [\n            {\n                \"answer\": \"same shape as original answer\",\n                \"feedback\": \"feedback string\",\n                \"params\": {...} # Any parameters to set or override\n            },\n            ...\n        ]\n    }\n}\n</code></pre>"},{"location":"advanced/evaluation_functions/feedback/#execution-logic-for-the-eval-command","title":"Execution Logic for the <code>eval</code> command","text":"<ol> <li>First <code>evaluation_function</code> is called using the response, answer and params</li> <li>If evaluation threw an error, then return the error message</li> <li>If evaluation was successful, check for matching cases<ol> <li>If \"params\" contains a non-empty list of \"cases\", determine the correct feedback, add it to the result and return the block (Logic for this is described in the next section) </li> <li>If \"params\" doesn't contain a list of cases, simply return the result</li> </ol> </li> </ol>"},{"location":"advanced/evaluation_functions/feedback/#determining-the-correct-feedback-case","title":"Determining the correct feedback case","text":"<ol> <li>Iterate through each case in the list of <code>cases</code>:<ol> <li>Validate the case has an 'answer' and 'feedback'</li> <li>If the case contains 'params', then merge them with the original 'params', overwriting values if they already exist</li> <li>Call <code>evaluation_function</code> with the student \"response\", case \"answer\" and merged \"params\"<ol> <li>If the function returns \"is_correct: true\", we have a match, store case and feedback returned from the evaluation function</li> <li>If the function returns an error, catch it and add it to a list of warnings</li> </ol> </li> </ol> </li> <li>If no matches were found, don't return any feedback </li> <li>If exactly one match was found, check if <code>override_eval_feedback</code> is in parameters<ol> <li>If <code>override_eval_feedback</code> is set to true, return the case feedback</li> <li>If <code>override_eval_feedback</code> is not set or set to false, append the evaluation function feedback to the case feedback, separated by a linebreak and the return the result</li> </ol> </li> <li>If more than one matches were found, return the first one (using the same procedure as if only one match was found) and add a warning explaining which cases matched, and why only the first was selected.</li> </ol>"},{"location":"advanced/evaluation_functions/local/","title":"Running and Testing Functions Locally","text":""},{"location":"advanced/evaluation_functions/local/#simple","title":"Simple","text":""},{"location":"advanced/evaluation_functions/local/#using-docker","title":"Using Docker","text":"<p>This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image, the container contains a HTTP client which can be used to locally simulate Lambda execution events. </p> <p>Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as <code>docs</code> and <code>healthcheck</code> can be tested.</p> <ol> <li> <p>Install Docker on your machine</p> </li> <li> <p>Navigate to the root directory of your function</p> </li> <li> <p>Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it <code>eval-tmp</code>.     <pre><code>docker image build -t eval-tmp app\n</code></pre></p> </li> <li> <p>Spin up a container using the image built in the previous step.     <pre><code>docker run --rm -d --name eval-function -p 9000:8080 eval-tmp \n</code></pre></p> </li> <li> <p>You can now simulate requests to the function using any request client (like Insomnia or Postman). By default, the url you can hit is:     <pre><code>http://localhost:9000/2015-03-31/functions/function/invocations\n</code></pre></p> Warning <p>When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway. This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function.</p> <p>Specifically, this means structuring requests in the following way: <pre><code>{\n  \"headers\": {\n    \"command\": \"eval\"\n  },\n  \"body\": {\n    \"response\": \"a\",\n    \"answer\": \"a\",\n    \"params\": {\n      \"garlic\": \"moreish\"\n    }\n  }\n}\n</code></pre></p> <p>The main difference is that <code>headers</code> and <code>body</code> are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the <code>command</code> field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the <code>response</code>, <code>answer</code> and <code>params</code> fields.</p> </li> <li> <p>(Optional) The <code>run</code> command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run:     <pre><code>docker container logs -f eval-function \n</code></pre></p> </li> </ol> Tip <p>You will very rarely need this, but you can peek into the running container by opening a shell within it using:</p> <pre><code>docker exec -it eval-function bash\n</code></pre>"},{"location":"advanced/evaluation_functions/local/#useful-links","title":"Useful Links","text":""},{"location":"advanced/evaluation_functions/module/","title":"evaluation-function-utils Package","text":"<ul> <li>Error Reporting </li> <li>Schema validation</li> <li>Local testing</li> </ul>"},{"location":"advanced/evaluation_functions/module/#errors","title":"Errors","text":"<p>Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors.</p>"},{"location":"advanced/evaluation_functions/module/#class-evaluationexception","title":"class <code>EvaluationException</code>","text":"<p>This class extends the usual python <code>Exception</code>, with additional functionality. It can be used to package additional fields and values to errors thrown and returned by evaluation functions.</p> <p>Example</p> <p>If at some point in the execution of the <code>evaluation_function</code>, an error is thrown:</p> <pre><code>from evaluation_function_utils.errors import EvaluationException\n\nif isinstance(input, str):\n    raise EvaluationException(\n        \"The input must not be a string\", \n        valid_types=[\"int\", \"float\", \"array\"],\n    )\n</code></pre> <p>Then the output generated by the lambda function will look like:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"error\": {\n    \"message\": \"The input must not be a string\",\n    \"valid_types\": [\n      \"int\", \"float\", \"array\"\n    ]\n  }\n}\n</code></pre> <p>This class contains an error_dict property, which packages the additional arguments given to the Exception instance into a JSON-serializable object. It does so in an error-safe way, also reporting serialization errors if they occur.</p>"},{"location":"advanced/evaluation_functions/module/#client","title":"Client","text":"<p>This submodule contains a custom <code>EvaluationFunctionClient</code>, which can be used to call other deployed evaluation functions.</p>"},{"location":"advanced/evaluation_functions/module/#class-evaluationfunctionclient","title":"class <code>EvaluationFunctionClient</code>","text":"<p>Client wrapped around the botocore.client.Lambda, for invoking deployed evaluation functions. On initialisation, it fetches credentials from environment variables \"INVOKER_KEY\", \"INVOKER_ID\" and \"INVOKER_REGION\", or from an optional environment file prescrived by <code>env_path</code>. </p> <p>Example</p> <pre><code>from evaluation_function_utils.client import EvaluationFunctionClient\nclient = EvaluationFunctionClient()\n\ndef evaluation_function(response, answer, params): \n    return client.invoke('isExactEqual', response, answer, params)\n</code></pre> <p>In this example, the evaluation_function completely offloads grading to the deployed 'isExactEqual' function. </p> <p>Note: The <code>EvaluationFunctionClient.invoke</code> method was designed to behave exactly as if the <code>evaluation_function</code> function defined in the targeted deployed function was called directly. This means that if errors are encountered an <code>EvaluationException</code> is raised.</p>"},{"location":"advanced/evaluation_functions/quickstart/","title":"Developing Evaluation Functions: Getting Started","text":""},{"location":"advanced/evaluation_functions/quickstart/#what-is-an-evaluation-function","title":"What is an Evaluation Function?","text":"<p>It's a cloud function which performs some computation given some user input (the response), a problem-specific source of truth (the answer), and some optional parameters (params). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer. However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined. </p>"},{"location":"advanced/evaluation_functions/quickstart/#getting-setup-for-development","title":"Getting Setup for Development","text":"<ol> <li>Get the code on your local machine (Using github desktop or the <code>git</code> cli)<ul> <li>For new functions: create and clone a new repository using the boilerplate template. Make sure the new repository is set to public (it needs access to organisation secrets).</li> <li>For existing functions: please make your changes on a new separate branch </li> </ul> </li> <li>If you are creating a new function, you'll need to set it's name (as it will be deployed) in the <code>config.json</code> file, available in the root directory.<ul> <li>The name must be unique. To view existing grading functions, go to:<ul> <li>Staging API Gateway Integrations</li> <li>Production API Gateway Integrations</li> </ul> </li> </ul> </li> <li> <p>You are now ready to start making changes and implementing features by editing each of the three main function-logic files:</p> <ol> <li> <p><code>app/evaluation.py</code>: This file contains the main <code>evaluation_function</code> function, which ultimately gets called to compare a response to an answer. </p> <p><code>evaluation.py</code> Specification</p> </li> <li> <p><code>app/evaluation_tests.py</code>: This is where you can test the logic in <code>evaluation.py</code>, following the standard <code>unittest</code> format. </p> <p><code>evaluation_tests.py</code> Specification</p> </li> <li> <p>Documentation files:</p> <ul> <li> <p><code>app/docs/dev.md</code>: This file should be edited to reflect any changes/features implemented, following a developer perspective. It is baked into the function's image to be pulled by this documentation website under the deployed functions section.</p> </li> <li> <p><code>app/docs/user.md</code>: This file documents how the function can be used by a teacher user, from the perspective of editing content on the LambdaFeedback platform. This time, files are collated and displayed in the Teacher section.</p> </li> </ul> </li> </ol> </li> <li> <p>Changes can be tested locally by running the tests you've written using: <pre><code>python -m unittest app/evaluation_tests.py\n</code></pre> Running and Testing Functions Locally</p> </li> <li> <p>Merge commits into the default branch will trigger the <code>test-and-deploy.yml</code> workflow, which will build the docker image, push it to a shared ECR repository, then call the backend <code>grading-function/ensure</code> route to build the necessary infrastructure to make the function available from the client app.</p> </li> <li> <p>You can now test the deployed evaluation function using your prefered request client (such as Insomnia or Postman or simply <code>curl</code> from a terminal). Functions are made available at:     <pre><code>https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/&lt;function name as defined in config.json&gt;\n</code></pre></p> <p>Example Request to SymbolicEqual</p> <pre><code>``` \ncurl --request GET \\\n    --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/symbolicEqual \\\n    --header 'Content-Type: application/json' \\\n    --header 'command: eval' \\\n    --data '{\"response\": \"x + x\", \"answer\": \"2*x\"}'\n```\n</code></pre> </li> <li> <p>In order to make your new function available on the LambdaFeedback platform, you have to register it via the Admin Panel. This is done by supplying its name, url (the same as the one above) and supported response types. </p> </li> </ol>"},{"location":"advanced/evaluation_functions/quickstart/#more-info","title":"More Info","text":"<ul> <li> <p>General Function Specification and Behaviour</p> <ul> <li>Function philosophy including deployment strategy</li> <li>Request/Response schemas and communication spec </li> <li>Base layer logic, properties and behaviour</li> </ul> </li> <li> <p>EvaluationFunctionUtils (python package)</p> <ul> <li>Error Reporting </li> <li>Schema validation</li> <li>Local testing</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/specification/","title":"Evaluation Function Specification","text":""},{"location":"advanced/evaluation_functions/specification/#introduction-and-philosophy","title":"Introduction and Philosophy","text":"<p>Functionality for each evaluation function is split up as follows:</p> <p>Universal function behaviour applicable to every function, such as the ability to run tests, return documentation and execute the evaluation is handled by the Base Layer. This is the docker image which is extended by every developed evaluation function.</p> <p>Functionality that may be required in more than one function (but not necessarily all), such as the ability to call already deployed functions and error reporting is handled by the evaluation_function_utils python package. This package comes pre-installed in the base layer, and can optionally be imported and called from the evaluation_function.</p> <p>Finally, specific comparison logic and handling of bespoke evaluation parameters is done in the custom evaluation_function, unique to each deployed instance. This is the logic that differenciates each function (comparing numbers, matrices, images, equations, graphs, text, tables, etc ...).</p>"},{"location":"advanced/evaluation_functions/specification/#commands","title":"Commands","text":"<p>Commands are handled by the base layer. They define a unified interface for interacting with all deployed evaluation functions on the web. Practically, these are specified in the \"command\" request header.</p> <p>Example</p> <p>To execute the <code>docs-user</code> command for a function, the following header would be specified alonside the http request made to the endpoint on which the function is made available:</p> <pre><code>```bash\ncurl --request GET \\\n--url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/isExactEqual \\\n--header 'command: docs-user'\n```\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#eval","title":"<code>eval</code>","text":"<p>This is the default command, used to compare a student's <code>response</code> and correct <code>answer</code>, given certain <code>params</code>. Outputs for this command depend on the success of the execution of the user-defined <code>evaluation_function</code>. If an error was thrown during execution, it is caught by the main handler and an error block is returned - otherwise, successful execution outputs are supplied under a <code>result</code> field.</p> <p>Output Structure: Successful evaluation</p> <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": \"&lt;bool&gt;\",\n\n        # Optional fields added by feedback generation (1)\n        \"feedback\": \"&lt;string&gt;\",\n        \"warnings\": \"&lt;array&gt;\"\n\n        # This output can also contain any number of fields given by `evaluation_function`\n    }\n}\n</code></pre> <ol> <li>See the Feedback Page for more information</li> </ol> <p>Output Structure: Error thrown during Execution</p> <pre><code>{\n    \"command\": \"eval\",\n    \"error\": {\n        \"message\": \"&lt;string&gt;\", # Always present\n\n        # This object can contain other number of additional fields\n        # passed through by the EvaluationException (1) for debugging e.g.:\n        \"serialization_errors\": [],\n        \"culprit\": \"user\",\n        \"detail\": \"...\"\n    }\n}\n</code></pre> <ol> <li>This is a custom error class from the evaluation-function-utils package, which developers are encouraged to use in order to output richer errors. See the Error handling section for more information.</li> </ol>"},{"location":"advanced/evaluation_functions/specification/#preview","title":"<code>preview</code>","text":"<p>This command is similar to <code>eval</code>, except it doesn't return whether an answer is correct or provide feedback. Instead, <code>preview</code> provides a way for students view their response after some pre-processing, e.g. as rendered LaTeX when using Sympy for symbolic algebra.</p> <p>This should be faster to compute than <code>eval</code>, allowing students to get live preview of their response.</p>"},{"location":"advanced/evaluation_functions/specification/#healthcheck","title":"<code>healthcheck</code>","text":"<p>This command runs and returns a summary three testing suites: requests, responses and evaluation. Request and response tests check that inputs and outputs to the function work correctly, and follow the correct syntax. Evaluation tests are unique to each evaluation function and test the actual comparison logic.</p>"},{"location":"advanced/evaluation_functions/specification/#docs-user","title":"<code>docs-user</code>","text":"<p>Command returns the <code>docs/user.md</code> file (base64 encoded)</p>"},{"location":"advanced/evaluation_functions/specification/#docs-dev","title":"<code>docs-dev</code>","text":"<p>Command returns the <code>docs/dev.md</code> file (base64 encoded)</p>"},{"location":"advanced/evaluation_functions/specification/#base-layer","title":"Base Layer","text":""},{"location":"advanced/evaluation_functions/specification/#file-structure","title":"File Structure","text":"<p>A standard evaluation function repository based on the provided boilerplate will have the following file structure:</p> <pre><code>app/\n    __init__.py\n    evaluation.py # Script containing the main evaluation_function\n    evaluation_tests.py # Unittests for the main evaluation_function\n    requirements.txt # list of packages needed for algorithm.py\n    Dockerfile # for building whole image to deploy to AWS\n\n    docs/ # Documentation pages for this function (required)\n        dev.md # Developer-oriented documentation\n        user.md # LambdaFeedback content author documentation\n\n.github/\n    workflows/\n        test-and-deploy.yml # Testing and deployment pipeline\n\nconfig.json # Specify the name of the evaluation function in this file\nREADME.md\n.gitignore\n</code></pre> <p>Warning</p> <p>If you want to split up function logic into different files, these must be added to the <code>Dockerfile</code>. This is so they are packaged with the built image when deployed. For example, if <code>evaluation.py</code> imports functionality from an <code>app/utils.py</code> file, then the following line must be added:</p> <pre><code>RUN pip3 install -r requirements.txt\n\n# Copy the evaluation and testing scripts\nCOPY evaluation.py ./app/\nCOPY evaluation_tests.py ./app/\n\n# Copy additional files\nCOPY utils.py ./app/\n\n# Copy Documentation\nCOPY docs/dev.md ./app/docs/dev.md\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#evaluationpy","title":"<code>evaluation.py</code>","text":"<p>The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the <code>evaluation_function</code> function within it, to be more precise.</p>"},{"location":"advanced/evaluation_functions/specification/#the-evaluation_function","title":"The <code>evaluation_function</code>","text":""},{"location":"advanced/evaluation_functions/specification/#inputs","title":"Inputs","text":"<p>All evaluation functions are passed three arguments:</p> <ul> <li><code>response</code>: Data input by the user</li> <li><code>answer</code>: Data to compare user input to (could be from a DB of answers, or pre-generated by other functions)</li> <li><code>params</code>: Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...)</li> </ul> <p>For evaluation functions that use Sympy or LaTeX for mathematical expressions, it's not always possible for a student to type the correct symbols. Instead we need to use simpler symbols. For example, \\(\\overline{U_{ij}}\\) cannot be written using standard sympy syntax, and therefore has to be substituted for something else, such as <code>\"u\"</code> or <code>\"U\"</code>.</p> <p>Therefore, evaluation functions using mathematical expressions should be able to handle multiple symbols to represent the same variable. To achieve this, every evaluation function is passed a <code>symbols</code> entry in <code>params</code>, to allow functions to convert a student's response:</p> <pre><code>{\n    \"response\": \"user input\",\n    \"answer\": \"model response to compare against\",\n    \"params\": {\n        \"symbols\": {...},\n        ... # params set by the teacher\n    }\n}\n</code></pre> <p><code>symbols</code> is a dictionary, where each key represents the main Sympy symbol (known as the <code>code</code>), and has two entries:</p> <ul> <li><code>latex</code>: the string used for rendering the symbol in LaTeX</li> <li><code>aliases</code>: a list of alternative Sympy symbols that can be used by the student to represent the <code>code</code>.</li> </ul> <p>For the example above with \\(\\overline{U_{ij}}\\), <code>symbols</code> would have the form:</p> <pre><code>{\n    ...\n    \"params\": {\n        \"symbols\": {\n            \"u\": {\n                \"latex\": \"\\\\overline{U_{ij}}\",\n                \"aliases\": [\"U\"]\n            }\n        }\n    }\n}\n</code></pre> <p>Note that in JSON, special characters need to be escaped, so the latex symbol above will have a double-backslash instead.</p> <p>Currently, the backend only supports one LaTeX symbol for multiple Sympy symbols. In future, this will be a many-to-many relationship.</p>"},{"location":"advanced/evaluation_functions/specification/#context","title":"Context","text":"<p>When a student submits a response to a response area the number of previously submitted responses submitted to the same response area byt the same student will be sent to the evaluation function. The following format is used: <pre><code>    {\n        \"submission_context\": {\n            \"submissions_per_student_per_response_area\": # non-negative integer that represent the nubmer of previously processed responses\n        }\n    }\n</code></pre></p>"},{"location":"advanced/evaluation_functions/specification/#outputs","title":"Outputs","text":"<p>The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed:</p> <p><code>is_correct: &lt;bool&gt;</code>: Boolean parameter indicate whether the comparison between <code>response</code> and <code>answer</code> was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red).</p> <p>Info</p> <p>More standardised function outputs that the frontend can consume are to come</p>"},{"location":"advanced/evaluation_functions/specification/#error-handling","title":"Error Handling","text":"<p>Error reporting should follow a specific approach for all evaluation functions. If the <code>evaluation_function</code> you've written doesn't throw any errors, then it's output is returned under the <code>result</code> field - and assumed to have worked properly. This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways:</p> <p>Letting <code>evaluation_function</code> fail: On the request handler in the Base Layer, the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the <code>error.detail</code> field.</p> <p>Custom errors: If you want to report more detailed errors from your function, use the <code>EvaluationException</code> class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end.</p> <p>Example</p> <p>It is discouraged to do the following in the evaluation code: <code>python     if something.bad.happened():         return {             \"error\": {                 \"message\": \"Some important message\",                 \"other\": \"details\",             }         }</code></p> <pre><code>As this causes the actual function output (by the AWS lambda function) to be:\n```json\n{\n    \"command\": \"eval\",\n    \"result\": {\n        \"error\": {\n            \"message\": \"Some important message\",\n            \"other\": \"details\"\n        }\n    }\n}\n```\n\nInstead, use custom exceptions from the [evaluation-function-utils](module.md#errors) package.\n```python\nif something.bad.happened():\n    raise EvaluationException(message=\"Some important message\", other='details')\n```\n\nAs the actual function output will look like:\n```json\n{\n    \"command\": \"eval\",\n    \"error\": {\n        \"message\": \"Some important message\",\n        \"other\": \"details\"\n    }\n}\n```\n\nThis immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed.\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#evaluation_testspy","title":"<code>evaluation_tests.py</code>","text":"<p>This file is intended to contain unit tests for the <code>evaluation_function</code>. Python's built-in <code>unittest</code> framework is used. These tests are run by Github Actions whenever changes are pushed to the main branch, and the evaluation function is not deployed unless all the tests pass.</p> <p>Example</p> <p>A minimal example of a test: <pre><code>import unittest\nfrom .evaluation import evaluation_function\n\n# Tests are functions beginning with \"test_\" in \n# a class that inherits from unittest.TestCase\nclass TestEvaluationFunction(unittest.TestCase):\n    def test_trivial(self):\n        result = evaluation_function(\"a + b\", \"a + b\", {})\n        self.assertTrue(result[\"is_correct\"])\n</code></pre> Tests can be run locally using  <pre><code>$ python -m unittest app.evaluation_tests\n</code></pre></p>"},{"location":"advanced/evaluation_functions/specification/#autotests","title":"Autotests","text":"<p>For writing simple tests, it may be easier to write the tests in a config file and have them run on the evaluation function automatically. This can be achieved using the autotests library, which can easily be integrated into an existing project by adding a decorator to the test class. See the autotests README for more information.</p> <p>Another benefit of this approach is that the tool that collects evaluation function documentation (EvalDocsLoader) can read this file and auto-generate examples of correct and incorrect responses. This can help new users understand the capabilities of your evaluation function.</p> <p>For an example of how this looks, see the user docs for compareBoolean.</p>"},{"location":"advanced/evaluation_functions/specification/#documentation","title":"Documentation","text":"<p>Evaluation function documentation is stored in two files, which contain documentation for developers and users respectively. These files are fetched by  EvalDocsLoader, which integrates them into this documentation site. </p> <p>In order for EvalDocsLoader to find your docs, your evaluation function must:</p> <ol> <li>be deployed to the production site;</li> <li>belong to the lambda-feedback organisation on Github;</li> <li>have a topic called <code>evaluation-function</code>.</li> </ol> <p>Once these requirements are met, the docs you write should appear on the documentation site.</p>"},{"location":"advanced/evaluation_functions/specification/#docsdevmd","title":"<code>docs/dev.md</code>","text":"<p>This should contain documentation that would be useful for new developers working on your function.</p>"},{"location":"advanced/evaluation_functions/specification/#docsusermd","title":"<code>docs/user.md</code>","text":"<p>This should contain information for non-technical users, such as an overview of capabilities, examples, and a description of parameters.</p>"},{"location":"advanced/response_areas/overview/","title":"Overview of response areas","text":""},{"location":"advanced/response_areas/overview/#list-of-response-areas","title":"List of response areas","text":"<p>The list of response areas is maintained in the Teacher section here. In the Developer area (here), the behaviour of the response areas is documented.</p>"},{"location":"advanced/response_areas/overview/#data-types-when-submitting-empty-responses-default-behaviour","title":"Data types when submitting empty responses (default behaviour)","text":"<p>If a user submits a response without inputing a value, the response areas convert the responses as follows before passing them to the evaluation functions:</p> Input Value Type Default Value Comment number undefined [Behaviour needs updating] string undefined [Behaviour needs updating] MATRIX <code>\" \"</code> empty string in all cells TABLE <code>\" \"</code> empty string in all cells MULTIPLE CHOICE <code>False</code> all choices set to false"},{"location":"releases/detailed_releases/","title":"Detailed Releases","text":""},{"location":"releases/detailed_releases/#release-20260130","title":"Release 2026/01/30","text":"<ul> <li>b909-stats-displayed-without-permission - Ensured student progress statistics are displayed only when the user has the View Stats permission. Statistics are now returned only for students the user is authorised to view, including correct handling of tutor-only access.</li> <li>b951-images-response-area - Added a new Images response area type, allowing students to submit image-based responses and enabling teachers to view and assess image submissions consistently with other response areas.</li> <li>b953-null-responseconfig-api-error \u2013 Prevented GraphQL errors when response configuration is absent by aligning schema nullability.</li> <li>b954-dont-fetch-stats-without-access \u2013 Prevented stats queries from being issued when the teacher role does not grant access to set statistics, avoiding unnecessary backend errors.</li> </ul>"},{"location":"releases/detailed_releases/#release-20260116","title":"Release 2026/01/16","text":"<ul> <li>b889-make-the-displayed-numbers-when-importing-students-less-confusing \u2013 Improved student import header mapping by making system column identifiers visually secondary.</li> <li>b909-stats-displayed-without-permission \u2013 Fixed stats being shown without permission by enforcing VIEW STATS access in the Students table.</li> <li>b915-header-for-personal-tutors \u2013 Fixed module header so personal tutors see the correct module title instead of the \u201cTEACHER\u201d label.</li> <li>b930-navigation-to-empty-student-details-page-in-teacher-view \u2013 Fixed navigation to student details so persisted filters do not lead to an empty or incorrect view.</li> <li>b932-api-retrieves-all-students-not-filtered-by-teacher-permissions \u2013 Fixed student filtering so teachers and tutors only see students they are authorised to view.</li> <li>b934-admin-modules-filter-module-name-replaced-by-module-id-after-returning-to-page \u2013 Fixed Admin -&gt; Modules filter to restore module names instead of internal IDs when returning to the page.</li> <li>b940-filter-teacher-modules-students-does-not-remember-sets \u2013 Fixed set-based filters not being restored in the Teacher -&gt; Module -&gt; Students view when navigation away and back.</li> </ul>"},{"location":"releases/detailed_releases/#release-20260113","title":"Release 2026/01/13","text":"<ul> <li>b937-meq-likert-scale \u2013 Added Likert scale visualisation for MEQ stats and improved N/A handling.</li> <li>b944-meq-stats-fixes \u2013 Improved MEQ statistics access control, evaluated teacher selection, and various minor changes.</li> </ul>"},{"location":"releases/detailed_releases/#release-20260107","title":"Release 2026/01/07","text":"<ul> <li>b943-meq-explore-button-throws-error \u2013 Fixed an issue where the Explore page could throw an error for MEQ submissions with empty responses.</li> </ul>"},{"location":"releases/detailed_releases/#release-20260106","title":"Release 2026/01/06","text":"<ul> <li>b808-meq-stats-page - Improved MEQ statistics visibility, filtering, and access control, ensuring teachers and moderators see only appropriate, recent, and sufficiently populated data.</li> <li>b919-meq-moderation-improvements - Improved MEQ moderation by excluding empty submissions, consolidating identical responses, showing only the latest submissions, and refining moderation navigation.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251219","title":"Release 2025/12/19","text":"<ul> <li>931-chat-cursor-fix - Keep the chat input focused after sending a message</li> <li>b936-numeric-units-ra-type-cannot-be-cleared - Fix bug preventing NUMERIC_UNITS response type to clear</li> <li>b933-numeric-and-numericalunits-allow-empty-student-response - Fix a bug that allowed empty submission on BOOLEAN, ESSAY, NUMBER, NUMERIC_UNITS and  TEXT response types.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251216-bis","title":"Release 2025/12/16 bis","text":"<ul> <li>b935-login-fails-when-email-address-changes - Fix login for Microsoft users after they've changed their email address.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251216","title":"Release 2025/12/16","text":"<ul> <li>b547-chat-ui-tweaks - Improved chat interface behaviour, including correct tab selection when switching questions, immediate display of user messages, updated styling, and a new typing animation.</li> <li>b890-admin-chat-statistics-tweaks - Simplified the Admin Chat Statistics page by replacing descriptive sentences with concise table headings and labels.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251211","title":"Release 2025/12/11","text":"<ul> <li>b923-cannot-assign-moderator-role-to-a-new-teacher-in-admin - Allow admin to assign moderator roles</li> <li>b926-handle-null-submission-on-teacher-errors - Show errors with empty submittions</li> <li>927-meq-discard-confirmation - Ask for confirmation when navigating away from a filled but not submitted MEQ</li> </ul>"},{"location":"releases/detailed_releases/#release-20251205","title":"Release 2025/12/05","text":"<ul> <li>b916-meq-tweaks-batch-5 - Minor MEQ UI tweaks: tab and title layout; green ticks on tabs.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251201","title":"Release 2025/12/01","text":"<ul> <li>b917-meq-release - Make MEQs visible on students' home page</li> </ul>"},{"location":"releases/detailed_releases/#release-20251127","title":"Release 2025/11/27","text":"<ul> <li>b912-meq-batch-import - Added modules batch import features for MEQ launch.</li> <li>b914-meq-tweaks-batch-4 - Minor wording and layout changes for MEQ sets.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251126","title":"Release 2025/11/26","text":"<ul> <li>b830-remember-search-and-filter-settings-across-the-app - Added support for remembering search and filter settings across the app during a session.</li> <li>b877-meq-evaluated-teachers-list - Added the ability to configure which teachers are evaluated in each MEQ set, including a new \u201crepeat statements for each teacher\u201d option and several small usability improvements.</li> <li>b878-meq-likert-updates - Improved the Likert question type with editable columns, clearer submission behaviour for surveys, and several small usability and layout fixes.</li> <li>b895-set-name-edit-issues - Fixed issues with editing set names, including unwanted navigation when opening the dialog and repositioning the edit icon next to the title.</li> <li>b898-meq-tweaks-batch-1 - Improved the MEQ sets page with corrected headers, refreshed layouts and styling (including icons, alignment, and backgrounds), and ensured the student module list always reloads correctly.</li> <li>b906-meq-tweaks-batch-2 - Improved the MEQ experience with smoother global submission, clearer messages and titles, support for empty Likert responses, a \u2018Next MEQ\u2019 button, and better handling of redacted content.</li> <li>b908-meq-layout-fix - Fixed a layout issue that caused a double-scroll effect after adding the sets navigation bar.</li> <li>b910-meq-tweaks-batch-3 - Improved the MEQ moderation table and mobile experience, including clearer question numbering, consistent approve/reject buttons, visible essay inputs on mobile, and correct handling of draw mode.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251120","title":"Release 2025/11/20","text":"<ul> <li>b845-add-filter-to-global-tags-multi-widget \u2013 Added a search feature to the global tags multi-select widget, making it easier to find tags quickly.</li> <li>b875-meq-student-home-page-links \u2013 Added links to Module Evaluation Questionnaires (MEQs) on the student home page.</li> <li>b876-meq-set-navigation \u2013 Renamed \u201cSurveys\u201d to Module Evaluation Questionnaires (MEQs) and improved navigation so students can easily access MEQs for all their modules.</li> <li>b879-meq-module-stats \u2013 Improved the statistics view so it is clear which sets are MEQ sets.</li> <li>b880-meq-set-dates-defaults \u2013 Added default hide and release dates for MEQs at the tenant level to simplify setup.</li> <li>b886-student-progress-in-teacher-view-is-not-aligned-if-it-contains-stars \u2013 Improved the alignment of progress indicators in the teacher view, including when a star (submitted solution by student) is shown.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251117","title":"Release 2025/11/17","text":"<ul> <li>b849-import-tags-with-same-column-name \u2013 Allow columns in the student-upload spreadsheet to have identical titles.</li> <li>b864-admin-chat-statistics \u2013 Added a new page in the admin view to preview chat usage statistics.</li> <li>b868-set-card-items-do-not-align-on-some-cards \u2013 Aligned items inside set cards in the teacher view so they line up.</li> <li>b870-cant-add-links-in-lexdown-except-in-raw-markdown \u2013 Fixed link creation in the Lexdown editor.</li> <li>b874-making-saving-draft-message-to-appear-subtle \u2013 Made the \u201csaving draft\u201d message for question versions less intrusive.</li> <li>b882-expanded-list-of-teacher-roles-overflows-window \u2013 Improved UI to ensure the expanded teacher-roles popup is centered.</li> <li>b886-student-progress-in-teacher-view-is-not-aligned-if-it-contains-stars \u2013 UI fix to ensure student progress bars remain aligned even when they contain stars.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251106","title":"Release 2025/11/06","text":"<ul> <li>b871-set-number-in-mobile-view - Fixed the display of set numbers shown as part of the question title in the mobile view.</li> <li>b847-explore-should-be-grey-when-no-data - Fixed the clickability of buttons in the response area in the teacher view.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251105","title":"Release 2025/11/05","text":"<ul> <li>b851-empty-question-title-prevents-save - Question title validation and error handling improvements.</li> <li>b765-audio-record-tweaks - Improved UI for audio recordings.</li> <li>b839-captions-with-latex-adjustments - Revert b774-style-in-captions.</li> <li>b842-button-widths - Teacher editor UI: response area button widths aligned.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251031","title":"Release 2025/10/31","text":"<ul> <li>b853-remove-feedback-when-user-changes-answer - Feedback disappears when user input is modified.</li> <li>b867-sentry-disable-spans-and-replays - Adjusted Sentry configuration to prevent a large number of alerts for the same cause.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251030","title":"Release 2025/10/30","text":"<ul> <li>b860-reaction-icons-are-flashing-numbers - Fix: numbers were briefly appearing above all reaction icons.</li> <li>b865-teacher-preview-display-feedback-on-feedback - Restored reactions in the teacher question preview.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251029","title":"Release 2025/10/29","text":"<ul> <li>b221-overall-progress - Display overall progress of all students to the teacher within a module.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251028","title":"Release 2025/10/28","text":"<ul> <li>b854-metadata-when-cloning-questions \u2013 Additional information when cloning from another question.</li> <li>b856-global-tags-owner-validation-error \u2013 Fixed validation logic for global tag ownership.</li> <li>b863-sentry-error-variable-input-got-invalid-value \u2013 Resolved an error reported by the automated alert system that occurred when an empty value was sent to the API.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251027","title":"Release 2025/10/27","text":"<ul> <li>b539-chasing-likes \u2013 Improved the UI for reaction icons to prevent them from disappearing before the mouse can reach them.  </li> <li>b843-feedback-feedback \u2013 Added reactions to response areas; students can give feedback on the feedback they receive.  </li> <li>b846-global-tags-editing-group-emails \u2013 Reordered fields on the Global Tag editing page for better clarity and usability.</li> </ul>"},{"location":"releases/detailed_releases/#release-20251017","title":"Release 2025/10/17","text":"<ul> <li>b814-retire-expression-ra - Removed unused Mathpix equation endoint</li> <li>782-branching-strategy - New deployment workflow</li> </ul>"},{"location":"releases/detailed_releases/#release-20251010","title":"Release 2025/10/10","text":"<ul> <li>b638-any-external-chat-function - Chatbots implemented as external microservices</li> <li>b810-multiple-chatbots-student - Student UI: choice of chatbot</li> </ul>"},{"location":"releases/detailed_releases/#release-20251009","title":"Release 2025/10/09","text":"<ul> <li>b827-unenroll-many-users \u2013 part 3 \u2013 Added information about the number of students to be unenroled  </li> <li>b802-all-drop-downs-and-tables-in-correct-order \u2013 Checked and corrected default order in drop-down lists and tables across the application  </li> <li>b824-remove-spaces-from-module-instance-dropdown \u2013 Module instance header drop-down UI tweak  </li> </ul>"},{"location":"releases/detailed_releases/#release-20251008","title":"Release 2025/10/08","text":"<ul> <li>b827-unenroll-many-users - part 2 \u2013 improved button labels  </li> <li>b834-reduce-size-of-description-box-in-settings-modal \u2013 made the description box narrower  </li> <li>b789-admin-global-tag-pages-tweaks \u2013 changed global tag type values to make them more explanatory  </li> <li>b790-students-import-wizard-add-info-on-step-2 \u2013 added explanations for each column in Step 2 of the students import wizard  </li> </ul>"},{"location":"releases/detailed_releases/#release-20251003","title":"Release 2025/10/03","text":"<ul> <li>b773-export-image-captions-to-json \u2013 included image captions in exported question JSON files  </li> <li>b774-style-in-captions \u2013 enabled LaTeX and Markdown support in image captions  </li> <li>b775-image-caption-character-limit \u2013 removed the 250-character limit on image captions  </li> <li>b827-unenrol-many-users - part 1 \u2013 added a new feature to allow unenrolling multiple students at once in the teacher view</li> <li>b828-display-student-stats-even-for-0-user-access \u2013 stats graphs are now displayed even when they show 0 student accesses (previously no graph was shown)  </li> </ul>"},{"location":"releases/detailed_releases/#release-20251002","title":"Release 2025/10/02","text":"<ul> <li>b516-set-layout-ui-upgrades - new set layout with 2 adjustable panes</li> <li>b740-student-nav-upgrades - add set navigation within set for students</li> <li>b831-teacher-nav-upgrades - add set navigation within set for teachers</li> <li>b826-teacher-drag-handles - add drag handle for re-ordering response areas</li> <li>b821-more-firebase-auth-errors-caught - ensure usre is fully authenticated before making request to the backend</li> </ul>"},{"location":"releases/detailed_releases/#release-20251001","title":"Release 2025/10/01","text":"<ul> <li>b815-filter-tweaks-ii - improved module filtering in the student view for closed module instances</li> </ul>"},{"location":"releases/detailed_releases/#release-20250929","title":"Release 2025/09/29","text":"<ul> <li>b567-instance-dropdown-tweak \u2013 improved UI for the module instance dropdown  </li> <li>b813-fix-question-overview-buttons-alignment \u2013 corrected alignment of buttons on the question file tab  </li> <li>b814-remove-expression-ra \u2013 removed code for the legacy EXPRESSION response area  </li> <li>b817-wrong-order-of-module-instances-in-dropdown-in-teacher-mode \u2013 corrected the order of module instances in TEACHER mode  </li> <li>b818-omni-input-tweaks \u2013 tweaks to the multi-line expression response area </li> <li>b825-teacher-view-of-tutees - improved ui for the student statistics in TEACHER mode</li> </ul>"},{"location":"releases/detailed_releases/#release-20250925","title":"Release 2025/09/25","text":"<ul> <li>b816-likert-response-type-mobile-view \u2013 adjusted Likert grid to fit on mobile screens  </li> <li>b536-optimise-student-module-list-retrieval \u2013 optimised data retrieval for the home screen</li> </ul>"},{"location":"releases/detailed_releases/#release-20250923","title":"Release 2025/09/23","text":"<ul> <li>b777-expression-multi-line - multi-line expression response area tweaks</li> <li>755-rework-expression-ra - single-line expression response area improvements</li> <li>801-use-mathpix-app-tokens - use Mathpix directly from the client-side</li> </ul>"},{"location":"releases/detailed_releases/#release-20250915","title":"Release 2025/09/15","text":"<ul> <li>b777-expression-multi-line - multi-line expression response area added</li> </ul>"},{"location":"releases/detailed_releases/#release-20250912","title":"Release 2025/09/12","text":"<ul> <li>b708-filter-modules-in-teacher-view-by-teacher-roles \u2013 improved module filtering, adding support for filtering by teacher roles and the closed modules flag  </li> <li>b800-likert-response-type \u2013 introduced the new likert response type for surveys</li> </ul>"},{"location":"releases/detailed_releases/#release-20250908","title":"Release 2025/09/08","text":"<ul> <li>b550-ui-tweaks-iv \u2013 General UI improvements, mainly alignment adjustments.  </li> <li>b783-ugly-mcq-questions-scroll-bars \u2013 Removed unwanted scrollbars in the MCQ response area on Firefox.  </li> <li>b798-expression-type-mode-does-not-display-feedback-correctly \u2013 Fixed an issue where typed input in the Expression area was not parsed correctly. This was caused by earlier fixes, which have now been reverted:  </li> <li>b796 \u2013 Check button not reappearing after changing the answer  </li> <li>b787 \u2013 Discrepancy in teacher and student submission payload  </li> <li>b335 \u2013 Enabled expression live preview in Teacher mode  </li> </ul>"},{"location":"releases/detailed_releases/#release-20250905","title":"Release 2025/09/05","text":"<ul> <li>b796-check-button-does-not-appear-again-after-changing-the-answer - a fix provided so that Check button reappears after the user changes his answer</li> </ul>"},{"location":"releases/detailed_releases/#release-20250903-part-2","title":"Release 2025/09/03 - part 2","text":"<ul> <li>b787-discrepancy-in-teacher-and-student-submission-payload - fixed the submission API payload for the TEACHER mode</li> <li>b793-non-interpretable-equation-message - displaying correct message for typed, non-interpretable equation in TEACHER mode</li> </ul>"},{"location":"releases/detailed_releases/#release-20250903","title":"Release 2025/09/03","text":"<ul> <li>b335-enable-expression-live-preview-in-teacher-mode - enable live preview of expressions in the TEACHER mode</li> <li>b557-the-app-is-hanging-if-the-set-has-no-question - improved handling of the situation when a set has no question</li> <li>b701-create-tag-on-edit-tags-page-in-admin-mode - added new admin pages to maintain global tags</li> <li>b729-revisit-roles-redirect - improved navigation to modules pages when switching from TEACHER view to STUDENT view and vice versa</li> <li>b752-user-filter-tweaks - UI improvement to the user filter feature</li> <li>b756-zero-stars - improved stars display in the generated PDF documents</li> <li>b761-bulk-rollover-tweaks - bulk rollover improvements</li> <li>b762-content-formatting-help-direct-to-milkdown-page - corrected the navigation from the content formatting help link</li> <li>b772-types-for-global-tags - introduced global tag types</li> <li>b779-standard-cancel-and-save-buttons-positioned-at-the-end-of-a-page - fixed positioning of the save and cancel buttons</li> <li>b780-sort-the-evaluation-functions-in-the-drop-down-list-alphabetically - sorted the evaluation function in the alphabetic order</li> </ul>"},{"location":"releases/detailed_releases/#release-20250722","title":"Release 2025/07/22","text":"<ul> <li>b516-ai-bubble-width - Fix workspace conversation bubbles style</li> <li>b762-content-formatting-help-direct-to-milkdown-page - Fix content editing documentation links</li> <li>b766-unescape-chars - Fix characters escaping when switching from Markdown view</li> <li>b768-ra-sandbox - Introduce response area sandbox</li> <li>hotfix - Fix TLDraw crashes when using text shape</li> </ul>"},{"location":"releases/detailed_releases/#release-20250710","title":"Release 2025/07/10","text":"<ul> <li>b746-bulk-rollover-enhancements - Converted the bulk module rollover into a backend process and added a dedicated page to monitor its progress.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250708","title":"Release 2025/07/08","text":"<ul> <li>b504-canvas-pen-to-be-small-by-default - Changed the pen to have options s,m,l and made the option s the default one.</li> <li>b611-expression-ra-updates - Improved the expression response area preview.</li> <li>b727-improve-the-graph-library-dynamic-scale - Improved the dynamic scale of the graph library.</li> <li>b739-audio-recorder - Improved the Lexdown to allow to record audios directly.</li> <li>b757-chat-errors-response-time - Improved the error handling of the chat-function.</li> <li>b745-backup-stopped-in-prod - Improved the db backup.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250703","title":"Release 2025/07/03","text":"<ul> <li>b696-personal-tutor-students-filters-teacher \u2013 Added a new advanced user filter to admin pages (listing admins and teachers) and to teacher pages (listing students).</li> <li>b713-teacher-students-page-add-filter-to-see-closed-modules \u2013 Filtered module instances to display only current ones by default on the Modules page in TEACHER mode, and added an \"Include closed module instances\" switch.</li> <li>b720-allow-teachers-to-be-linked-to-global-tags-as-students \u2013 Teachers and admins can now be linked to global tags not only as teachers but also as students.</li> <li>b735-students-import-change-to-optional-columns \u2013 When importing students, the teacher email field is now optional, and tag name is mandatory. The teacher name was removed entirely.</li> <li>b742-allow-to-a-module-teacher-or-tutor-access-to-the-the-content-tab-unrestricted \u2013 Removed the \"Modify content\" restriction from the Content tab in TEACHER mode.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250618","title":"Release 2025/06/18","text":"<ul> <li>b720-allow-teachers-to-be-linked-to-global-tags-as-students \u2013 Display both global tags (student and teacher) for admin and teacher users. Use only student global tags for student users.</li> <li>b723-set-edit-icon-buttons \u2013 Display the set visibility icons as buttons.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250612","title":"Release 2025/06/12","text":"<ul> <li>b706-default-end-date-on-module-instances - Made the module instance end date a required field.</li> <li>b719-access-denied-redirect - Improved module instance permission checks to account for both teacher and tutor roles.</li> <li>b726-synchronise-stats-db-queries - Synchronized the start and end dates used in module, student, and student-module access statistics. Now includes all students, regardless of their user roles.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250611","title":"Release 2025/06/11","text":"<ul> <li>b710-navigate-to-explore-student-from-students-module-in-teacher-view \u2013 Enabled navigation to the student explore page from the teacher's students page.  </li> <li>b712-alignment-on-student-home-page \u2013 UI alignment adjustments on the student home page.  </li> <li>b715-admin-teachers-display-which-modules-the-teacher-is-assigned-to \u2013 Added a new admin page showing all modules to which a teacher is assigned, either as a teacher or tutor.  </li> <li>b718-feed-displays-different-content-when-scrolling-through-modules \u2013 Fixed feed pagination on the teacher home page.  </li> <li>b724-personal-tutor-imports-qa-requests \u2013 Improved student import process.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250605","title":"Release 2025/06/05","text":"<ul> <li>b588-student-current-active-session-timings - Improved accuracy of question statistics graphs.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250604","title":"Release 2025/06/04","text":"<ul> <li>b714-teacher-students-page-and-panel-ui-adjustments - UI alignment of the Students and Modules tables on the Teachers home page.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250602","title":"Release 2025/06/02","text":"<ul> <li>b668-lexdown-updates - A set of improvements to the lexical editor implementation.</li> <li>b705-personal-tutor-imports - Introduction of student import, including personal tutor and global tag assignment.</li> <li>/687-admin-teachers-add-global-tags - Added global tags to the admin teachers page.</li> <li>b711-overlapping-elements - Fixed a styling issue in the guidance widget.</li> <li>b716-lexdown-styling-tweaks - A set of improvements to the styling of the lexical editor.</li> <li>b717-lexdown-raw-markdown-improvements - Improvements to raw markdown handling.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250516","title":"Release 2025/05/16","text":"<ul> <li>b703-personal-tutor-tweaks - A set of improvements to the personal tutor functionality.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250507","title":"Release 2025/05/07","text":"<ul> <li>b684-access-modules-as-personal-tutor \u2013 Added links to the modules listed in the Students panel on the teacher landing page, allowing personal tutors to access the modules.</li> <li>b686-special-teacher-role-for-personal-tutor \u2013 Introduced a special teacher role, \"personal tutor\", to configure access permissions for personal tutors.</li> <li>b702-filter-by-global-tag-admin \u2013 Created a new filter component to filter users by email and/or cohort. Currently added only to the admin students page for testing.</li> <li>b707-paging-on-admin-modules-page-does-not-work \u2013 Fixed a bug that caused pagination to not work correctly in several tables.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250428","title":"Release 2025/04/28","text":"<ul> <li>b683-personal-tutor-introduction - The teacher home page now includes a Students panel for personal tutors to view their tutor group's students and their their progress.</li> <li>b689-allow-teacher-to-see-user-role-permissions - Teachers can now view the permissions assigned to each teacher role.</li> <li>b691-admin-dashboard-unique-user-logs -  Improved metric on the admin dashboard to count unique users.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250425","title":"Release 2025/04/25","text":"<ul> <li>b674-breadcrumbs-tweaks - Updated breadcrumbs to maintain consistency across the application.</li> <li>b682-teacher-role-type - Admins can now view the teacher role type, and they can change the description for the \"owner\" role.</li> <li>b688-allow-multiple-teachers-per-global-tag - Global tags can now have more than one teacher assigned.</li> <li>b695-display-an-error-if-a-content-cannot-be-displayed - Teachers to see error if a content could not be parsed (by the Lexdown parser).</li> </ul>"},{"location":"releases/detailed_releases/#release-20250414","title":"Release 2025/04/14","text":"<ul> <li>b616-teacher-sets-list-redesign - New look of the student list in the teacher view.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250410","title":"Release 2025/04/10","text":"<ul> <li>b699-pdf-generator-inline-images - Fixed an error that occurred when a question contained no images. This is a temporary fix, which causes non-captioned images to be left-aligned.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250404","title":"Release 2025/04/04","text":"<ul> <li>b599-teacher-set-timings-statistics - set timing statistics introduced</li> <li>b647-fix-tab-numbers - correct number of activities and errors on the tabs</li> <li>b664-milkup-vertical-spacing - ui improvements</li> <li>b/681-teacher-landing-page teacher landing page introduced</li> </ul>"},{"location":"releases/detailed_releases/#release-20250402","title":"Release 2025/04/02","text":"<ul> <li>b690-lexdown-legacy-content - updated Lexdown to ensure all legacy content is correctly displayed</li> <li>b676-user-permissions-tweaks-and-bugs - added access restrictions to the question Stats tab, specifically limiting access to Explore and Configure functionality</li> </ul>"},{"location":"releases/detailed_releases/#release-20250327","title":"Release 2025/03/27","text":"<ul> <li>b647-fix-tab-numbers - corrected tab to display the correct number of activities</li> <li>b561-comments-buttons - simplified button arrangement for posting comments</li> <li>b680-lexdown-insert-image-triggers-submit-event - updated Lexical text editor implementation to prevent incorrect triggering of submit events</li> <li>b650-reactions-scroll-under-tabs-in-teacher-edit-mode - improved UI so that reaction and flag icons scroll under the top panel instead of over it</li> <li>b599-teacher-set-timings-statistics - added a graph to show statistics on how students access and work on sets</li> </ul>"},{"location":"releases/detailed_releases/#release-20250324","title":"Release 2025/03/24","text":"<ul> <li>b531-teacher-modules-page-table-filtering-and-sorting - improved sorting of the module list in the teacher view</li> <li>b642-update-manually-hidden-in-sets-page-immediately-after-updating-in-settings - fixed the issue with the \"manually hidden\" switch not updating immediately</li> <li>b649-chatbot-message-reactions - introduced reactions and flagging features to allow students to comment on chatbot responses</li> <li>b657-breadcrumbs-should-show-module-name - added module name to the breadcrumbs</li> <li>b671-admin-chat-flags-page - added a new admin page to view student flags on chatbot responses</li> <li>b660-global-tag-attributes - added teacher email address as an attribute to the global tag to link the global tag with the corresponding teacher</li> </ul>"},{"location":"releases/detailed_releases/#release-20250314","title":"Release 2025/03/14","text":"<ul> <li>b655-small-chat-improvements - Minor chat enhancements, primarily focused on providing suggestions.</li> <li>b663-milkup-editor-improvements-ii - Text editor tweaks.</li> <li>b659-style-of-chats-to-match-the-lexical-text-editor - Updated chat styling.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250311","title":"Release 2025/03/11","text":"<ul> <li>b658-milkup-editor-improvements-i - Text editor upgrades, including handling tables, centering images in PDFs, handling modals, and improving step-by-step display in worked solutions and tutorials.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250305","title":"Release 2025/03/05","text":"<ul> <li>b652-introducing-milkup-editor - Implemented a new Milkup editor (a Lexical-based editor with extensions developed by a student group) to replace the existing Milkdown editor.</li> <li>b627-add-a-switch-for-text-editors - Introduced switch to activate either Mildown editor or Milkup editor.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250225","title":"Release 2025/02/25","text":"<ul> <li>b373-show-number-of-unresolved-activities-on-tab-header - numbers added to tabs in TEACHER mode.</li> <li>b645-fix-question-version-duplicate-statistics - Fixed statistics to avoid duplicate counting.</li> <li>b634-teacher-modules-progress-bar-tooltip-fix - Removed module IDs from the progress bar tooltip.</li> <li>b601-stats-switch-to-edit - New switch in TEACHER mode from response area stats to edit.</li> <li>b635-chat-improvements-more-question-info - Additional question data provided to chatbots.</li> <li>b630-stats-not-refreshed-after-enrolling-or-deleting-a-student - stats refreshed immediatley when a student is added/removed to/from the module.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250219","title":"Release 2025/02/19","text":"<ul> <li>631-chat-welcome-message-for-new-conversations - Added a welcome message for students using the chatbot feature for the first time.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250213","title":"Release 2025/02/13","text":"<ul> <li>b574-teacher-roles - Introduced teacher roles to manage permissions and access levels.</li> <li>b583-student-view-should-be-the-same-for-teacher-and-admin - Admin users now have the same view in student mode as teachers.</li> <li>b624-teacher-modules-stats-performances - Improved performance when retrieving data for statistics and graphs in teacher modules.</li> <li>b637-chat-functions-upsert-service - Standardised chatbot function deployment to use the same mechanism as evaluation functions.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250207","title":"Release 2025/02/07","text":"<ul> <li>b600-teacher-set-ra-statistics - Set statistics for teachers</li> <li>b555-firefox-expression-writing-and-scanning-misaligned - Upgraded canvas library to make sure canvas is working with Firefox</li> <li>b568-header-no-drop-down-if-only-one-instance - The module instance to be displayed as a text (instead of dropdown) in the header if there is only one module instance</li> </ul>"},{"location":"releases/detailed_releases/#release-20250205","title":"Release 2025/02/05","text":"<ul> <li>b559-instances-in-order - corrected order of module instances in the header for both, teacher and student view, to be in descending chronological order</li> <li>b598-query-for-admin-dashboard-evaluation-functions-needs-optimisation - optiised query for evaluation function statistics in admin dashboard</li> <li>b604-teacher-modules-stats - Show set's activity and progress statistics on teacher's modules list page</li> <li>b607-duplicate-notifications - preventing email notifications to be sent twice</li> </ul>"},{"location":"releases/detailed_releases/#release-20250129","title":"Release 2025/01/29","text":"<ul> <li>b602-teacher-sets-overview-stats - Show set's activity and progress statistics on teacher's module overview page</li> <li>b603-teacher-sets-list-stats - Show set's activity and progress statistics on teacher's module content page</li> <li>b615-adjust-workspace-size-on-open - Ensure workspace width always stays in given boundaries</li> <li>b589-teacher-set-statistics-improvements - various improvements to the question statistics chart</li> </ul>"},{"location":"releases/detailed_releases/#release-20250129_1","title":"Release 2025/01/29","text":"<ul> <li>b591-workspace-split-panes-improvements - Reworked the workspace split panes</li> </ul>"},{"location":"releases/detailed_releases/#release-20250123","title":"Release 2025/01/23","text":"<ul> <li>b543-lost-canvas-snapshot - improve robustness of canvas saving</li> <li>b522-latex-edit-box-over-displayed-latex - milkdown UI fixes</li> </ul>"},{"location":"releases/detailed_releases/#release-20250121","title":"Release 2025/01/21","text":"<ul> <li>b65-enhanced-stats - chart added in the question STATS tab</li> <li>b590-chat-canvas-documentation - chatbot documentation added</li> <li>b598-query-for-admin-dashboard-evaluation-functions-needs-optimisation - part 1 - evaluation function statistics disabled. Query will be optimised.</li> </ul>"},{"location":"releases/detailed_releases/#release-20250115","title":"Release 2025/01/15","text":"<ul> <li>b524-modular-chatbot-workspace - new chatbot functionality</li> <li>b540-chatbot-switches - chatbot toggles</li> <li>b548-chat-mcqs-information-conversion - parse MCQ respones into a user-readable format for chatbots</li> <li>b552-pulumi-chat-infra-dev-staging-prod  - infrastructure for chatbots</li> <li>b578-inconsistent-lambda-function-number-of-errors - resolved inconsistencies in eval function error statistics</li> </ul>"},{"location":"releases/detailed_releases/#release-20250110","title":"Release 2025/01/10","text":"<ul> <li>b584-PDF-tables - ensure tables compile in PDFs</li> </ul>"},{"location":"releases/detailed_releases/#release-20250109","title":"Release 2025/01/09","text":"<ul> <li>b75-add-labels-to-users-to-allow-filtering-in-analytics-examples-of-useful-labels-guest-msc-group2-personaltutor-etc-user-type - added 'none' option to student filter</li> </ul>"},{"location":"releases/detailed_releases/#release-20250108","title":"Release 2025/01/08","text":"<ul> <li>b75-add-labels-to-users-to-allow-filtering-in-analytics-examples-of-useful-labels-guest-msc-group2-personaltutor-etc-user-type - student admin categories and student module tags  </li> <li>b562-single-multiple-choice-toggle-not-clear - tidied up multiple choice toggle in the response area configuration panel</li> </ul>"},{"location":"releases/detailed_releases/#release-20241212","title":"Release 2024/12/12","text":"<ul> <li>b563-misc-frontend-changes - client-side setup improvements</li> </ul>"},{"location":"releases/detailed_releases/#release-20241206","title":"Release 2024/12/06","text":"<ul> <li>b556-Solutions-PDF---Lambda-step-error - PDF generator in backend - fix inline LaTeX (only in structured tutorial/final answer/worked solutions)</li> </ul>"},{"location":"releases/detailed_releases/#release-20241209","title":"Release 2024/12/09","text":"<ul> <li>b440-notifications-in-ui - add notifications feed for teachers</li> </ul>"},{"location":"releases/detailed_releases/#release-20241206_1","title":"Release 2024/12/06","text":"<ul> <li>b556-Solutions-PDF---Lambda-step-error - fix to PDF generator Lambda to handle steps (e.g. in worked solutions)</li> </ul>"},{"location":"releases/detailed_releases/#release-20241202","title":"Release 2024/12/02","text":"<ul> <li>b533-teacher-header-eager-auth - Prevent header/wrapper requests to module without a user</li> <li>fix-week-bounds - Fix weekly recap bounds including Sunday</li> </ul>"},{"location":"releases/detailed_releases/#release-20241129","title":"Release 2024/11/29","text":"<ul> <li>b460-ui-tweaks-iii - additional tweaks to the user interface</li> <li>b541-module-cloning-should-copy-over-support-material-settings - module cloning carries over settings for visibility of structured tutorials, final answers, and worked solutions.</li> </ul>"},{"location":"releases/detailed_releases/#release-20241125","title":"Release 2024/11/25","text":"<ul> <li>b533-firebase-auth-idtoken-is-required - no change to UX. Fixed issue \"Error: idToken is required\" caused by a premature query before the token was available</li> <li>b537-a-gap-between-title-and-list-of-modules - fix: remove unintended gap between the page title and content when the canvas feature was opened</li> </ul>"},{"location":"releases/detailed_releases/#release-20241123","title":"Release 2024/11/23","text":"<ul> <li>b544-eval-function-schema-403 - evaluation functions base layer: schema included to avoid 403 errors when retrieving</li> </ul>"},{"location":"releases/detailed_releases/#release-20241120","title":"Release 2024/11/20","text":"<ul> <li>b526-studentgetmodules-query-is-slow - optimized data retrieval, sorting, and filtering for the student modules page</li> </ul>"},{"location":"releases/detailed_releases/#release-20241115","title":"Release 2024/11/15","text":"<ul> <li>b463-save-button-for-all-response-types - save button to save work before submission. Configurable per response type at ADMIN level, and per response area at TEACHER.</li> <li>b468-email-updates-settings - introduced an admin feature allowing changes to the recap schedule setting for each teacher</li> <li>b503-the-list-of-errors-and-flags-in-teacher-view-to-contain-info-about-the-part - enhanced the teacher view by including details about which part each flag and error was created against</li> <li>b527-numericunits-displays-answers-incorrectly-in-the-configure-panel - resolved an issue with numeric units that previously displayed incorrectly when spaces were present</li> <li>b528-upgrade-next-from-1423-to-1424 - updated Next.js library</li> <li>b529-update-branches-info-in-readmemd - updated the README.md to provide developers with the latest information about the ticket board and testing processes</li> </ul>"},{"location":"releases/detailed_releases/#release-20241113","title":"Release 2024/11/13","text":"<ul> <li>b506-add-authentication-with-google - replaced MSAL (Microsoft Authentication Library) with Firebase Authentication to allow sign-in using both Microsoft and Google accounts</li> </ul>"},{"location":"releases/detailed_releases/#release-20241031","title":"Release 2024/10/31","text":"<ul> <li>b465-do-not-remove-whitespace-from-input-symbols - fix: remove spaces from input symbols only at the beginning and end, preserving spaces in the middle</li> <li>b477-set-json-generation-returns-403-if-a-media-is-not-accessible - improvement: PDF generation now returns a clearer error message when media fails to download</li> <li>b507-module-options-edit-does-not-work-correctly - fix: refresh module options in the teacher view after they are updated</li> <li>b511-publish-whole-set-questions-missing-from-list - fix: ensure the list of unchanged questions in information messages displays all relevant questions</li> <li>b512-add-eslint-rules-for-imports - improvement: adjusted import order in code for better readability and logical structure</li> <li>b513-open-link-choices-for-tab-columns-with-links - enhancement: added \u201copen link\u201d options to table columns containing hyperlinks, improving navigation across tables</li> <li>b514-pdf-generation-pandoc-exited-with-code-43-fontconfig-error-no-writable-cache-directories - adjustment: modified PDF generation to redirect Fontconfig logs to writable directories within the Lambda function</li> </ul>"},{"location":"releases/detailed_releases/#release-20241028","title":"Release 2024/10/28","text":"<ul> <li>473-middle-click-cmdclick-on-links-in-mui-tables - added \u201copen link\u201d options to table columns with hyperlinks</li> <li>488-links-on-module-home-page - enhanced cards on the teacher module home page to allow clicks that navigate to relevant tabs</li> <li>499-switching-to-canvas-resets-to-part-a - prevented reset to part A when opening or closing the canvas</li> </ul>"},{"location":"releases/detailed_releases/#release-20241023","title":"Release 2024/10/23","text":"<ul> <li>b498-feedback-does-not-handle-double-dollars - feedback string display offers basic support for double-dollars. (REVERTED)</li> <li>b505-enable-weekly-recap-by-default-for-new-users - email notifications tweaks for new users</li> </ul>"},{"location":"releases/detailed_releases/#release-20241018","title":"Release 2024/10/18","text":"<ul> <li>b493-one-student-progress-csv-format-change - adjustment to the CSV format of an individual student\u2019s progress</li> <li>b495-augment-feedback-colour - updated augment feedback functionality</li> <li>b481-cannot-switch-page-in-the-error-panel-in-the-admin-dashboard - fix: allowing multiple tables on one page with paging functionality</li> </ul>"},{"location":"releases/detailed_releases/#release-20241017","title":"Release 2024/10/17","text":"<ul> <li>b497-milkdown-response-area - milkdown response area type added</li> </ul>"},{"location":"releases/detailed_releases/#release-20241016","title":"Release 2024/10/16","text":"<ul> <li>b482-question-scrolls-up-when-clicking-check-or-mark-as-done-on-mobile-and-tablet - fix: resolved issue where marking student submissions as done caused the page to scroll to the top on mobiles and tablets.</li> <li>b483-feedback-area-does-not-support-latex-rendering-again - adjustment to the feedback to support latex</li> <li>b490-dashboard-high-no-of-students - improved the dashboard\u2019s calculation of the current number of students.</li> <li>b494-augment-feedback-if-the-augment-is-true-and-the-returned-feedback-is-empty - updated the augment feedback functionality to handle cases where the augment flag is true, but the returned feedback is empty.</li> <li>b496-feedback-to-handle-html-and-latex - adjustment to the feedback to support html alongside latex</li> </ul>"},{"location":"releases/detailed_releases/#release-20241015","title":"Release 2024/10/15","text":"<ul> <li>b461-bulk-rollover-follow-up-ii - updates to bulk rollover feature</li> <li>b479-scan-in-mobile-tablet-doesnt-activate-camera-or-file-selector - fix: camera did not activate in mobile or tablet when using scan option</li> </ul>"},{"location":"releases/detailed_releases/#release-20241009","title":"Release 2024/10/09","text":"<ul> <li>b486-cloning-parts-in-wrong-order - corrected the part order in cloned module instances and updated code to ensure future clones maintain correct part order.</li> </ul>"},{"location":"releases/detailed_releases/#release-20241007","title":"Release 2024/10/07","text":"<ul> <li>b220-download-one-student-progress - download an individual student\u2019s progress in CSV format.</li> <li>b471-admin-teacher-enrolment-should-accept-comma-separated-lists - admin enrolment of teachers accepts comma-separated lists, allowing multiple teachers to be added at once.</li> <li>b478-question-not-visible-in-tablet-mode-when-canvas-closed - fix: question was not visible in tablet mode when the canvas was closed.</li> <li>b480-draw-mode-proceed-button-almost-invisible-update-to-match-the-design-on-scan-mode - EXPRESSION response area: added the \u201cProceed\u201d button in draw mode to match the design on scan mode.</li> </ul>"},{"location":"releases/detailed_releases/#release-20241004","title":"Release 2024/10/04","text":"<ul> <li>b218-teacher-view-modules-students-explore-do-not-limit-next - teacher view of student progress: \u201cNext\u201d button progresses to all students, not just those listed on the current table page.</li> <li>b448-question-jumps-to-top-on-mark-as-done - UI fix: resolved issue where marking student submissions as done caused the page to jump to the top.</li> <li>b466-limit-number-of-student-submissions - eval functions receive the number of student submissions per response area, and can optionally limit submissions e.g. to external services.</li> <li>b467-correct-separatefeedbacks-to-separatefeedback - schema correction: <code>separatefeedbacks</code> to <code>separatefeedback</code> (relates to the 'Augment feedback based on correctness' functionality.)</li> <li>b475-set-all-augment-booleans-to-false-in-a-specific-module - Set all \"Augment feedback based on correctness\" booleans to <code>false</code> for a selected math module.</li> </ul>"},{"location":"releases/major_releases/","title":"Major releases","text":"<p>This page contains summaries; see also detailed releases.</p>"},{"location":"releases/major_releases/#2025-q2-new-features","title":"2025 Q2 new features","text":""},{"location":"releases/major_releases/#user-roles","title":"User roles","text":"<ul> <li>Access control: different Teacher roles have access to combinations of privileges, including content editing, user enrollment, viewing cohort data, and viewing student data. The screenshot below is an example, and more combinations can be created by admins:</li> </ul> <p>An example of the roles in action:</p> <p></p> <ul> <li>Personal tutor role: <ul> <li>This special role gives a specific teacher access to view data of a limited group of students.</li> <li>Access is by 'Global tags' associating a student to a teacher (across all modules).</li> <li>Teacher home page shows overview stats for personal tutees (see below).</li> <li>Stats can be expanded and explored in more detail.</li> <li>Data is symemtrically available to both students and tutors.</li> </ul> </li> </ul> <p></p> <ul> <li>Teachers - with relevant privilages - can now enrol other teachers, and change their roles.</li> </ul>"},{"location":"releases/major_releases/#2025-q1-new-features","title":"2025 Q1 new features","text":""},{"location":"releases/major_releases/#data-analytics","title":"Data analytics","text":"<p>Enriched data for teachers, including:</p> <ul> <li>Completion statistics per response area (in the 'STATS' tab within a question) </li> <li>Completion statistics per question (in the 'STATS' tab within a question)  </li> <li>Completion and activity statistics per Set (in the 'CONTENT' tab within a module)  </li> <li>Completion and activity statistics per Set (on the 'OVERVIEW' tab within a module)  </li> <li>Completion and activity statistics per Module (on the home page)  </li> </ul>"},{"location":"releases/major_releases/#chat-bots","title":"Chat bots","text":"<p>The workspace has a 'chat' pane where users chat with a bot that has access to the question and the user's work on the question. 'Chat functions' will operate like evaluation functions --- they can be developed externally and plugged in.  </p>"},{"location":"releases/major_releases/#new-content-editor","title":"New content editor","text":"<p>'Lexdown', a markdown-first version of Meta's lexical. As well as a more robust general component, this upgrade allows users to:</p> <ul> <li>Resize images</li> <li>Edit raw markdown</li> <li>Preview latex including when it doesn't compile (with errors in red) </li> <li>Improved table capabilities</li> </ul>"},{"location":"releases/major_releases/#user-tags","title":"User tags","text":"<ul> <li>Tag users within a module ('module tag'), to allow grouping and filtering</li> <li>Tag users across modules ('global tag'), to allow user-orientated analytics (e.g. for pastoral care)</li> </ul>"},{"location":"releases/relases_2023_24/","title":"2023-24","text":""},{"location":"releases/relases_2023_24/#release-20240927","title":"Release 2024/09/27","text":"<ul> <li>b457-option-to-omit-iscorrect-feedback - added the option for teachers to customise the feedback prefix and colour</li> <li>b459-image-on-the-same-line-as-preceding-text - corrected PDF generation to prevent images from appearing on the same line as the preceding text</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240923","title":"Release 2024/09/23","text":"<ul> <li>b449-improve-mobile-navigation - improve mobile UI</li> <li>b462-final-summer-tweaks - ui improvements and fixes</li> <li>b146-follow-up - email notifications tweaks and fixes</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240918","title":"Release 2024/09/18","text":"<ul> <li>b444-bulk-rollover-follow-up - updates to bulk rollover feature</li> <li>b452-ui-tweaks-ii - additional ui enhacements</li> <li>b453-config-panel-disabled-feedback-tab-is-still-accessible-by-next-and-previous-buttons - fixed an issue where disabled tabs in the response area configuration panel could still be accessed using navigation buttons</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240914","title":"Release 2024/09/14","text":"<ul> <li>b134-ipados-ios-safari-like-flag-problem-help-off-the-page - fixed scrolling of problem sets when using iPadOS or iOS Safari</li> <li>b439-disable-download-when-no-pdf-available - improved behavior of the \u201cDownload set\u201d drop-down button</li> <li>b407-all-questions-published-when-publishing-one - corrected version history page to display accurate data when viewing past question versions</li> <li>b434-ui-redesign-tweaks - additional tweaks to the user interface</li> <li>b442-bring-back-sets-download-options - restored \u201cSet Download\u201d drop-down button that was removed during the UI redesign</li> <li>b447-aws-db-backups-stopped - fixed the database backup issue</li> <li>b451-display-modules-even-if-sets-are-hidden - updated the system so students can view their modules, even if no sets are available</li> <li>b146-email-recap - add weekly email recap</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240910","title":"Release 2024/09/10","text":"<ul> <li>b413-authentication-failures - code cleanup for updated authentication to avoid logging out (see b413 in 2024/08/23)</li> <li>b438-enable-canvas-for-all-users - enabled canvas for all users (see b343 in 2024/06/28)</li> <li>b441-custom-milkdown-theme-bug - adjusted the new Milkdown theme to improve compatibility with math mode</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240904","title":"Release 2024/09/04","text":"<ul> <li>b441-revert-to-nord-theme - undo milkdown theme switch until math block is fixed</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240903","title":"Release 2024/09/03","text":"<ul> <li>b386-provide-drop-down-list-for-ra-default-lambda-function</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240902","title":"Release 2024/09/02","text":"<ul> <li>b398-upgrade-student-module-list - add new card view to module list student page</li> <li>b430-fix-redesign-bugs - add proper error handling to teacher pages when not a teacher</li> <li>b435-remove-env-from-git - remove .env file from git as it shouldn't be checked into vcs</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240828","title":"Release 2024/08/28","text":"<ul> <li>b383-check-for-existing-module-name - Check if module name already exists when creating new module</li> <li>b383_do_not_query_without_module_id - do not run query from header without module id </li> <li>b393-canvas-stored - Store canvas in database</li> <li>b425-studentmodule-and-studentmodules-api-retrieve-hidden-sets - do not retrieve hidden sets for students</li> <li>b427_display_pdf_errors_after_set_publish - Display PDF error message when publishing whole set</li> <li>b428-bulk-rollover-qa-comments - updates to bulk rollover feature</li> <li>b431-add-sentry - Add Sentry error monitoring</li> <li>b433-not-possible-to-enrol-students - fix to student enrollment</li> <li>b436-fix-instance-swapper - fix instance swapper</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240823-2","title":"Release 2024/08/23 - 2","text":"<ul> <li>b380-fix-tables - Table paging fix</li> <li>b379-disappearing-r-at-beginning-of-line-milkdown - fix disappearing 'R' at beginning of line milkdown</li> <li>b413-force-renew - authentication update to avoid logging out</li> <li>b429_deleting_cases_and_tests_qa - ensure feedback case order is maintained</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240823","title":"Release 2024/08/23","text":"<ul> <li>b413-grounded - updates to the authentication flow</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240822-2","title":"Release 2024/08/22 - 2","text":"<ul> <li>b402-create-table-view - table view added to the teacher module view</li> <li>b406-prepare-ui-update - changes in the user interface</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240822","title":"Release 2024/08/22","text":"<ul> <li>b356-delete-all-questions-in-set-not-handled-gracefully - create blank question when all others are deleted</li> <li>b409-module-bulk-rollover - bulk creation of new module instances (for admins)</li> <li>b417-set-part-to-a-when-switching-question - in teacher mode, when switching a question, always display part (a)</li> <li>b419-deleting-case-is-causing-feedbacks-to-shift-incorrectly - display correct remaining cases after deleting one</li> <li>b423-pdf-generation-displays-different-error-first-time - first PDF generation message consistent with follow up messages</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240807","title":"Release 2024/08/07","text":"<ul> <li>b375-correct-tex-and-pdf-for-solutions - PDF generation improvements</li> <li>b412-export-import-wrong-order-of-parts - ensure correct part and response-area order when importing</li> <li>b413-authentication-failures - part 2 - prevent application logouts</li> <li>b416-download-set-as-json-downloads-wrong-set - ensure correct set when exporting JSON</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240802","title":"Release 2024/08/02","text":"<ul> <li>b391-milkdown-fault-if-latex-on-last-line - fixed milkdown bug - if text ends with latex, it rendered raw but now renders properly.</li> <li>b415-guidance-time-suggestion-feature-by-colin - guidance time suggestion (using Machine Learning based on our database of content)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240726","title":"Release 2024/07/26","text":"<ul> <li>b369-guidance-text-on-enrollment - add a guidance text when adding students or teachers</li> <li>b381-not-possible-to-add-multiple-teachers - allow to add multiple teachers</li> <li>b410-open-doc-links-in-new-tab - open external links in a new tab</li> <li>b413-authentication-failures - fixed problems with application logouts</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240724","title":"Release 2024/07/24","text":"<ul> <li>b378-admin-module-instance-breadcrumbs-incorrect - corrected the application header (for Admin module instance page)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240723","title":"Release 2024/07/23","text":"<ul> <li>b132 - security update for milkdown</li> <li>b366-non-imperial-users-to-be-able-to-log-in - non-Imperial logins enabled</li> <li>b374-admin-analytics-evaluation-functions - analytics for evaluation function errors</li> <li>b387-import-question-does-not-import-worked-solutions - corrected question export with solutions</li> <li>b388-tutorials-and-solutions-steps-messed-up - corrected sorting of solution branches</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240718","title":"Release 2024/07/18","text":"<ul> <li>b392-essay-code-ras - add two new response area types: code and essay</li> <li>b396-better-feedback-box - improve the feedback box UI and accessibility</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240716","title":"Release 2024/07/16","text":"<ul> <li>b395-vertical-text-align - remove excess margin from top part of question</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240628","title":"Release 2024/06/28","text":"<ul> <li>b258-performance-analyse-db - faster response from DB queries</li> <li>b343-canvas - student canvas in beta mode (hidden by default)</li> <li>b351-teacher-module-page-ui-upgrades - Teacher module home page UI upgrades (tabs added)</li> <li>b357-upgrade-aws-sdk-v2-to-v3 - software library updates</li> <li>b359-generate-tex-file - upload/download whole Sets as LaTeX files</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240614","title":"Release 2024/06/14","text":"<ul> <li>b305-export-whole-set - import/export whole sets</li> <li>b334-upgrade-node-next-nest - library updates</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240529","title":"Release 2024/05/29","text":"<ul> <li>b352-support-to-eval-function-20-get-all-routes-by-getroutes - evaluation function deployment (check existing routes)</li> <li>b353-question-import-filter-out-unicode-characters - remove character (U-2006) on json import</li> <li>b354-double-confirm-on-confirmation-pop-ups - fix confirm button on modals in teacher mode</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240524","title":"Release 2024/05/24","text":"<ul> <li>b97-remove-all-references-to-mongodb - remove code linking to legacy databases</li> <li>b243-add-question-id-to-the-url - add question identifier to the url</li> <li>b285-move-module-instance-drop-down-to-left-to-replace-the-instance-label - more user-friendly module instance selection</li> <li>b304-milkdown-element-in-admin-that-will-display-on-home-page - add an administrator page to configure a home page banner</li> <li>b330-modal-update - question version switch: allow teacher either to save or discard existing draft</li> <li>b349-support-to-eval-function-20-ensure-deployments - evaluation functions: production and non-production versions</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240515","title":"Release 2024/05/15","text":"<ul> <li>b250-expression-ra-scan-mode-copy-and-paste - allow copy and paste and other improvements in the scan mode functionality</li> <li>b314-remove-experimental-from-ra-panel - \"experimental\" from the photo upload and handwritting labels removed</li> <li>b338-create-a-new-set-when-creating-a-module - a default set automatically creating when a new module is created</li> <li>b342-do-not-generate-pdf-when-creating-new-question - prevent PDF generation when a new question is added</li> <li>b345-after-pdf-generation-extraction-cleanup - a technical improvements into the PDF generation</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240426","title":"Release 2024/04/26","text":"<ul> <li>b230-pdf-generation-in-a-separate-lambda-function - PDF generation is faster, more secure, and frees up bandwidth on the main server.</li> <li>b328-improve-expression-ui-in-tests - fix expression response area preview in tests tab</li> <li>b331-work-solutions-empty-content-not-handled-as-no-work-solutions - work solutions and structured tutorials buttons not to be displayed if the content is empty</li> <li>b339-accessible-response-area-feedback - displaying error returned by evaluation functions in a user-friendly format</li> <li>b340-remove-input-type-changed-warning-on-new-ra - do not display warning about response area type change for new response areas</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240419","title":"Release 2024/04/19","text":"<ul> <li>337-individual-tests-always-fail - fix individual reponse area test runs</li> <li>327-consolidate-response-area-components - improvements for better response area consistency</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240321","title":"Release 2024/03/21","text":"<ul> <li>b332-table-smart-resizing - resize table width based on the screen size</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240319","title":"Release 2024/03/19","text":"<ul> <li>b286-ra-analytics-when-config-is-changed - Fix aggregates in stats</li> <li>b311-expression-area-layout-issues - Improve expression RA layout</li> <li>b325-populate-new-tests-with-the-answer - Populate new tests with the answer</li> <li>b322-enable-live-preview-in-teacher-mode - Attempt to enable live preview in teacher mode</li> <li>b35/b329-simplify-response-components - Simplificaton of Response Type components code</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240313","title":"Release 2024/03/13","text":"<ul> <li>b84-legacy-content-db-tables - DB updates. No change to UX</li> <li>b315-include-answer-when-importing-case - ensure the answer value is included when importing a case from stats</li> <li>b320-response-type-allowlist - improves modular response areas</li> <li>b325-populate-new-tests-with-the-answer - prepopulate the answer by the correct answer when creating a new test</li> <li>b326-question-alignments - imroves alignments on the edit question page</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240308","title":"Release 2024/03/08","text":"<ul> <li>b283-table-with-1-column-layout - improves table layout</li> <li>b324-show-required-error-on-number-input-wizard - improves number input validations</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240305","title":"Release 2024/03/05","text":"<ul> <li>b301-redesign-part-response-areas-and-text-between-them - teachers can drag response areas while surrounding text stays in place, and merges where necessary.</li> <li>b35/b310-modular-response-areas-phase-6-cleanup - completes modular response areas. Code improvements and removing legacy tables.</li> <li>b323-delete-empty-answer-in-ra-panel - ensure delete works in answer box in response area panel</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240304","title":"Release 2024/03/04","text":"<ul> <li>b319-survey-promotion-banner-on-home-page - add a banner onto the landing page advertising a survey with a link</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240301","title":"Release 2024/03/01","text":"<ul> <li>b287-limit-access-to-sets-published-outside-of-current-date - ensure access to Sets follows release rules, including via URL</li> <li>b303-redirect-help-to-userdocs - redirect lambdafeedback.com/help to user documentation and lambdafeedback.com/[module slug] to the module page</li> <li>b318-url-for-survey - redirect lambdafeedback.com/survey</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240229","title":"Release 2024/02/29","text":"<ul> <li>b35/b308-modular-response-areas-phase-4-custom-response-types - allow admin to dynamically create and manage new response types</li> <li>b35/b309-modular-response-areas-phase-5-migration - migrate all existing response types to the new modular type</li> <li>b313-always-display-post-ra-text-in-pdf-but-not-in-stats-mode - include all text in PDF (including after first response area)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240226","title":"Release 2024/02/26","text":"<ul> <li>b35-number-input-nan - fix handling of non-number input in the number answer wizard</li> <li>b317-no-header-refetch-on-mount - avoid unwanted refetch when resizing browser window on a set page</li> <li>b35/b307-modular-response-areas-phase-3-all-writes - start writting new and edited Response Area's Response to the new modular table</li> <li>b274-when-deleting-a-question-display-loading-message - display \"loading\" message when deleting a question</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240220","title":"Release 2024/02/20","text":"<ul> <li>b290-the-final-answer-button-is-displayed-even-if-there-is-no-final-answer - fix: only display 'final answer' button when there is content to show</li> <li>b297-give-error-if-creating-module-with-same-name-as-deleted-module - improved formatting of error messages</li> <li>b299-legacy-content-db-tables-ra-contents - DB updates. No change to UX.</li> <li>b302-modal-warning-before-disabling-branching - warning modal when disabling branching in worked solutions and structured tutorials</li> <li>b35/b306-modular-response-areas-phase-2-new-modular-type - backend updates for modular response areas.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240215","title":"Release 2024/02/15","text":"<ul> <li>b35/b295-modular-response-areas-phase-1-switchless-frontend - a technical improvement to the response area building blocks in the code, so that it is easier, more intuitive and more straight forward to add new response areas</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240213","title":"Release 2024/02/13","text":"<ul> <li>b271-unify-modals - unified modals to use same style</li> <li>b294-check-imports-from-material-ui - prevent importing whole library when importing an icon</li> <li>b300-delete-ra-add-warning-into-the-modal-that-the-text-below-the-ra-will-be-deleted-as-well - when deleting a response area (RA), warning modal that text below RA will also be deleted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240125","title":"Release 2024/01/25","text":"<ul> <li>b240-structured-tutorial-component-upgrade - converted structured tutorial to use the same structure and logic as worked solutions</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240124","title":"Release 2024/01/24","text":"<ul> <li>b273-limit-access-to-unpublished-sets - ensure no student access to hidden sets via a url</li> <li>b277-milkdown-first-non-markdown-update-is-ignored - milkdown fix to for edge cases that were not saved (single character; deleting selection).</li> <li>b279-table-with-1-column - wider columns for table response areas with one column</li> <li>b280-change-response-colour-to-white - specifically for 'riskAssessment' evaluation function: display feedback for incorrect answer in white colour</li> <li>b281-tweaks-to-ra-analytics - tweaks to response area analytics</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240116","title":"Release 2024/01/16","text":"<ul> <li>b272-legacy-db-tables-tutorial-sections - refactoring the database. No change to UX.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240110","title":"Release 2024/01/10","text":"<ul> <li>b264-untangle-changes - a technical improvement to make the milkdown wrapper code clearer.</li> <li>b247-re-generate-pdf-after-deleting-a-question - an improvement so that the PDF is automatically re-generated when a published question is deleted</li> <li>b158-change-prod-bucket-to-prod-not-staging - a technical change so that imported images and generated PDF files are saved in the correct AWS bucket dependently on the environment (production, staging or development)</li> <li>b232-ra-analytics-visual-alignment - a change to display response area analytics correctly aligned with labels</li> <li>b77-published-question-change-of-input-type - an improvement to allow changing of the input type on the response area that was already published.</li> <li>b262-legacy-content-db-tables-part-contents - refactoring the database. No change to UX.</li> <li>b245-question-numbering-is-sometimes-wrong-on-the-student-module-home-page - a correction so that question numbers are reconciled after a question is deleted</li> <li>b141-update-link-in-modal - a correction of the link from the modal (which appears when deleting a response area) to the user documentation</li> <li>b211-response-area-preview-remove-border - a change in the question preview in the teacher mode so that it is displayed in the same way as in the student mode</li> <li>b103-milkdown-slow-rendering - a technical change to speed up testing in local development environments</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231215","title":"Release 2023/12/15","text":"<ul> <li>b103-milkdown-slow-rendering - developers can set a flag in local environment to speed up rendering pages with milkdown</li> <li>b235-content-with-hash-copied-across - prevent milkdown copying content with hash from one question to another</li> <li>b244-fix-notes-saving-in-the-student-mode - ensure student notes are visible including when switching from teacher to student mode</li> <li>b248-remove-unwanted-content-from-pdf - removed legacy response area pre-text and post-text from PDFs</li> <li>b251-post-a-reply-in-one-click - post a reply to a comment with one click</li> <li>b256-include-frequency-data-when-downloading-csv - correction to csv file generation for question stats, to include question numbers and frequency</li> <li>b260-number-and-unit-ra-do-not-align-with-pre-text-in-student-mode - align pre-text in the response area with number and units in student mode</li> <li>b261-master-content-sometimes-not-saved - ensure master content entered by the user is saved after publishing a question (not copied from the published version)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231208","title":"Release 2023/12/08","text":"<ul> <li>b246-rendering-of-list-of-sets-in-teacher-mode-takes-long-time - an improvement to render list of sets in teacher mode quicker</li> <li>b255-recover-lost-marked-parts - further corrections to DB. Some question parts were not marked correctly as DONE for questions imported from JSON between 13/10/23 and 5/12/24.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231205","title":"Release 2023/12/05","text":"<ul> <li>b242-mark-as-done-copied-across-questions - correction to DB submissions for questions imported from JSON between 13/10/23 and 5/12/24, which were linked together incorrectly.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231204","title":"Release 2023/12/04","text":"<ul> <li>b224-add-guidance-to-help - guidance on a question, already visible to users in a widget on top-right, is now also visible with the support material below the question   </li> <li>b228-legacy-content-db-tables-master-content - refactoring the database. No change to UX.</li> <li>b109-expression-input-tweaks - tweaks to the few improvements in the expression response area (555 in 2023/05/26): icons, placeholder, upload size limit.</li> <li>b249-selected-question-index-lost - editor UX, improve the robustness of: when a question is added or published, ensure that question remains in focus to the user.</li> <li>b241-link-from-feed-needs-updating - corrected a URL linking from the teacher feed to a question.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231113","title":"Release 2023/11/13","text":"<ul> <li>b227-correct-set-estimates - time format improvement for displaying time estimate for each set in the list of set</li> <li>b233-publish-set-pdf-generation - an adjustment to the Publish whole set functionality to generate PDF after the confirmation button is clicked   </li> </ul>"},{"location":"releases/relases_2023_24/#release-20231109","title":"Release 2023/11/09","text":"<ul> <li>b186-add-time-estimates-for-each-set-in-teacher-mode - added set estimates which is calculated as summary of estimates of all questions</li> <li>b204-input-symbols-empty-row-should-not-be-validated - an improvement to prevent validation of input symbols when a new row to enter input symbols is added</li> <li>b206-input-symbols-with-spaces - an improvement to remove potential spaces entered into the input symbol alternatives (the values must be seaparated by comma without spaces to make sure they work correctly)</li> <li>b226-update-question-split-prisma-transaction - extended Prisma timeout when a question is being saved or publish</li> <li>b225-bug-in-timed-release-for-pm-times - a change to display hours in 24 hour format when displaying time</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231103","title":"Release 2023/11/03","text":"<ul> <li>b214-admin-dashboard-carry-on - admin dashboard improvements:</li> <li>A drop down list to select the time period for the user access events graph</li> <li>The last part of the graph lines are dotted to make clear that last values are subject to change</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231101","title":"Release 2023/11/01","text":"<ul> <li>b207-pressing-enter-in-the-flag-textbox - an improvement so that when a user is using an expression response area and he attempts to submit a comment (or flag a problem) at the same time by clicking the enter, then only the comment (or the problem message) is submitted (and not the answer in the response area)</li> <li>b213-question-export-import-to-handle-mp3 - an improvement to allow to export and import questions containing an audio (or more audios)</li> <li>b217-remove-header-text-on-module-page-for-students - removed the header on the student module page as it is not needed</li> <li>b215-do-not-update-or-delete-notes-in-teacher-preview - an improvement to prevent submitting student solutions in the teacher preview mode</li> <li>b208-unposted-comments - an imrovement to handle the scenario when a user enters a comment and then, withouth submitting it, selects different question (the comment was copied to the newly selected question which is not a desired feature)</li> <li>b209-zero-comments-invite-comments - an improvement to open comments when there are no comments to invite users to comment</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231020","title":"Release 2023/10/20","text":"<ul> <li>b202-ensure-eval-function-defaults-for-new-response-areas - an improvement so that evaluation function parameters are set to default values when creating a new response area</li> <li>b71-analytics-tweaks-teacher-view - the students list, view and contact pages were merged into a single page: - Filters by email and/or by access are available to filter the single list of students - A click on a student email opens a view which displays the same analytics the student can see   </li> <li>b162-analytics-tweaks-stats-modal - improvements in the analytics view: - Colour is indicating the answer's case colour, if any, or the correct/incorrect default colour - Checkmark is indicating that the answer was correct - More options added to allow the user to agregate student answers   </li> <li> <p>b67-simplify-stats-interaction - few changes to response area statistics in the teacher mode:</p> </li> <li> <p>The case is imported straight into the relevant response area</p> </li> <li> <p>The response area menu has a new button EXPLORE so that the teacher can see the statistics per response area     </p> </li> <li> <p>b192-reaction-count-one-hour-challenge - users can see the individual count of each type of reaction</p> </li> </ul> <p></p> <ul> <li>b183-activity-feed-make-clear-there-are-more-flags-than-5 - make clear to the user how many flags and comments there are in total as there might be more than 5 displayed on the teacher dashboard</li> </ul> <p></p> <ul> <li>b110-import-multiple-jsons-from-a-single-zip - allows to import more questions from one zip file. This includes questions with attached pictures. Import of questions with attached audio files is yet to come.</li> <li>b205-admin-analytics-initial-work - first version of the admin dashboard is now provided. It includes information about number of current users, questions and user access events</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231012","title":"Release 2023/10/12","text":"<ul> <li>b180-prod-freezing-and-restarting - increasing allocated memory to accommodate multiple users triggering heavy processes (PDF compilation)</li> <li>b193-implement-auto-scaling-on-infrastructure - Infrastructure upgrades for larger scale usage.</li> <li>b124-question-export-with-pictures-fails-sometimes-on-cors-error - forcing Chrome to refresh media retriaval from S3 bucket to make sure correct headers are attached to the response</li> <li>b199-migration-script-for-physics-expression-ra - DB migration for legacy content.</li> <li>b194-create-set-and-first-question-improvement - new set automatically has a blank question ready.</li> <li>b197-not-possible-to-delete-a-question - increase timeout when deleting a question</li> <li>b143-more-info-in-modal-when-publish-whole-set - displaying list of questions that will be published in a modal before publishing whole set.</li> <li>b144-modal-to-check-before-removing-branches - a warning message is displayed before a branch from worked solutions is deleted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231005","title":"Release 2023/10/05","text":"<ul> <li>b136-change-to-breadcrumbs - an improvement to remove module instances from teachers and students breadcrumbs as they do not link to any pages</li> <li>b188-add-information-when-rendering-a-new-question - adding information that a question is being created when adding a new question</li> <li>b189-failed-fetching-your-problem-set-message-appearing-when-it-should-not - a warning message 'Failed fetching your problem set' is to be displayed only if there is an error</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231003","title":"Release 2023/10/03","text":"<ul> <li>b191-expression-response-area-defaults - an improvement so that when creating a new response area of type EXPRESSION, the default values are set to:</li> <li>TRUE for Live preview</li> <li>FALSE for Display input symbols</li> <li>FALSE for Include in PDF</li> <li>TRUE for Enable handwriting input</li> <li>TRUE for Enable photo upload</li> <li>b187-support-materials-access-enhancements - enhancements to the support materals student access configuration:</li> <li>A new button event was added to record whether students proceeded or cancelled after a warning message appeared when a student tried to open a support material</li> <li>Labels were renamed to make their meaning clearer (e.g. 'Open' was changed to 'Available' and 'Hidden' to 'Unavailable')</li> <li>When a question part is marked as done, then no warning is displayed to a student when opening a support material (even if marked as Open with warnings)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230929","title":"Release 2023/09/29","text":"<ul> <li>b148-problem-adding-new-question-after-changing-name-of-current-question - an improvement so that a user cannot start changing newly added question (e.g. changing name) until all processes are finished and therefore preventing these changes to be wiped out.</li> <li>b161-renaming-question-straight-after-making-it - this is the same problem as b148</li> <li>b151-quote-marks-can-break-flags - an improvement so that double-quote marks, if used in a text, are displayed correctly in the generated csv file</li> <li>b164-grade-param-type-changed-reverts-to-string-when-value-is-empty - an improvment to identify a number as a number in the grade parameters, so that the type is displayed number and not as string</li> <li>b166-no-template-questions-in-the-list - an improvement to display all existing template questions in the list (when adding a new question from a template)</li> <li>b190-draw-area-width-keeps-changing - an improvement to stop the drawing area changing its width when a warning message is displayed that the writting cannot be interpreted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230927","title":"Release 2023/09/27","text":"<ul> <li>b157-new-eval-function-reset-parameters - improvement in the response area panel, when the evaluation function is changed, then the default evaluation function parameters are re-set.</li> <li>b167-teachers-are-sent-to-the-most-recent-instance-on-the-module-homepage-even-when-they-dont-have-access-they-should-be-sent-to-the-most-recent-one-that-they-have-access-to</li> <li>b163-failed-fetching-your-problem-set-displayed-on-every-page-load - an improvement so that the warning message only appears when the fetch returns an error.</li> <li>b149-restrict-access-to-worked-solutions - restrict student access to support materials on set level and on question level.   </li> <li>b165-preview-not-the-same-as-student-view - an improvement to displaye pre-text, value and post-text aligned horizontally in the response area student view</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230908","title":"Release 2023/09/08","text":"<ul> <li>128-feedback-area-does-not-support-latex-rendering - Feedbacks returned by the evaluation function are displayed using latex editor.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230907","title":"Release 2023/09/07","text":"<ul> <li>b155-aws-ending-support-for-nodejs-14-in-aws-lambda - A clear-up of an outdated library.</li> <li>b153-pressing-enter-in-a-number-response-adds-new-line-to-the-response - Handle Enter in the response area as a submission of the answer.</li> <li>b139-archive-feature-enhancements - Enancements of module as module instance archiving.</li> <li>b68-cleaning-up-the-editor - many ui enhancements in the question editing page</li> <li>b115-case-color-under-feedback-tab-for-response-area-is-not-functional - The custom colour for feedbacks is now displayed correctly.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230830","title":"Release 2023/08/30","text":"<ul> <li>b33-audio-clips - in the content editor, drag-and-drop an audio file, and it will add a sound (e.g. narration) to the content.</li> <li>b145-xetex-pdf - PDFs are now compiled with xelatex, not PDFlatex.</li> <li>b150-extracting-code-from-listener-into-callback-fn - stats for typed expressions now record full submissions only (not keystrokes)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230822","title":"Release 2023/08/22","text":"<ul> <li>b147-time-guidance-is-currently-very-small - An adjustment after upgrading one of the libraries which caused the time guidance to shrink.</li> <li>b142-module-clone-enhancements - An enhancemnt to include links to already generated PDF files for all sets in the cloned module instance.</li> <li>b138-503-error - An enhancement to navigate to the teacher module / module instance after clicking Cancel button in the Set Metadata page.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230818","title":"Release 2023/08/18","text":"<ul> <li>b114-matrix-input-centering-in-teacher-mode-but-not-in-student-mode - The Check button for matrix questions in the response area panel is now vertically centred in the student view.</li> <li>b140-response-area-pre-text-doubled - The legacy response area pre-text was removed from the student view.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230816","title":"Release 2023/08/16","text":"<ul> <li>b127-cloned-instances-are-missing-tutorials-and-worked-solutions - An enhancement of the module cloning functionality to include worked solutions and tutorials.</li> <li>b125-when-publishing-question-update-the-student-view - An enhacement so that when a teacher publishes a question then, this question is visible in the student view without having to refresh the browser or log out and back in again.</li> <li>b83-revisit-set-archiving - This is a technical improvement of the existing functionality to archive sets so that it is done in the same way as archiving of other entities. It has no visible any impacts to a user.</li> <li>b111-archive-module-instance-option - A new feature to allow to archive a module instance. This feature is only available to an administrator.</li> <li>b126-archive-module-option - A new feature to allow to archive a module. This feature is only available to an administrator.</li> <li>b108-error-when-clicking-add-question-button-while-inside-part-content-box - Technical improvement. Upgrade of some libraries (Material UI) to prevent errors caused by issues in the older library version.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230721","title":"Release 2023/07/21","text":"<ul> <li>b101-tests-run-from-the-configure-panel-have-the-islatex-parameter-set-to-true - A correction to the settings on the new Expression input (see 555 in 2023/05/26). When calling an evaluation function, the <code>is_latex</code> parameter dependends on the type of input (type/draw/scan).</li> <li>b120-PDF-skill-time-info - PDFs now include information on skill level, time estimates, and guidance below the question title and above the question content.</li> <li>b122-multi-year-carry-on - extended UI features referring to module instances (see b82 below).</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230719","title":"Release 2023/07/19","text":"<ul> <li>b82-multi-year-duplicate-module-instance-and-link-entities - new feature to clone module instances   </li> <li>b118-multi-year-tidy-up - multi module feature enhancements such as sorting and filtering module instances on the admin Module page</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230714","title":"Release 2023/07/14","text":"<ul> <li>b72-multi-year-module-instances-introduction - All Modules now exist as an 'Instance' of a Module, in preparation for allowing multiple Instances. The UI navigation is updated to handle Module Instances.</li> <li>b81-show-preview-of-ra-in-input-type-select - Selecting an Input Type for a Response Area: a searchable preview of Input Types improves the UX: users see the preview while selecting.   </li> <li>b91-prevent-multiple-blank-questions - When a question is added, the 'add quesiton' button is temporarily disabled while the application updates.</li> <li>b112-bug-the-tab-navigation-bar-at-the-top-disappears - Editor tabs are pesistent including during keyboard navigation</li> <li>b116-pdf-display-between-ras - PDF generation: for multiple Response Areas in a Part, the order is now always correct</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230622","title":"Release 2023/06/22","text":"<ul> <li>b39-new-editor-menus - question editor area menus have been converted into tabs. Other improvements have also been made to the editor layout inlcuding switching between teacher and student mode and staying on the same question. </li> <li>b62-add-tabs-to-reponse-area-panel - the Response Area panel is grouped into tabs that aid navigation and encourage a workflow that matches the way teachers think. Other layout improvements were also made within the tabs. </li> <li>b79-input-type-on-published-ra-should-not-be-editable - input type cannot be changed after publishing (see 598 here 2023/06/05).</li> <li>b85-incorrect-required-error-message - enhanced validation for number 0 in numeric response area.</li> <li>588-question-import-export-handle-images - import export includes images; a zip file is used to combine the JSON and the images.</li> <li>601-parameter-defaults-for-an-eval-function-cpq - improved the appearance of boolean evaluation function parameters.</li> <li>603-user-docs-updates - user documentation repo renamed from \"documentation\" to \"user_documentation\".</li> <li>608-link-word-sign-in-to-sign-in-on-homepage - on the home page 'sign in' text is now a link to sign in.</li> <li>619-mcq-check-button-should-be-vertically-central - the Check button for multi-choice questions in the response area panel is now vertically centred.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230605","title":"Release 2023/06/05","text":"<ul> <li>598-published-questions-change-of-approach - questions are now fully editable after publishing. All data from student responses persists through these changes. One exception is that the input type of a response area cannot be changed after publication, because this would change the format of the data that is recorded (you can, however, delete the response area and create a new one instead). Other new features: duplicate a Response Area; reorder Response Areas using drag and drop (in a similar way as reordering Parts).</li> <li>613-enable-publish-whole-set - see 606 below (2023/05/26). The 'Publish Whole Set' button is now enabled.</li> <li>614-error-with-stats-on-dev - ensures statistics still work with the new handwriting input (see 555 below).</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230526","title":"Release 2023/05/26","text":"<ul> <li>555-handwriting-response-area-upgrades - A new version of the Expression input type is in use. Input by handwriting onscreen or with scanned images is an option for teachers to make available to students (default: off). Also, regardless of the input mode (type/draw/scan) the live preview now gives 'pre-submission' feedback on whether the response can be interpreted, and the Check button is only available if interpretation is successful.</li> <li>606-publish-whole-set-causing-stats-to-disappear - The 'Publish Whole Set' button in Teacher Edit mode has been disabled because it was causing data to become unlikned in the DB, giving the effect of data like number of completed parts 'disappearing'. Existing data has now been relinked and is all visible to users. The feature that caused the problem has been disabled while we prepare a replacement to be pushed shortly.</li> <li>612-whole-part-marked-as-done-with-more-response-areas - Student functionality. If a question part has multiple Response Areas, the logic is now that only if all Response Areas are correctly answered will the 'Mark as done' feature be automatically checked. Previously only one correct answer was required to trigger this effect.</li> <li>585-question-simple-import-and-export - Teacher functionality. The import/export functionality has been enhanced so that it Response Area parameters, cases, and tests are now all included.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230321","title":"Release 2023/03/21","text":"<ul> <li>571-simple-teacher-comment-feed - Teacher functionality. New 'Activity feed' (formerly 'Flagged Questions') contains flagged questions and comments. The teacher can filter the table to see e.g. only flags or only comments. The teacher can also sort the table e.g. to see the new activities first.</li> <li>569-numeric-input-strips-out-strings-that-may-have-meaning - Technical dept. For the Response Area input type 'Number', additional validation added; if the input contains a non-numeric value then a relevant error message is displayed to the user (this is linked to the 573-response-area-validation-specific-errors below).</li> <li>573-response-area-validation-specific-errors - Teacher and student functionality. More specific error messages are displayed when the user inserts a value in an incorrect format (e.g. a non-numeric value into the input that expects a number).</li> <li>582-empty-structured-tutorial-shouldnt-display - Technical debt. When a tutorial is deleted, it is not displayed at all to students (as opposed to being blank).</li> <li>585-question-simple-import-and-export - Teacher functionality. Export a question to a file in JSON format. Import a question from a file in JSON format. Images are not imported/exported - these need to be handled manually until a new feature is ready. This feature opens the door to file imports if content can be converted into the correct format.</li> <li>586-question-import-add-schema-validation - Teacher functionality. When importing a question from a file, the data structure and format is validated. If the validation fails then relevant error messages displayed to the user.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230306","title":"Release 2023/03/06","text":"<ul> <li>566-pdf-error-identification - Teacher functionality. When a PDF fails to compile, the location of the error source is given in more detail, e.g. 'Q2(c)'.</li> <li>576-orderedsetids-throws-error-in-main - Technical. When loading sets in a module on the teacher side, an error no longer appears in the console.</li> <li>522-adding-teacher-when-creating-module-inadequate-error-message - Admin functionality. When adding a new module, a teacher can be added simultaneously. If the proposed teacher is not already registered as a teacher, then they are now automatically created as a teacher and a confirmation message is displayed.</li> <li>524-remove-teacher-from-list - Admin functionality. Remove a teacher from the list of teachers. If the teacher is still a teacher on a module, then display a modal confirming which modules the teacher will be removed from. If the user confirms, the teacher is removed from all the modules and then they are deleted from the list of teachers.</li> <li>572-comment-upvote-tweeks - Teacher and Student functionality. Right margin on the comments tweaked so that the sorting feature and 'post' button are not too far away from each other.</li> <li>483-show-all-button - Teacher functionality. The Show All feature is now enabled in the question preview mode.</li> <li> <p>520-default-to-an-eval-function-after-selecting-the-response-area - Teacher functionality. In the Response area edit panel, automatically select a default eval function as follows (it can be edited by the teacher if necessary). The default selections are:</p> Response area Default evaluation function MCQ arrayEqual NUMERIC isSimilar Expression and Text symbolicEqual Table and Matrix arraySymbolicEqual NUMERIC_UNITS comparePhysicalQuantities </li> </ul>"},{"location":"releases/relases_2023_24/#release-20230210","title":"Release 2023/02/10","text":"<ul> <li>560-comment-feature-tweaks - UI improvements to the comments feature.</li> <li>550-comment-upvotes - users can upvote comments by clicking on the heart. Sorting by upvotes is default</li> <li>546-always-test-a-response-area-when-saving - [For teachers only] this is an invisible feature that automatically tests a response area, when closing the editing panel, to check that the correct answer is accepted as correct. A failure to pass this test will show an error and not save the response area until fixed. The reason for this feature is to catch things like empty cells in the teacher answer which, e.g. for a matrix, would make marking student answers impossible. If such errors were allowed to pass, then students would experience errors when using the response area - hence it cannot be allowed. The auto-test feature is not enabled for all response areas as some are not compatible - the option can be enabled/disabled for each response area by admins.</li> <li>568-numeric-input-expects-string - upgrade to the numeric input type when dealing with string inputs.</li> </ul>"},{"location":"releases/status/","title":"Status","text":"<p>Lambda Feedback is a cloud-native application that is available with full service 24/7.</p> <p>This page contains information about any known incidents where service was interrupted. The page begain in November 2024 following a significant incident. The purpose is to be informative, transparent, and ensure lessons are always learned so that service improves over time.</p> <p>The Severity of incidents is the product of: </p> <ul> <li>number of users affected (for 100 users, N = 1), </li> <li>magnitude of the effect (scale 1-5 from workable to no service), </li> <li>duration (in hours). </li> </ul> <p>Severity:</p> <ul> <li>x &lt; 1       is LOW</li> <li>1 &lt; x &lt; 100 is SIGNIFICANT</li> <li>x &gt;     100 is HIGH. </li> </ul> <p>The severity is used to decide how much we invest in preventative measures, detection, mitigation plans, and rehearsals.</p>"},{"location":"releases/status/#2025-december-4th-brief-db-outage-severity-low","title":"2025 December 4th: Brief DB outage (Severity: LOW):","text":""},{"location":"releases/status/#timeline-gmtutc","title":"Timeline (GMT/UTC)","text":"<p>10:58 DB was unavailable for a few seconds, affecting about 20 users (e.g. page won't load)</p> <p>11:14 Alerts automatically created</p> <p>11:15 Developers responded </p> <p>11:40 Decision and action</p> <p>11:45 Incident over</p>"},{"location":"releases/status/#analysis","title":"Analysis","text":"<p>The incident was triggered by a mistake by a developer on the DB configuration, which triggered a DB restart. Restart was successful so issues only arose during the brief restart period.</p> <p>The analysis and decision concluded that the configuration needed to be reverted, and the DB restarted again.</p> <p>The DB connections to the app remained open during the configuration change, avoiding any need for users to re-authenticate. This minimised the impact of the incident, but meant the quickest and safest response required a second restart. </p>"},{"location":"releases/status/#actions","title":"Actions","text":"<p>We have implemented protections against destructive actions on the DB, increasing barriers to this type of event.</p> <p>We have increased user security requirements to confgure the DB (this incident was not security related, but it was a useful prompt).</p> <p>Second-developer reviews are now required before any DB configuration changes are required.</p> <p>Developers should only make configuration changes when fully aware of the consequences and able to handle the process</p> <p>We have documented the error messages that correspond to this issue, to make detection faster and more accurate in future.</p> <p>N=20, effect = 4, duration = 0.01. Severity = 0.008 (LOW)</p>"},{"location":"releases/status/#2025-november-18th-some-evaluation-functions-failing-severity-low","title":"2025 November 18th: Some evaluation functions failing (Severity: LOW):","text":"<p>Some evaluation functions returned errors.</p>"},{"location":"releases/status/#timeline-uk-gmt","title":"Timeline (UK / GMT)","text":"<p>The application was fully available during this time period.</p> <p>2025/11/18 21:18 GMT: some but not all evaluation functions (external microservices) failed. Investigation initiated and message added on home page 2025/11/18 21:39 GMT: home page updated to users that the cause was identified. 2025/11/18 21:45 GMT: issue resolved. Home page updated.</p>"},{"location":"releases/status/#analysis_1","title":"Analysis","text":"<p>Some of our evaluation functions still use an old version of our baselayer, which calls GitHub to retrieve a schema and validate inputs. GitHub git services were down (https://www.githubstatus.com/incidents/5q7nmlxz30sk), which meant that those of our functions that call GitHub could not validate their schemas and therefore failed. Other evaluation functions had previously been updated to remove the need to call GitHub and were therefore not affected by the issue.</p> <p>The same root cuase meant that we could not push updates to code during the incident, due code being deployed via GitHub.  GitHub had announced they were resolving the issue, and when it was resolved our services returned to normal.</p>"},{"location":"releases/status/#recommended-action","title":"Recommended action","text":"<p>Update all evaluation function baselayers to remove dependency on external calls when validating.</p> <p>N=1, effect = 2, duration = 0.5. Severity = 1 (LOW)</p>"},{"location":"releases/status/#2025-november-10th-service-unresponsive-severity-significant","title":"2025 November 10th: Service unresponsive (Severity: SIGNIFICANT):","text":"<p>The application was unresponsive.</p>"},{"location":"releases/status/#timeline-uk-gmt_1","title":"Timeline (UK / GMT)","text":"<p>2025/11/10 14:21 Service became unresponseive, e.g. pages not loading. Reports from users through various channels. Developers began investigating, message sent to Teachers.</p> <p>2025/11/10 14:28 Service returned to normal. Home page message displayed to inform users.</p>"},{"location":"releases/status/#analysis_2","title":"Analysis","text":"<p>During the period of unresponsiveness, the key symptoms within the system were overloading the CPU of the servers. Error logging and alerts did successfully detect downtime and alert the developer team, who responded. Although developers were looking into the problem, and tried to increase resource to resolve the problem, in fact the autoscaling solved the problem itself. </p> <p>The underlying cause was a combination of high usage, leading to CPU overload. This type of scenario is normal and correctly triggered autoscaling. The issue in this case was that autoscaling should happen seamlessly, without service interruptions in the intervening period. </p>"},{"location":"releases/status/#action-taken","title":"Action taken:","text":"<ul> <li>Decrease the CPU and memory usage level at which scaling is triggered. This increases overall costs but decreases the chance of service interruptions.</li> <li>Enhance system logs so that more information is available if a similar event occurs</li> <li>Investigate CPU and memory usage to identify opportunities for improvements (outcome: useage is typical for NODE.js applications, no further action)</li> </ul> <p>N=3, effect = 5, duration = 0.15. Severity = 2.25 (SIGNIFICANT)</p>"},{"location":"releases/status/#2025-october-17th-handwriting-input-temporarily-unavailable-severity-significant","title":"2025 October 17th: Handwriting input temporarily unavailable (Severity: SIGNIFICANT)","text":"<p>Handwriting in response areas (but not in the canvas) did not return a preview and could not be submitted. Users received an error in a toast saying that the service would not work. All other services remained operational.</p>"},{"location":"releases/status/#timeline-uk-bst","title":"Timeline (UK / BST)","text":"<p>2025/10/17 08:24 Handwriting inputs ceased to return previews to the user due to a deployed code change that removed redudant code, but also code that it transpired was required. </p> <p>2025/10/17 12:20 We became aware of a problem from using the system and alerted the dev team. A response began at 12:52.</p> <p>2025/10/17 12:58 Message on home page: \"We are aware that handwriting input is not functioning. We will update this message when we have more info.\"</p> <p>2025/10/17 12:59 Code revert began. </p> <p>2025/10/17 13:07 Problem resolved. Message on home page: \"The system is now fully operational. From 08:24-13:07 UK time handwriting inputs were not working. This has been fixed and we will follow up with an investigation.\"</p>"},{"location":"releases/status/#analysis_3","title":"Analysis","text":"<p>Technically, the issue was caused by removing code that was necessary. </p> <p>Operationally, the process was as follows: - Removal of 'unused' code submitted by one dev and reviewed by another and approved.  - The code was not subject to user testing ('QA') due to no anticipated effect to test. - The code was pushed in the morning to minimise impact on users - Alerts were not monitored closely</p> <p>Post-hoc analysis shows that approximately 20 users were affected.</p>"},{"location":"releases/status/#lessons-learned","title":"Lessons learned","text":"<ul> <li>Basic QA of all changes going to PROD is necessary (on STAGING). It won't always catch problems but it will sometimes (and in this case it would have).</li> <li>Monitoring immediately after pushes, and approximately an hour after pushes, should be standard procedure.</li> <li>Integration tests would help, although they are considered outside the scope of this project at the current stage due to the resource required to continually maintain those tests</li> </ul> <p>N=0.2, effect = 2, duration = 5. Severity = 2 (SIGNIFICANT.)</p>"},{"location":"releases/status/#2025-august-27th-evaluation-functions-temporarily-unavailable-severity-low","title":"2025 August  27th: Evaluation functions temporarily unavailable (Severity: LOW)","text":"<p>The app was available and fully functional during this time and successfully called external evaluation functions. The evaluation functions managed by the Lambda Feedback team (which is most of them at the current time) became unavailable due to the API gateway of those functions being modified incorrectly. During this time, users submitting an answer on the app were given an error message.</p>"},{"location":"releases/status/#timeline","title":"Timeline","text":"<p>2025/08/26 17:54 Evaluation functions became unavailable due to a deployment error.</p> <p>2025/08/26 18:21 Message added to the home page. Fix began development and testing.</p> <p>2025/08/26 21:51 Fix is complete and home page eupdated.</p> <p>Estimated number of users affected: one. This low number was due to a quiet period in the academic year, and the rapid response to the problem.</p>"},{"location":"releases/status/#analysis_4","title":"Analysis","text":"<ul> <li>Due to evaluation functions having only one environment ('staging') that was used by both the STAGING and PROD versions of the app, changes to the staging gateway affected the production application.</li> <li>The error itself happened because an update to infrastructure included changes by a different developer that weren't noticed by the one pushing the changes</li> </ul>"},{"location":"releases/status/#lessons-learned_1","title":"Lessons learned","text":"<ul> <li>Implement independent staging and prod environments for evaluation functions (DONE as part of the fix)</li> <li>When pushing infrastructure changes, always run Pulumi preview before starting, to see if changes are already awaiting push</li> <li>Don't push infrastructure changes when no other developers are available to support any issues</li> <li>Create a feature on the app for admins to optionally declare a base URL for evaluation functions, allowing groups of evaluation functions to be rapidly redirected</li> </ul> <p>N = 0.01, effect = 3, duration = 4. Severity = 0.12 (LOW)</p>"},{"location":"releases/status/#2025-march-28th-access-blocked-within-a-particular-organisations-wifi-severity-significant","title":"2025 March 28th: access blocked within a particular organisation's WiFi (Severity: SIGNIFICANT)","text":"<p>The URL lambdafeedback.com is served by a content delivery network (CDN), that was blocked by a particular organisation's WiFi. During this period, users on that WiFi couldn't access the site.  </p>"},{"location":"releases/status/#timeline_1","title":"Timeline","text":"<p>2025/03/27 09:17 GMT We received a report that some users can't load the website at all. We announced this on the home page.</p> <p>2025/03/27 10:37 GMT The issues were identified as isolated to Imperial WiFi. Update on home page including advice to use a different WiFi (e.g. hotspot, or other location), or a different DNS. Ticket with ICT and number shared with users. There was a response within minutes requesting more information, but no further response until the next morning.</p> <p>2025/03/28 09:22 GMT Imperial ICT acknowledged that their security software had blocked the whole CDN. Lambda Feedback was specifically unblocked and full service was resumed. We have asked for a broader unblocking.</p> <p>2025/03/28 09:32 GMT Authentication services were down (no logins) but resumed within a few minutes and service remained good. We'll investigate and report.</p> <p>2025/03/28 13:47 GMT Imperial ICT confirmed a wider unblocking.</p> <p>2025/03/28 14:45 GMT Incident closed. </p>"},{"location":"releases/status/#lessons-learned_2","title":"Lessons learned:","text":"<p>Networks that provide internet access can block or incorrectly redirect users when trying to access Lambda Feedback. The block can be specific to one site or, as in this case, it can block a whole content delivery network (CDN) that serves many sites. </p> <p>Affected users will never reach the site, and we will have no way to know that they are failing to access. </p>"},{"location":"releases/status/#recommended-actions","title":"Recommended Actions","text":"<ul> <li> <p>Alert the ICT departments of key user groups to this problem, and ensure in advance that the relevent CDNs are not blocked.</p> </li> <li> <p>Create a backup plan for if the URL or the CDN are not correctly routed</p> </li> <li> <p>Monitor traffic to identify drops in usage that may indicate an issue (we already do this, but the drop was not significant enough in this case to be evident)</p> </li> <li> <p>Monitor the Lambda Feedback email address (this was effective in this case and we were in touch with users)</p> </li> <li> <p>Create a live chat with 'power users' for better communication during these incidents. A chat was started during this incident and will continue to be used</p> </li> <li> <p>Consider local WiFi/networks as a possible cause for blocking site access, and test for this cause when troubleshooting access issues</p> </li> <li> <p>Investigate the cause of the short unavailability of logins at 09:33 GMT on 28th March 2025.</p> </li> </ul>"},{"location":"releases/status/#2025-february-13th-incident-related-to-new-teacher-roles-feature-severity-low","title":"2025 February 13th: Incident related to new teacher roles feature (Severity: LOW)","text":"<p>During this period teachers were not able to access teacher pages.</p>"},{"location":"releases/status/#timeline_2","title":"Timeline","text":"<p>7:20am deployed a set of new features, including teacher roles</p> <p>9:47am issue reported - users with the TEACHER role were unable to access teacher pages.</p> <p>10:27am Issue reproduced on staging</p> <p>10:39am Issue fixed on staging; release to production initiated</p> <p>10:49am Confirmation that the fix worked in production</p>"},{"location":"releases/status/#lessons-learned_3","title":"Lessons learned:","text":"<ul> <li>Features that behave differently for users with the TEACHER role (compared to users with the ADMIN role) must be tested by a user with the TEACHER role who is not a super-admin. This is because super-admins automatically revert from TEACHER back to ADMIN. The same applies to features that behave differently for users with the STUDENT role.</li> </ul>"},{"location":"releases/status/#2024-mid-december-to-2025-january-2nd-imperial-college-security-measures-affected-logins-severity-high","title":"2024 mid-December to 2025 January 2nd: Imperial College security measures affected logins (severity: HIGH)","text":"<p>During this period the application was 100% available and operational. We were alerted on 2nd January that some users were not given permission by Imperial College London Microsoft 365 to login to third party applications. This was a a severe incident as it affected access to the application for some users.</p>"},{"location":"releases/status/#timeline_3","title":"Timeline","text":"<p>11:19 GMT We discovered the issue, escalated to Imperial College ICT and put a notice on our home page.</p> <p>16:51 GMT The application was approved by Imperial College ICT, which resolved the problem.</p> <p>Monitoring immediately after and the following morning showed that two known users who had issues no longer have issues. Other users continued to login before, during, and after the incident. No further reports of issues received. Case closed.</p>"},{"location":"releases/status/#analysis_5","title":"Analysis","text":"<ul> <li> <p>Lambda Feedback has been using Imperial College Microsoft 365 logins (Entra / Azure Active directory) since July 2021 without issues until this incident.</p> </li> <li> <p>ICT reported on 2nd January: \"due to heightened security arrangements put in place in mid December, ICT prevented unauthorised App registrations as these can pose security threats through unwarranted access to user information and the access to other systems. We have now granted access for the Lambda feedback app.\"</p> </li> <li> <p>Although the app was 'registered' on Entra/AAD in July 2021 within the Imperial College tenant, the permissions required for authentication (read the profile of the user) were granted by the user the first time they logged on. The changes imposed by ICT withdrew the privilege from users to grant such permissions, hence the inability to login. Permissions can be bulk granted by admins in the tenant, but this had not been done. On 2nd January, those permissions were given by admin and the problem was resolved.</p> </li> <li> <p>Due to the holiday season, this problem only surfaced on 2nd January, despite the cause being in 'mid-December'.</p> </li> </ul>"},{"location":"releases/status/#lessons-learned_4","title":"Lessons learned:","text":"<ul> <li>When an organisation uses single sign-on (SSO), ensure that the Lambda Feedback application permissions are granted by the organisation admin, even if the service initially works without those permissions being granted by admins. This action will protect against possible future issues, especially like the incident reported here.</li> </ul>"},{"location":"releases/status/#2024-november-4-8th-incident-related-to-new-logins-severity-significant","title":"2024 November 4-8th: Incident related to new logins (Severity: SIGNIFICANT)","text":"<p>Deployment of a new authentication process caused service interruptions. Login was not possible at certain times, affecting all users. Effects were between Monday 4th and Friday 8th November, all related to release b506.</p>"},{"location":"releases/status/#timeline_4","title":"Timeline","text":"<p>Monday 4th November:</p> <p>7:30am deployed new login system. Initially appeared OK but concerns over server loading.</p> <p>8:15am began reverting to the old system to avoid any unnecessary risks. Revert took longer than rehearsed.</p> <p>8:55am system live and operational in its reverted form. No issues hereafter.</p> <p>The issue was identified and fixed ready to push with the new login system early the next day.</p> <p>Tuesday 5th November</p> <p>6:30am deployed new login system with update to avoid errors found on previous push. Systems fine.</p> <p>9:27am first report of login issues with ipads.</p> <p>9:37am issue with Safari and iOS identified. Message update on the app.</p> <p>10:45am revert complete after efforts to implement a fix failed.</p> <p>The issue was related to blocked thrid-party cookies being on some browsers. A fix was developed to push the next day.</p> <p>Wednesday 6th November</p> <p>7:08am deployed new login system with updated auth URLs. Testing on all device types OK.</p> <p>12:00pm Two users reported issues via email. Investigations continued including emails with users.</p> <p>Thursday 7th November</p> <p>Throughout the day the problem was analysed and we found that 12 users had logged in during a brief configuration error, and as a result were not stored correctly in the new auth DB, and this caused problems. All users access was restored and they were contacted with information and an apology.</p> <p>Friday 8th November</p> <p>10:31am The app became intermittent. This was traced to an updated in the logging. Resource was increased which partially solved the problem. A deployment was pushed to solve the problem.</p> <p>11:00am (approx) the issues were fully solved.</p>"},{"location":"releases/status/#lessons-learned_5","title":"Lessons learned:","text":"<ul> <li>When practicing a revert before a deployment, make sure the exact same revert process is used in production (e.g. don't go via CircleCI if rehearsals were via GitHub). Lesson from Monday morning following a delayed revert process.</li> <li>Test changes on all device types. Lesson from Tuesday following issues with Apple devices.   If systems are used temporarily in production, check for adverse affects on any users who used both systems. Lesson from Wednesday/Thursday following issues for a small number of users.</li> <li>Ensure errors due to trivial issues, like changes to logs or failed schema in the DB, do not cause the whole app to go down. Lesson from Monday and Friday.</li> <li>Ensure adequate logging systems are in place (these have been improved already), and that a clear process is in place for users to contact the team (use lambdafeedback@imperial.ac.uk, or support@lambdafeedback.com which will redirect there)</li> <li>Ensure important messages to users can be seen, e.g. even if they can't log in.   Keep teachers informed promptly and with transparent info. Generally this protocol was followed during this incident.</li> </ul> <p>Conclusion:</p> <ul> <li>The new login system offers significant benefits, including allowing logins with a range of systems such as Google or personally created accounts. The app is now available to essentially any users.</li> <li>The deployment involved errors which caused access issues for over an hour on more than one occasion in the same week.</li> <li>Most of the outages were preventable with improved testing</li> <li>The mitigation attempts were successful in reducing the severity of the incident</li> <li>Mitigation could have been better. New logs, lower risk tolerance, and better reversion are needed in future</li> <li>Overall the level of outage is not considered acceptable and in future should be avoided</li> </ul>"},{"location":"student/","title":"Student/User Documentation","text":""},{"location":"student/#question-structure","title":"Question structure","text":"<p>The image above shows an example question, with numbers to indicate:</p> <ol> <li>Breadcrumbs showing location</li> <li>Name of the Problem Set</li> <li>PDF version (link)</li> <li>Names of the questions in the Set, indicating which question is open</li> <li>Question number and name</li> <li>Guidance (expands on hover)</li> <li>Master content (always visible to student)</li> <li>Part selection (tabs)</li> <li>Part content (only visible when relevant part is open - (a),(b), etc.)</li> <li>Response area, where student responses are entered and feedback is given</li> <li>Feedback to the teacher (currently in flux regarding the design - 02/07/25)</li> <li>Access to content 'below the line' providing extra support.</li> <li>Workspace - Opens tab with canvas and chat</li> <li>Comments</li> </ol>"},{"location":"student/#below-the-line","title":"Below the line","text":"<ul> <li>Structured tutorial - teachers use this in different ways. It is generally a way to provide scaffolding if you're struggling.</li> <li>Final Answer - warning, don't ever look at the answer before you make your own genuine attempt at answering the question.</li> <li>Worked solutions - warning, don't ever look at the solutions before you make your own attempt. If necessary, look at the first line and reveal a step at a time.</li> </ul>"},{"location":"student/MEQ/","title":"Module Evaluation Questionnaires (MEQ)","text":""},{"location":"student/MEQ/#information-for-students","title":"Information for students","text":"<p>Students can provide feedback to teachers to recognise good practice and help improve teaching.</p> <p>Each module can contain one or more Module Evaluation Questionnaire (MEQ), which appears in the content as a separate type of Set. When accessed, the MEQ is seen in the context of all open MEQs at the time (across all modules). </p> <p>Student responses are treated as follows:</p> <ul> <li>Responses are saved when 'SUBMIT' is pressed, with confirmation provided.</li> <li>Multiple responses are possible. Responses are initialised by the last saved submission, and can then be edited and re-submitted with the 'SUBMIT' button.</li> <li>Only the last response is used when displaying data to teachers.</li> <li>All responses are confidential to all users of the platform, i.e. the author of the response is never associated with the response when it is displayed to teachers.</li> <li>No responses are visible to teachers while an MEQ is open.</li> </ul>"},{"location":"student/MEQ/#access-to-meq-data","title":"Access to MEQ data","text":"<p>MEQ data are only accessible to teachers, and only under certain conditions. A minimum of 5 responses are required, after which data is accessible to teachers depending on question type and user role. Note that the 'moderator' is assigned per-module by central admin, and it is typically the director of studies. Data availability is as follows: </p>"},{"location":"student/MEQ/#numerical-data-likert-scale","title":"Numerical data (Likert scale):","text":"MEQ is open Response is about a specific teacher? Available to Yes n/a Data not available No No Any teacher or moderator on the module No Yes Only that teacher and the moderator"},{"location":"student/MEQ/#text-data-comments","title":"Text data (comments):","text":"MEQ is open Response is about a specific teacher? Moderator approved? Available to Yes n/a n/a Moderator (only) No No No Moderator (only) No No Yes Any teacher on the module No Yes No Moderator (only) No Yes Yes Only that teacher and the moderator"},{"location":"student/MEQ/#progress-bar","title":"Progress bar","text":"<p>When a student has open MEQs that are incomplete, a progress bar will appear on the home page. When all MEQs are complete, or none is open, the bar will disappear.</p>"},{"location":"student/answering_questions/","title":"Answering Questions","text":""},{"location":"student/answering_questions/#overview","title":"Overview","text":"<p>The main view of a question is divided into two parts. The top half contains content that is relevant to the whole question, and the bottom half contains content for each individual part. Additionally, the part content may include one or more  response areas. When you think you have answered the question, enter your answer into the response area, if it exists, and press the \"Check\" button to check your work. If you are correct, the question will be marked as \"done\". If there is no response area (e.g. for a \"show that...\" question), you can manually mark the question as done using the box at the bottom right.</p>"},{"location":"student/answering_questions/#answers-and-worked-solutions","title":"Answers and Worked Solutions","text":"<p>If you are stuck, you can view worked solutions using the \"Worked Solutions\" option on the bottom ribbon. The steps in the solution are revealed step-by-step, so you should avoid the temptation to look at the whole solution at once, and try to complete as much as possible independently. </p> <p></p> <p>You can also view the answer to each question using the \"Final Answer\" option on the bottom ribbon. This contains the  answer only, with no intermediate results or working.</p> <p>It is important that you always make your own genuine attempt to solve each problem before resorting to the final answers or the worked solutions. To help encourage this, a warning will appear if you try to access help before a question-specific time limit has elapsed.</p>"},{"location":"student/faq/","title":"Frequently Asked Questions","text":""},{"location":"student/faq/#why-cant-i-find-the-module-i-am-looking-for","title":"Why can't I find the module I am looking for?","text":"<p>Access to each module is provided by the teacher owning the module.</p> <p>If you cannot find the module you are looking for, please contact your teacher.</p>"},{"location":"student/getting_started_student/","title":"Get started as a student using Lambda Feedback","text":""},{"location":"student/getting_started_student/#accessing-content","title":"Accessing content","text":""},{"location":"student/getting_started_student/#log-in","title":"Log in","text":"<p>Use your Imperial Microsoft account to sign in and access your modules. Once you sign in, you should see a list of the modules you are enrolled in:</p> <p>You can see the current progress of each module in this view.</p> <p></p>"},{"location":"student/getting_started_student/#select-a-module","title":"Select a module","text":"<p>Click on the module name to select it. You should now see a list of available problem sets. If none are available, your teacher may not have assigned any yet.</p> <p>You can see the current progress of each problem set in this view.</p> <p></p>"},{"location":"student/getting_started_student/#select-a-problem-set","title":"Select a problem set","text":"<p>Select the problem set you wish to work on, and you should see a list of questions on the left-hand side, with the selected question on the right. If a question has sub-parts, you can select them on the right.</p> <p></p>"},{"location":"student/getting_started_student/#accessing-the-pdf-version-of-a-problem-set","title":"Accessing the PDF version of a problem set","text":"<p>If you prefer to work on a PDF version of the problem set, you can generate a PDF by clicking the 'pdf' button underneath the problem set title.</p> <p></p>"},{"location":"student/getting_started_student/#answering-questions","title":"Answering questions","text":"<p>You can make progress on the problem by entering correct answers or clicking the 'Mark as done' button on the bottom right of each question page. This can be useful to track progress if working on the PDF version, or for questions which do not have a response box, e.g., show that questions.</p> <p>See the Answering Questions page for more help with answering questions.</p> <p></p>"},{"location":"student/getting_started_student/#using-the-workspace","title":"Using the Workspace","text":"<p>The Workspace provides you with various functionalities to assist you during your learning process. Your edits and progress in the Workspace are saved per each Question you preview. So, you will be able to view your old edits for the Question you are currently on.</p> <p>Here are the various functionalities:</p>"},{"location":"student/getting_started_student/#canvas","title":"Canvas:","text":"<p>A pane where you can write down your thought process and notes for the previewed question (handwriting, sticky notes &amp; text).</p> <p></p>"},{"location":"student/getting_started_student/#chat","title":"Chat:","text":"<p>A chat interface connecting you with helpful Chatbots. The Chatbots are AI Assistants that you can chat with to ask for help or further explanations regarding the Question that you are working on. </p> <p></p> <p>For more information on what the chatbot knows about you and how you can use it to its full potential:</p> <p>Chatbots - More Info</p>"},{"location":"student/lexdown_student/","title":"Text Editing","text":"<p>The lexdown is widely used in Lambda Feedback wherever rich text input is required. On the student interface, it is used to add personal solution notes, and to write comments.</p> <p>It accepts:</p> <ul> <li>Standard Markdown</li> <li>\\(\\LaTeX\\)</li> <li>Images (paste or drag and drop)</li> <li>Videos (paste a URL)</li> </ul>"},{"location":"student/lexdown_student/#latex","title":"LaTeX","text":"<p>LaTeX is a typesetting system widely used in academia to produce well-formatted documents. It is mostly used in Lambda Feedback for its capability to render complex mathematical expressions clearly and accurately. </p> <p>As an example, the following LaTeX code:  <pre><code>\\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S\n</code></pre> Produces the following output:</p> \\[ \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S \\] <p>In the Lexdown editor, anything surrounded by dollar signs (like <code>$ x^2 $</code>) will be interpreted as LaTeX.  Only the subset supported by KaTeX, which includes most common LaTeX functions, can be used.</p>"},{"location":"student/lexdown_student/#latex-equations-in-5-minutes","title":"LaTeX equations in 5 minutes","text":""},{"location":"student/lexdown_student/#numbers-and-letters","title":"Numbers and letters","text":"<p>Numbers and Latin letters can be entered as you would expect: <pre><code>1, 2, 3, 3.14159, -2.5, x, y, z\n</code></pre></p> \\[ 1, 2, 3, 3.14159, -2.5, x, y, z \\] <p>Subscripts can be written with <code>_</code> and superscripts can be written with <code>^</code>, for example in <code>x^2</code> (\\(x^2\\)) or <code>x_2</code> (\\(x_2\\)). Only the first character or command after a <code>_</code> or <code>^</code> will be taken. To subscript or superscript multiple characters, they can be  grouped in curly braces, like this: <code>V_{ab}</code> (\\(V_{ab}\\)).</p>"},{"location":"student/lexdown_student/#basic-functions","title":"Basic functions","text":"<p>Functions in LaTeX start with a backslash <code>\\</code>.</p> <p>Some common functions include Greek letters:</p> <ul> <li><code>\\pi</code> (\\(\\pi\\))</li> <li><code>\\delta</code> (\\(\\delta\\))</li> <li><code>\\Delta</code> (\\(\\Delta\\)) </li> <li>etc.</li> </ul> <p>Equalities: </p> <ul> <li><code>\\approx</code> (\\(\\approx\\))</li> <li><code>\\ne</code> (\\(\\ne\\))</li> <li><code>\\gt</code> (\\(\\gt\\))</li> <li>etc.</li> </ul> <p>Symbols/operators: - <code>\\int</code> (\\(\\int\\)) - <code>\\sum</code> (\\(\\sum\\)) - <code>\\sin</code> (\\(\\sin\\)) - <code>\\ln</code> (\\(\\ln\\)) - etc.</p>"},{"location":"student/lexdown_student/#functions-with-arguments","title":"Functions with arguments","text":"<p>Some functions take arguments. Arguments are given between curly braces <code>{}</code>. </p> <p>Some commonly functions with arguments are:</p> <ul> <li><code>\\sqrt{x}</code> (\\(\\sqrt{x}\\)). This places a square root sign around the argument.</li> <li> <p><code>\\frac{x}{y}</code> (\\(\\frac{x}{y}\\)). This command takes two arguments, and produces a fraction with the first argument in the numerator and the second on the denominator.</p> </li> <li> <p>Diacritics:</p> <ul> <li><code>\\vec{x}</code> (\\(\\vec{x}\\)).</li> <li><code>\\dot{x}</code> (\\(\\dot{x}\\)). </li> <li><code>\\hat{x}</code> (\\(\\hat{x}\\)).</li> <li>etc.</li> </ul> </li> <li> <p><code>\\mathrm{x}</code> (\\(\\mathrm{x}\\)). This formats the argument as regular, upright text, rather than italics. Should be used for units and operators (e.g. \\(\\frac{\\mathrm{d}y}{\\mathrm{d}x}\\)).</p> </li> </ul>"},{"location":"student/lexdown_student/#nesting","title":"Nesting","text":"<p>Functions can be nested arbitrarily. For example, a square root may contain a fraction, which may contain another square root: <pre><code>\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\n</code></pre></p> \\[\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\\]"},{"location":"student/lexdown_student/#going-further","title":"Going further","text":"<p>If you are unsure of the correct function to use to produce the desired result, there is a list of all supported KaTeX functions here.</p>"},{"location":"student/response_areas/","title":"Response Areas","text":""},{"location":"student/response_areas/#numerical-answers","title":"Numerical answers","text":"<p>This type of response area expects a numerical answer. Usually, a tolerance is allowed, so you will still be marked as correct if your response differs from the answer by a small amount. </p> <p></p> <p>Some questions will also require you to enter the units of the answer. In this case, any form of the same unit, with any SI prefix, should be accepted. For example, if the answer to a question is <code>10 MPa</code>, <code>0.01 GPa</code> and <code>10 MNm^-2</code> should both be marked correct. </p>"},{"location":"student/response_areas/#entering-mathematical-expressions","title":"Entering mathematical expressions","text":"<p>Entering mathematical expressions on Lambda is very similar to if you were doing it in Matlab, for example. </p>"},{"location":"student/response_areas/#examples","title":"Examples","text":"Expression Lambda Feedback input \\(x^2 - x - 2\\) <code>x^2 - x - 2</code> \\(\\sqrt{\\sin(x) + \\frac{\\pi}{2}}\\) <code>sqrt(sin(x) + pi/2)</code> $e^{\\frac{\\pi x}{2} + 1} <code>exp((pi*x)/2 + 1)</code>"},{"location":"student/response_areas/#reference","title":"Reference","text":"Operator Symbol Lambda Feedback input Addition \\(a + b\\) a+b Subtraction \\(a - b\\) a-b Multiplication \\(a \\times b\\) a*b Division \\(\\frac{a}{b}\\) a/b Exponentiation \\(a^b\\) a^b Square root \\(\\sqrt{a}\\) sqrt(a) <p>Common elementary functions such as \\(\\sin\\), \\(\\cos\\), \\(\\arcsin\\), \\(\\ln\\) etc. are also supported.</p>"},{"location":"teacher/","title":"Teacher and Content-author Documentation","text":"<p>Welcome to teachers! We've put a lot of love into the teacher experience. Do give us feedback if you need anything. For now, try Getting Started ...</p>"},{"location":"teacher/guides/MEQ/","title":"Module Evaluation Questionnaires (MEQ)","text":""},{"location":"teacher/guides/MEQ/#information-for-teachers","title":"Information for teachers","text":"<p>Students can provide feedback to teachers to recognise good practice and help improve teaching.</p> <p>General information about MEQs is available on the relevant student page on MEQ.</p> <p>This page provides additional information relevant to teachers.</p>"},{"location":"teacher/guides/MEQ/#create-an-meq","title":"Create an MEQ","text":"<p>An MEQ is a special type of Set. It can be created in the Content tab of a module in TEACHER mode. MEQs use templates that are controlled centrally on the tenant (e.g. school or university). To modify or extend the available templates, speak to your administrator.</p> <p>When you add an MEQ from a template, the following can be edited:</p> <ul> <li>Name of the MEQ</li> <li>Visibility (Opening and closing dates; and 'manually hidden' toggle) </li> <li>Which teachers from the module are evaluated (if any)</li> </ul> <p>Once an MEQ has been created, it becomes a separate copy; future changes to the central templates will not appear in the MEQ you have created. The only options to edit the MEQ are the above listed properties. </p> <p>There is no limit to the number of MEQs that can be created in a module.</p>"},{"location":"teacher/guides/MEQ/#teacher-specific-questions","title":"Teacher-specific questions","text":"<p>The template question includes general content and response areas that are tailored to the questionnaire environment. One option for each response area is to designate a question as </p> <p>'Repeat statements for each teacher'</p> <p>For questions with this toggle, the question repeats identically for each teacher listed in the MEQ settings. The default setting is no teachers, and the teachers need to be manually added in the settings by selecting from teachers who are enrolled on the module.</p>"},{"location":"teacher/guides/MEQ/#data-visibility","title":"Data visibility","text":"<p>An overview of data visibility is provided on the student page on MEQ. The moderator(s) for each module are set by the Administrators, and are visible on the Teachers tab in the module in TEACHER mode.</p>"},{"location":"teacher/guides/MEQ/#information-for-students","title":"Information for students","text":"<p>See also, MEQ for students.</p>"},{"location":"teacher/guides/analytics/","title":"Analytics Guide","text":""},{"location":"teacher/guides/analytics/#module-level-analytics","title":"Module-level analytics","text":"<p>The module overview tab displays cohort progress and cohort activity. Access to the overview tab is subject to teacher role privileges. </p> <p>The Content tab displays all Sets, with cohort-level data on activity and progress within each Set. Access to the data within the Content tab is subject to teacher role privileges. </p> <p>Within the content there is a stats tab which shows cohort-level data on question completion and statistics on time spent on each question. Response Areas are listed and show completion rates overall, and best per student. Detailed response statistics are available in the 'Explore' button on the Response area. The stats tab availability is subject to teacher role privileges.</p> <p>The ID of Response Areas is linked between module instances. If a Response Area maintains the response data shape then the data across multiple instances can be combined for stronger statistics. As of 10/7/25 there are no features in the UI to link data across instances, but these links will be added in future (the data saved is in the correct structure to allow this feature on all past data).</p> <p></p>"},{"location":"teacher/guides/analytics/#analytics-and-question-versions","title":"Analytics and question versions","text":"<p>Analytics begin when a question is published. After publishing a question for the first time it becomes available to students and their usage is logged and fed back to the student and the teacher. </p> <p>Each question can have add or remove response areas. When a response area is removed, then it is removed only from the \"current version\" of the question (the version that the teacher is editing) and it persists on the previous version(s) of the question. This means that all submissions and analytics remain, but they are now linked to the response area which only exists on a previous version(s) of the question.</p> <p>Currently (as of 10/7/25) only analytics against the published version of the question can be seen in the UI. Other data, although still saved, cannot be seen.</p>"},{"location":"teacher/guides/content-sets-questions/","title":"Editing questions","text":"<p>This guide explains how to use the editor to create and modify sets and questions.</p>"},{"location":"teacher/guides/content-sets-questions/#click-a-set-to-edit-or-add-questions-to-it","title":"Click a Set to edit or add questions to it.","text":""},{"location":"teacher/guides/content-sets-questions/#a-guide-to-the-editor","title":"A guide to the editor:","text":"Label Name Description 1 Add Question Add a new blank question, duplicate an existing question, or upload a .zip file containing one or more question JSON files. 2 File, Preview, and Stats Access pages for:\u2022 File: Manage versions, download as a JSON file, or delete the question.\u2022 Preview: See the question as a student would.\u2022 Stats: View statistics on student responses for the question. 3 Question Name Edit the name of the question. 4 Master Content The main content for the question, which is always visible above the individual parts. This field uses the Lexdown editor. 5 Current Part Indicates which question part you are currently editing. 6 Part Content Edit the content for the selected question part (i.e., the sub-question). 7 Response Area The input field where a student submits their answer. Adding a response area is optional.  See here for an in-depth explanation. 8 Question Help Options Add optional support materials for students, such as a Structured Tutorial, a Final Answer, or Worked Solutions. The buttons shown here are the same ones students will see. 9 Teacher/Student View Toggle Toggle between the teacher editing view ('EDIT') and the student 'PREVIEW' to see how the question will appear to students. 10 Edit Guidance Add extra details for the question, such as guidance notes, estimated completion time, and skill level. 11 Part Options Add a new part, duplicate the current part, or delete a part (only available if there is more than one)."},{"location":"teacher/guides/faq/","title":"Frequently Asked Questions","text":""},{"location":"teacher/guides/faq/#how-can-i-enroll-students-on-my-new-module","title":"How can I enroll students on my new Module?","text":"<p>Students and users are given access to a module using their college email address (from microsoft).</p> <ol> <li>Login and navigate to your Teacher dashboard</li> <li>Select the module on which you want to enroll students</li> <li>When on the module page, click the View Students button</li> <li>Enter the enrolment page by clicking the Entroll Students  button</li> <li>Enroll students by supplying one or more student email addresses</li> </ol> <p>For a visual guide, click here</p>"},{"location":"teacher/guides/faq/#how-can-i-move-questions-between-problem-sets","title":"How can I move questions between problem sets?","text":"<p>When creating a new question the teacher can choose to \"clone\" from an existing question. The teacher can then delete the original version.</p> <ol> <li>In the problem set you wish to move the question to, select the v symbol to the right of the Add Question button</li> <li>From the dropdown menu, select Clone From Question</li> <li>Select the title of the question you wish clone from the list that appear</li> <li>If you wish, go back and delete the question from its original location</li> </ol>"},{"location":"teacher/guides/faq/#how-can-i-share-a-link-to-a-problem-set","title":"How can I share a link to a Problem Set?","text":"<p>To share a link with students, open the Problem Set in STUDENT mode (light blue top bar), and copy the URL from the browser.</p> <p>To share a link with teachers who will access the content editor and analytics, share a link from TEACHER mode (orange top bar); students won't be able to access this link.</p>"},{"location":"teacher/guides/faq/#how-can-i-set-parameters-for-evaluation-functions","title":"How can I set parameters for evaluation functions?","text":"<p>The most common parameters will be visible uder the EVALUATE tab in the configure panel.</p> <p>If there is a parameter that is not already visible it can be set using the Advanced - raw parameters (also under the EVALUATE tab) by doing the following:</p> <p></p> <ol> <li> <p>Hover over the list of parameters in the Advanced - raw parameters area. Click the green plus-symbol that appears.</p> <p></p> </li> <li> <p>Type the name of the parameter (without quotation marks).</p> <p></p> </li> <li> <p>Hover over the box that says <code>NULL</code> next to the newly added parameter. Click the green pen symbol that appears to the right of it.</p> <p></p> </li> <li> <p>Type in the desired value in box that appears. By default it will be assumed that the parameter value is a string. The webclient will infer other possible types based on the written input. If the setting should be a string, click the green checkmark to the top right, and if you want the inferred type click the green checkmark at the bottom right.</p> <p></p> </li> <li> <p>The parameter is now set.</p> <p></p> </li> </ol>"},{"location":"teacher/guides/faq/#how-do-i-reorder-questions","title":"How do I reorder questions?","text":"<p>It is only possible to reorder published questions in a set. This prevents inadvertently inserting new questions between two published ones. This ensures consistency to the student when viewing a published set as existing questions will remain in an unchanged order, with new questions being added to the bottom (unless manually changed by the teacher). You can tell a question is unpublished as it will take the '1.X' numbering format</p> <p>To reorder questions:</p> <ol> <li>Publish the questions you wish to reorder using FILE &gt; SAVE AND PUBLISH (alternatively click on the PUBLISH WHOLE SET button)</li> <li>Refresh the page</li> <li>Drag and drop the questions into the new order</li> <li>Ensure the green box pops up saying: 'questions reordered successfully' - there is no need to republish the set</li> </ol>"},{"location":"teacher/guides/faq/#what-to-do-when-space-is-not-showing-in-the-pdf-generated-by-lambda-feedback","title":"What to do when <code>\\space</code> is not showing in the pdf generated by lambda feedback?","text":"<p>The Pandoc library that lambdafeedback use to generate a pdf does not support <code>\\space</code>. Alternatives that could be use to generate a space in math block is to use the tilde symbol <code>~</code> or <code>\\,</code> for thinner spacing.</p>"},{"location":"teacher/guides/faq/#what-to-do-if-the-pdf-is-not-compiling-my-inline-math-equation","title":"What to do if the pdf is not compiling my inline math equation?","text":"<p>Please check if there is an additional space at the start or a the end of the equation. This is usually the cause for inline math blocks not compiling.</p> <p>Sometimes if you are copy-pasting text into equations you may end up with certain characters that look normal but actually have different ASCII codes than what you intended. This may also cause a PDF not to compile.</p>"},{"location":"teacher/guides/faq/#how-can-i-have-the-same-font-for-the-unit-and-for-the-number-in-the-math-block","title":"How can I have the same font for the unit and for the number in the math block?","text":"<p>You can use the code <code>\\mathrm{}</code> or <code>{\\rm}</code>. Both code will give you your units in serifed Times New Roman, which is the same font as the number in the math block when compiled.</p>"},{"location":"teacher/guides/faq/#complex-numbers-notation","title":"Complex Numbers Notation","text":"<p>If you want to use <code>I</code> for the imaginary constant, add the parameter <code>complexNumbers</code> to \"advanced - raw parameters\" by clicking the green (+). Type in <code>complexNumbers</code> and press enter. Click the green edit button, type in \"True\" and a pop-up <code>bool - true</code> will appear. Click the green tick.</p> <p></p> <p>You can denote <code>i</code> and <code>j</code> as <code>I</code> by using the input symbols below. </p> <p></p> <p>Furthermore, the system can equate <code>exp(Ix)</code> to <code>cos(x)+Isin(x)</code>.</p>"},{"location":"teacher/guides/gettingstarted/","title":"Getting Started with Lambda Feedback for Teachers","text":""},{"location":"teacher/guides/gettingstarted/#access-a-module","title":"Access a Module","text":"<p>Use your Imperial Microsoft account to sign in and access your modules. By default, you are logged in as a student, and the interface will be blue. If you have teacher privileges, you will see a 'Teacher' button at the top.</p> <p></p> <p>To enter teacher mode, click the 'Teacher' button, and the colour of the interface will change to orange. This is where you can access all your modules, as well as upload and edit problem sets.</p> <p>As of July 2023, new modules can only be added to Lambda Feedback by administrators. Please contact an administrator if you want your module added to the website.</p> <p></p> <p>To find the module you want, you can sort them in ascending/descending order, or filter them as shown below:  Image: Quick sort (left) or filtering (right)</p> <p>Select the module you wish to edit.  </p>"},{"location":"teacher/guides/gettingstarted/#create-a-new-problem-set","title":"Create a New Problem Set","text":"<p>Click your module and then click 'Content' (in the upper-left corner).  </p> <p>Create a new set by clicking the 'Create Set' button.</p> <p>A menu will appear with options to edit the name, description, and other settings for the new set.</p> <p>To edit the content, click the set name. This will open the set in a 'WYSIWYG' editor. The first question is automatically created with a default name.</p> <p>The question structure is described here.</p>"},{"location":"teacher/guides/gettingstarted/#below-the-line","title":"Below the Line","text":"<p>Below the main question content, you can provide high-quality support material for students.</p> <p></p> <p>A student guide is here. Teachers use the 'below the line' content as follows:</p> <ul> <li>Structured tutorial is to provide scaffolding for students struggling with the question.</li> <li>Final answer is self-explanatory.</li> <li>Worked solutions provides detailed, step-by-step solutions.</li> </ul> <p>All content below the line uses the Lexdown content editor functionality. Worked solutions can be branched, or split into steps. Future developments will add branching and response areas to structured tutorials.</p> <p>It is not necessary to include all three methods of help. If you only provide content for one tab, only that button will appear in the published student version.</p> <p>For general terminology, see here.</p> <p>To see further details on how to edit your questions, see here.</p>"},{"location":"teacher/guides/gettingstarted/#enrolling-students","title":"Enrolling Students","text":"<p>In Teacher mode, open your module's home page, click the 'Students' tab, and then click 'Enrol Students'.</p> <p></p> <p>Enter a student's email address, or alternatively paste in a list of email addresses separated by commas.</p> <p></p> <p>Press 'Enter' to add the email addresses:</p> <p></p> <p>Then click 'SUBMIT' to enrol the students.</p>"},{"location":"teacher/guides/gettingstarted/#imperial-college-london-email-addresses","title":"Imperial College London Email Addresses","text":"<p>You must use the long-form email address:</p>"},{"location":"teacher/guides/gettingstarted/#valid","title":"Valid:","text":"<p>first.nameYY@imperial.ac.uk (student)  j.doe@imperial.ac.uk (staff)  first.name@imperial.ac.uk (staff) </p>"},{"location":"teacher/guides/gettingstarted/#invalid","title":"Invalid:","text":"<p>abc123@ic.ac.uk  abc123@imperial.ac.uk  user@ic.ac.uk  user@imperial.ac.uk  first.nameYY@ic.ac.uk </p> <p>This is because we use Azure Active Directory (i.e., Microsoft) to authorise users.</p>"},{"location":"teacher/guides/gettingstarted/#enrolling-teachers","title":"Enrolling Teachers","text":"<p>Teachers with the 'enrol teachers' privilege on a particular module instance can enrol other teachers on that instance. The process is similar to enrolling students as above, but in the 'Teachers' tab. When enrolling a teacher on a module instance, a role can be selected, and the privileges for that role are listed. </p> <p>Teachers without the 'enrol teachers' privilage on a particular module instance, if attempting to enrol teachers, will not be able to access the feature but will receive feedback accordingly. </p>"},{"location":"teacher/guides/good-practice/","title":"Good practice","text":""},{"location":"teacher/guides/good-practice/#formatting-style-for-readability","title":"Formatting &amp; style for readability","text":""},{"location":"teacher/guides/good-practice/#romanised-text-for-operators-and-units","title":"Romanised text for operators and units","text":"<p>Use romanised operators and scientific units to distinguish them from variables, which are italicised.</p> <ul> <li> <p>Operators: Use <code>\\sin</code>, <code>\\cos</code>, <code>\\log</code>, <code>\\det</code>, etc. For derivatives, use <code>\\mathrm{d}</code> as in <code>\\dfrac{\\mathrm{d}y}{\\mathrm{d}x}</code>.</p> <ul> <li>Correct: \\(\\sin(x)\\), \\(\\dfrac{\\mathrm{d}}{\\mathrm{d}x}\\)</li> <li>Incorrect: \\(sin(x)\\), \\(\\frac{d}{dx}\\)</li> </ul> </li> <li> <p>Units: Use <code>\\text{...}</code> or <code>\\mathrm{...}</code> (mathrm allows for numbers in the units, i.e. \\(\\mathrm{m^2}\\)). Ensure the unit's case is correct (e.g., <code>m</code> for milli, <code>M</code> for mega).</p> <ul> <li>Correct: \\(5 \\text{ m}\\), \\(10 \\text{ kN}\\)</li> <li>Incorrect: \\(5 m\\), \\(10 kN\\)</li> </ul> </li> </ul>"},{"location":"teacher/guides/good-practice/#space-between-numbers-and-units","title":"Space between numbers and units","text":"<p>Put appropriate space between a number and its unit, such as <code>5 m</code> or <code>3 kg</code>, according to the SI conventions.</p> <p>e.g. <code>$5 \\text{ m}$</code> to get \\(5 \\text{ m}\\)</p>"},{"location":"teacher/guides/good-practice/#empty-lines","title":"Empty lines","text":"<p>Using empty lines can improve the readability and neatness of your content.</p> <p>Empty lines are often useful before and after an equation, and between paragraphs of text. An empty line in markdown requires two spaces on the line, otherwise the line is ignored.</p> <p>Alternatively, adding empty LaTeX text, <code>\\text{}</code>, can work if all else fails.</p>"},{"location":"teacher/guides/good-practice/#using-platform-features-effectively","title":"Using platform features effectively","text":""},{"location":"teacher/guides/good-practice/#save-and-publish-as-you-go","title":"Save and publish as you go","text":"<p>Saving and publishing work regularly is recommended to prevent accidental data loss.</p>"},{"location":"teacher/guides/good-practice/#add-tests-to-response-areas","title":"Add tests to response areas","text":"<p>Tests allow you to enter potential student responses, define whether they are correct or not, then run the evaluation function on those student responses. This allows you to quickly test whether or not the evaluation function works as expected.</p> <p>In a response area, press <code>Configure</code> then <code>Test</code>.</p>"},{"location":"teacher/guides/good-practice/#pre-response-area-text","title":"Pre-response area text","text":"<p>Use pre-response area text to make clear what students should write.</p> <p>Pre-response area text is found under <code>Configure</code> - <code>Input</code> in the evaluation function.</p> <p>You can use LaTeX in the pre-response area text.</p> <p></p>"},{"location":"teacher/guides/good-practice/#live-preview","title":"Live preview","text":"<p>Live preview is on by default with the Expression response area. Live preview instantly renders a student's input. This is very useful for long/complicated equations, as it allows students to ensure their input is correct.</p> <p>Live preview settings are found under <code>Configure</code> - <code>Input</code> - <code>Display settings</code>.</p> <p></p>"},{"location":"teacher/guides/good-practice/#branching","title":"Branching","text":"<p>Branching is a feature for <code>worked solutions</code>. It allows you to have different solution pathways</p> <p>Usage examples:</p> <ul> <li>When a question can be solved via multiple different methods, branching can be used for each method.</li> <li>When a question has multiple parts, where each part involves substitution of different values, branching can be used for each part.</li> </ul> <p></p>"},{"location":"teacher/guides/good-practice/#input-symbols","title":"Input symbols","text":"<p>Input symbols help in two ways:</p> <ol> <li>Show the evaluation function how to interpret responses</li> <li>(If displayed) show the student how to express their response</li> </ol> <p>Input symbols require a display symbol where single dollars to delimit inline-math latex are recommended (e.g. <code>$x$</code> will display to students as \\(x\\)) and corresponding code to be used by the evaluation function (e.g. <code>x</code>). Displaying symbols to students is helpful when it's not clear how to type a symbol as code; for exaxmple symbol \\(\\rho\\) may be inserted is <code>rho</code> or <code>r</code> or <code>p</code> depending on the opinion of the teacher.</p> <p>Providing alternatives is optional but recommended. Alternatives help provide higher reliability feedback. For example, alternatives to <code>rho</code> could be <code>Rho</code>, <code>RHO</code>, <code>r</code>, <code>R</code>, <code>p</code>, <code>P</code> - although some of these may not be acceptable if they have another meaning in the context in question. </p>"},{"location":"teacher/guides/good-practice/#latex-help","title":"Latex help","text":""},{"location":"teacher/guides/good-practice/#use-dfrac-for-bigger-fractions","title":"Use <code>\\dfrac</code> for bigger fractions","text":"<p>Use <code>$\\dfrac{numerator}{denominator}$</code> for bigger fractions when you need to display them more clearly or emphasize them. For example, <code>$\\dfrac{3}{4}$</code> will produce a bigger fraction than <code>$\\frac{3}{4}$</code>:</p> <p>\\(\\dfrac{3}{4} \\quad\\)    (dfrac)</p> <p>\\(\\frac{3}{4} \\quad\\)    (frac)</p> <p>Alternatively, you can use <code>$\\displaystyle$</code> at the start of an inline equation to render everything afterwards full-size (as in display maths mode), this is especially helpful for integrals.</p>"},{"location":"teacher/guides/good-practice/#use-small-for-a-smaller-font","title":"Use <code>\\small</code> for a smaller font","text":"<p>Use <code>$\\small{text}$</code> when you need to display smaller fonts or fractions in your LaTeX expressions. For example, <code>$\\small{\\frac{1}{2}}$</code> will produce a smaller fraction than <code>$\\frac{1}{2}$</code>.</p> <p>\\(\\small{\\frac{1}{2}} \\quad\\) (small)</p> <p>\\(\\frac{1}{2}\\)</p>"},{"location":"teacher/guides/good-practice/#use-beginarray-to-generate-a-compact-table","title":"Use <code>\\begin{array}</code> to generate a compact table","text":"<pre><code>```latex\n\\begin{array}{|c|c|}\n\\hline\n\\theta_{2,0} &amp; \\theta_{1,L}\\\\\n\\hline\n-6700 &amp; 130.5641\\\\\n\\hline\n-6600 &amp; 161.6086\\\\\n\\hline\n\\end{array}\n```\n</code></pre>"},{"location":"teacher/guides/good-practice/#use-beginaligned-to-keep-your-working-formatted-nicely","title":"Use <code>\\begin{aligned}</code> to keep your working formatted nicely","text":"<pre><code>```latex\n\\begin{aligned}\nM_{d e f} &amp;=\\dfrac{1}{2}(M+M^T)\\\\\n&amp; =\\dfrac{1}{2} \\begin{pmatrix} 4 &amp; 14\\\\ -6 &amp; -11 \\end{pmatrix}+\\begin{pmatrix} 4 &amp; -6\\\\ 14 &amp; -11 \\end{pmatrix}\\\\\n&amp; =\\begin{pmatrix} 4 &amp; 4\\\\ 4 &amp; -11 \\end{pmatrix}\n\\end{aligned}\n```\n</code></pre>"},{"location":"teacher/guides/good-practice/#use-left-and-right-for-equations-with-multiple-brackets","title":"Use <code>\\left</code> and <code>\\right</code> for equations with multiple brackets","text":"<p>This sizes the brackets correctly to the height of the equation.</p> <pre><code>```latex\nf(x)=\\left (\\frac{(\\cos (x) -x) + i(\\sin (x) - x)}{wi} \\right)\n```\n</code></pre> <p></p> <p>This also works for <code>[ ]</code> and <code>\\{ \\}</code></p>"},{"location":"teacher/guides/good-practice/#shortcut-for-romanised-operators","title":"Shortcut for romanised operators","text":"<p>Use <code>\\sin</code>, <code>\\cos</code>, etc. as a shortcut for <code>\\text{sin }</code>, <code>\\text{cos }</code>, etc.</p>"},{"location":"teacher/guides/guidance/","title":"Guidance","text":"<p>Guidance provides a summary of the task, it's difficulty and the estimated time. Guidance is provided at the question level, and can be set to each question in the set.</p>"},{"location":"teacher/guides/guidance/#editing-guidance","title":"Editing Guidance","text":"<p>To edit the question guidance, click the \"Edit Guidance\" button at the top of the page.</p> <p>Here you can enter four parts: 1. Guidance Blurb - The short description of the question. 2. Minimum Time Estimate - The least amount of time the task should take in minutes. 3. Maximum Time Estimate - The longest amount of time the task should take in minutes. 4. Skill - The difficulty of the task, rated out of three stars. </p>"},{"location":"teacher/guides/guidance/#obtaining-guidance-time","title":"Obtaining Guidance Time","text":"<p>We also support suggesting the time estimate. This uses machine learning based on worked solution and skill level to determine an estimated time for the task.</p> <p>To use this feature do the following: 1. Fill in all the question's attributes as much as possible. i.e, the question's text, the worked solution, skill level etc., The more information is filled in, the more accurate the suggested guidance time will be.</p> <ol> <li>Click on the \"Suggest\" button in the guidance configuration tab after you have filled in all the question's attributes.</li> </ol> <p></p>"},{"location":"teacher/guides/lexdown/","title":"The Lexdown content editor","text":"<p>The Lexdown content editor is widely used in Lambda Feedback. It accepts:</p> <ul> <li>standard markdown</li> <li>\\(\\LaTeX\\) (delimited by $ and limited to KaTeX functionality)</li> <li>images (paste or drag and drop)</li> <li>videos (paste a URL)</li> </ul> <p>The Lexdown editor is an adapted version of lexical to use markdown-first, and incorporate features including drag-and-drop images, embedded videos and audio, and switch to raw markdown.</p>"},{"location":"teacher/guides/lexdown/#common-needs-in-the-lexdown-editor","title":"Common needs in the Lexdown editor","text":"<p>Here's a walkthrough to create some basic content:</p>"},{"location":"teacher/guides/lexdown/#inline-maths","title":"Inline maths","text":"<p>Use the <code>$</code> sign to delimited inline maths. For example type the following:</p> <p><code>This is inline maths, $\\alpha&lt;0$, and it is useful</code></p> <p></p>"},{"location":"teacher/guides/lexdown/#equation-mode","title":"Equation mode","text":"<p>Start a blank line with <code>$$</code> then press the space bar. This will introduce an equation editor. Type raw \\(\\LaTeX\\) into the shaded part and see the live preview in the lower part. *Press <code>ctrl+enter</code> (Mac: <code>cmd+enter</code>) to exit the equation editing box.</p> <p>For example, type the following after typing <code>$$ [space]</code> into a fresh line:</p> <p><code>f(x) = \\int_{-\\infty}^\\infty \\hat{f}(\\xi)\\,e^{2 \\pi i \\xi x} \\,\\mathrm{d}\\xi</code></p> <p></p>"},{"location":"teacher/guides/lexdown/#steps-in-worked-solutions","title":"Steps in worked solutions","text":"<p>If you begin a fresh line with <code>---</code> (three dashes) then a horizontal rule appears. Alternatively click the button on the toolbar to insert a horizontal rule.</p> <p></p> <p>If you are editing a worked solution, then Lambda Feedback will split the worked solution into steps according to the location of horizontal rules. You can delete and add the rules and the solution steps will update.</p> <p>For example:</p> <p><code>This is the first step of the solution - which is a good hint towards solving</code> <code>---</code> <code>This is a second step, which makes it more obvious</code> <code>---</code> <code>Finally we reach the solution</code></p> <p>When viewing the worked solutions, this is how it looks:</p> <p></p> <p>This is the process to create the solution steps:</p> <p></p>"},{"location":"teacher/guides/lexdown/#images","title":"Images","text":"<p>You can add images with drag-and-drop or copy-and-paste. Images can be resized with the mouse, or click on the image to configure, or edit the raw markdown.</p>"},{"location":"teacher/guides/lexdown/#audio-clips","title":"Audio clips","text":"<p>Drag + drop an audio file into the editor, or record audio on the 'Insert v' drowpdown in Lexdown.</p>"},{"location":"teacher/guides/lexdown/#empty-lines","title":"Empty lines","text":"<p>Use 'Enter' for a new paragraph, or 'shift-Enter' for a line break.</p>"},{"location":"teacher/guides/question-export-import/","title":"Question export and import","text":""},{"location":"teacher/guides/question-export-import/#export-a-question","title":"Export a question","text":"<p>Under the File menu, select the Export as JSON option:</p> <p></p> <p>The question and images (if any) will be downloaded into your download folder.</p>"},{"location":"teacher/guides/question-export-import/#import-a-question","title":"Import a question","text":"<p>Click Add question menu and select Import questions from file option:</p> <p></p> <p>The file explorer opens. Select the zip file containing the question and click open. The question will be added as the last question.</p> <p>The zip file must contain question data in a valid JSON format. The best way to obtain a valid JSON format is to export a question, unzip the download file and open the JSON file. If the question contains media, they must be in the media folder inside of the zip file.</p>"},{"location":"teacher/guides/question-export-import/#import-more-than-1-question","title":"Import more than 1 question","text":"<p>The zip file can contain more than one question. Each of the questions must be in the JSON file and in the correct format. All media must be in the media folder.</p> <p>It is possible to e.g. export 2 questions, then unzip the exported zip files and then zip both questions and their medias into one zip file and then import the one zip file. Here is an example of a folder containing 2 questions and their medias:</p> <p></p> <p>The name of the folder and names of json files are not important. However, the name of media files must correspond with the names used in the json files when referring the media.</p>"},{"location":"teacher/reference/content_management/","title":"Content structure","text":""},{"location":"teacher/reference/content_management/#modules","title":"Modules","text":"<p>Modules are the fundamental unit of content management in Lambda Feedback (not, for example, cohorts, or years/terms/semesters)</p>"},{"location":"teacher/reference/content_management/#instances-of-a-module-eg-202627","title":"Instances of a module (e.g. '2026/27')","text":"<p>Instances of a module are independentent but share the same umbrella title to help organise content. A module with multiple instances will have a drop-down menu in the title bar where users can switch modules. If you do not see the drop-down then you do not have access to other instances (or there are no other instances).</p> <p></p>"},{"location":"teacher/reference/content_management/#default-instance-when-there-are-multiple","title":"Default instance when there are multiple","text":"<p>When opening a module in Lambda Feedback the default instance opens. The default is defined based on start and end dates for the instances. There are obvious cases such is if there is only one instance, or only one instance that is open and not closed. Logic for other cases is as follows:</p> Case Default instance All instances closed in past Most recent start date All instances not started yet Latest start date* Multiple instances open Latest start date <p>* This logic seems incorrect, but as of 10/7/25 it was the behaviour of the system</p> <p>The UX is based around the default being the most common need. In the rare case that access to a different instance is required, navigation is available.</p> <p>Note that students rarely access multiple instances of a module; teachers rarely access previous/closed instances of a module. </p>"},{"location":"teacher/reference/content_management/#data-continuity-between-instances","title":"Data continuity between instances","text":"<p>Instances are independent. New instances are created without students enrolled, submissions or events recorded, or comments. Student and teacher enrollments are independent between different instances.</p>"},{"location":"teacher/reference/content_management/#teacher-roles","title":"Teacher roles","text":"<p>There are multiple Teacher roles, which are allocated per module instance. Admins manage the list of teacher roles available within a tenant. Each teacher role has a customised combination of privileges. One role always exists, which is 'Module Owner', which includes all privileges. </p>"},{"location":"teacher/reference/content_management/#enrolment","title":"Enrolment","text":"<p>Access for students or teachers is controlled by enrollment, which is detailed in the getting started guide. </p>"},{"location":"teacher/reference/content_management/#set-visibility","title":"Set visibility","text":""},{"location":"teacher/reference/content_management/#rules","title":"Rules","text":"<p>Visibility to students is governed by the 'Manually hidden' toggle, AND the opening times.</p> <p>Manually hidden (TRUE): regardless of timings, the Set will not be visible to students.</p> <p>Manually hidden (FALSE): the Set visibility to student depends on the opening and closing settings</p> <p>Opening time:  - before this time, the Set is not visible to students - after this time, or if this time is blank, the Set is visible to students if 'Manually hidden' is FALSE. </p> <p>Closing time: - after this time, the Set is not visible to students - before this time, or if this time is blank, the Set is visible to students if 'Manually hidden' is FALSE. </p>"},{"location":"teacher/reference/content_management/#editing-set-visibility","title":"Editing Set visibility","text":"<p>In TEACHER mode, users with Edit permissions can modify the visibility of a Set:</p> <p>Visibility can be edited within the 'Content' tab. Each set has an icon on the left hand side indicating the current visibility. Clicking on the icon opens a modal to edit and save the visibility: </p> <p></p> <p>Alternatively, clicking on the 'Settings' icon will open a broader list of options, including the visibility settings.</p> <p>Examples of the icon indicating visibility are below, with the mouse hover:</p> <p> </p> <p> </p>"},{"location":"teacher/reference/content_management/#question-visibility","title":"Question visibility","text":"<p>When a Set is visible, all pages within that Set can be accessed by a student user. </p> <p>However, the visibility of 'support material' (such as worked solutions) can be controlled.</p>"},{"location":"teacher/reference/content_management/#support-material-visibility","title":"Support material visibility","text":"<p>The following types of support materials are available to students in the <code>help</code> section:</p> <ul> <li>Structured tutorial</li> <li>Final answer</li> <li>Worked solutions</li> </ul> <p>Two methods can be used to hide support material:</p>"},{"location":"teacher/reference/content_management/#configuring-student-access-at-the-set-level","title":"Configuring student access at the Set level","text":"<p>Click Update Set:</p> <p></p> <p>The page contains the Student access to support material section:</p> <p></p> <p>Access to each support material type can be set to one of the following options:</p>"},{"location":"teacher/reference/content_management/#available","title":"Available","text":"<p>Students can open this support material type without any restrictions.</p> <p>This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).</p>"},{"location":"teacher/reference/content_management/#available-with-warnings","title":"Available with warnings","text":"<p>A warning window appears if the student opens the content before the recommended time.</p> <p>The recommended time is the Minimum time estimate (mins) which can be set on the question Guidance page:</p> <p></p> <p>However, the option will be changed to Available, if any of the following is true:</p> <ul> <li>The student has downloaded the PDF</li> <li>The part is marked as done</li> <li>There is no minimum time estimate set for the question</li> <li>The time now minus the time the student first accessed the question is more than the minimum time estimate (the student has accessed the question for longer than the minimum time estimate)</li> </ul> <p>This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).</p>"},{"location":"teacher/reference/content_management/#unavailable","title":"Unavailable","text":"<p>Students cannot open any support material for any question in the set.</p> <p>This is valid for all questions in the set, even those for which the support material access is set to Available at the question level (see below).</p>"},{"location":"teacher/reference/content_management/#configuring-student-access-at-the-question-level","title":"Configuring student access at the question level","text":"<p>The support material access configuration at the question level is located on the File tab:</p> <p></p> <p>All support material is available by default, it can be changed:</p> <ul> <li>If the switch is off, then the support material is available</li> <li>If the switch is on, then the support material is unavailable</li> </ul>"},{"location":"teacher/reference/content_management/#summary-overview","title":"Summary overview","text":"Set level setting Question level setting Result (using Final answer as an example) Description Comment Unavailable N/A The Final answer is disabled The setting at the question level is ignored Available Unavailable The Final answer is disabled Available with warnings Unavailable The Final answer is disabled The same result as above Available with warnings Available When the Final answer is clicked, a warning message appears Additional conditions must be met:  <li>PDF not downloaded</li> <li>Part not marked as done</li><li>The minimum time estimate is set for the question</li> <li>The time now minus the time the student first accessed the question is more than the minimum time estimate</li>If any of them is not met, then the support material will be available with no warnings."},{"location":"teacher/reference/latex_functionality/","title":"Latex functionality","text":"<p>Lambda feedback uses one content source to serve two outputs: web and PDF. Each output has different requirements, and content must meet both requirements in order to serve both outputs.</p>"},{"location":"teacher/reference/latex_functionality/#content-formatting","title":"Content formatting","text":"<p>All content is formatted in markdown. Headings, font style, lists, tables, images, \\(\\LaTeX\\), can all be created using standard markdown.</p> <p>Special attention is required when formatting \\(\\LaTeX\\) which, although it is formatted using standard markdown (i.e. delimited by the <code>$</code> for 'inline formulas', and <code>$$</code> for an equation environment), must use a subset of \\(\\LaTeX\\) in order to compile for both outputs. This sometimes requires a compromise by the author.</p>"},{"location":"teacher/reference/latex_functionality/#the-lexdown-content-editor","title":"The Lexdown content editor","text":"<p>All content in Lambda Feedback is stored as markdown (ASCII content), input/edited using our Lexdown editor. The editor has a preview mode providing live interactive previews of content, including \\(\\LaTeX\\) via katex, and a raw markdown mode.</p>"},{"location":"teacher/reference/latex_functionality/#web-requirements-katex","title":"Web requirements: katex","text":"<p>\\(\\LaTeX\\) content is rendered in the web browser using the katex Javascript library. The katex home page has an intereactive input where content is rendered and can be checked for validity. The documentation lists which functions are supported.</p> <p>Katex is a subset of \\(\\LaTeX\\). Therefore some functions that work in \\(\\LaTeX\\) do not work in katex and won't render on the web. For example, the <code>tikz</code> package - which is a complex graphics package - is not supported by katex. Unsupported packages have knock-on effects, for example while the <code>\\cancel{}</code> function is supported, <code>\\cancelto{}{}</code> is not because it relies on <code>tikz</code>.</p> <p>Ensuring that content renders correctly on the web is straightforward because the editor gives a live preview - and will indicate when errors occur.</p>"},{"location":"teacher/reference/latex_functionality/#pdf-requirements-pandoc-and-pdflatexxetex","title":"PDF requirements: pandoc and PDFlatex/XeTeX","text":"<p>When a Problem Set is saved in the editor, the PDF is automatically compiled by sending the markdown content to Pandoc, which internally uses \\(\\LaTeX\\) (either PDFlatex of xelatex - depending on the settings within the Lambda Feedback stack) to render a PDF.</p> <p>Problems can occur with PDF compilation even if the web rendering is successful, because it uses different libraries to the web browser. If the PDF fails to compile, a red error bar will appear and provide the location of the error within the Problem Set (identifying which question), and the error that Pandoc returned.</p> <p>One key limitation of the Lambda Feedback system is that it uses markdown content, so cannot produce all the richness of a full \\(\\LaTeX\\) document (and the AMS math library). All equation environments in markdown are signalled by the <code>$$</code> delimiter which is equivalent to <code>\\begin{equation*}</code>. This rules out using alternative primary environments, such as <code>align</code>, <code>gather</code>, <code>multiline</code>, <code>alignat</code>, <code>falign</code>.</p> <p>You can use subordinate environments within an equation environment, for example the following is valid:</p> <pre><code>$$\n\\begin{aligned}\na &amp; b\\\\\nc &amp; d\n\\end{aligned}\n$$\n</code></pre> <p>Here the suffix <code>-ed</code> in <code>aligned</code> implies a subordinate environment; likewise <code>gathered</code>, <code>alignedat</code> etc. are all valid within an equation environment.</p>"},{"location":"teacher/reference/latex_functionality/#warning-no-blank-lines-allowed-in-aligned-subordinate-environment","title":"Warning: No blank lines allowed in <code>aligned</code> subordinate environment","text":"<p>If a blank line is present within a subordinate environment <code>\\begin{aligned}</code> then Pandoc will fail to compile the PDF. For example:</p> <p>```Faulty code example: $$ \\begin{aligned} a &amp; b\\ c &amp; d</p> <p>\\end{aligned} $$ ```</p> <p>The above will fail to compile the PDF. But removing the blank line will solve the problem.</p> <p>For further reading search for the AMS math package and related literature.</p>"},{"location":"teacher/reference/latex_functionality/#numbering-equations","title":"Numbering equations","text":"<p>Equation numbering is problematic and our advice is to use manual numbering. Automatic numbering is possible, for example using <code>\\begin{equation}</code> within a <code>$$</code> environment. Note that sometimes the equation numbers continue counting from one environment to the next, while at other times they don't. You cannot use <code>\\ref</code> to refer to automatic equation numbers.</p> <p>An alternative approach is to use an unnumbered aligned environment, and to add an extra column with the equation number in (e.g. <code>&amp;(2)</code>).</p> <p>For some ad hoc good practice tips, see good practice.</p>"},{"location":"teacher/reference/pdf_generation/","title":"PDF Generation","text":"<p>Lambda Feedback uses a single source to render content both in the browser and by PDF. The browser view uses katex to render LaTeX, which limits the scope of LaTeX that can be used. katex doesn't use traditional LaTeX packages, but emulates many of the popular packages: Package Emulation</p> <p>When a question is published by a teacher, a PDF copy is also generated. The PDF compilation process uses <code>xelatex</code> and the Latex Template is public. The template installs relevant packages, and a list of packages is compiled below. This list will be updated as we receive more requirements from users.</p>"},{"location":"teacher/reference/pdf_generation/#supported-packages-when-generating-a-pdf","title":"Supported packages when generating a PDF","text":"Package Description amsmath Provides essential mathematical features like aligned equations, matrices, and advanced math functions; fundamental for most LaTeX math. amssymb Adds extra mathematical symbols beyond the base LaTeX set. babel Enables multilingual support with proper hyphenation and typographical conventions; allows switching languages within a document. biblatex Manages bibliographies with advanced features and customization; more flexible than traditional packages like <code>natbib</code>. bidi Enables bidirectional text support for mixing LTR and RTL scripts like Arabic and Hebrew. booktabs Provides commands for professional-looking tables with proper spacing and design; discourages vertical rules. bracket Offers commands for properly sizing and aligning brackets in math environments. cancel Draws slashes through math expressions to indicate cancellation; useful for derivations. eurosym Adds the Euro (\u20ac) symbol with options for appearance to integrate with fonts. fixltx2e Fixes bugs and improves LaTeX2e; useful for older distributions (deprecated in recent versions). fancyvrb Enhances verbatim text with customization; useful for code listings with line numbers and styling. fontenc Specifies font encoding; allows use of comprehensive encodings like T1 for accented characters. fontspec Allows use of system fonts (OpenType, TrueType) with XeLaTeX/LuaLaTeX; offers advanced font selection. geometry Simplifies setting page dimensions, margins, and layout parameters. graphicx Includes and manipulates images in documents. grffile Allows inclusion of graphics with filenames containing multiple dots or spaces. hyperref Creates hyperlinks in documents; makes references, citations, and TOC entries clickable. ifxetex Checks if document is compiled with XeTeX; useful for engine-specific configurations. ifluatex Checks if document is compiled with LuaTeX; enables engine-specific configurations. inputenc Specifies input encoding (e.g., UTF-8) for source files; allows direct typing of accented characters. listings Includes and formats source code with syntax highlighting and customization. longtable Creates tables that span multiple pages with automatic breaks and repeated headers. lmodern Provides the Latin Modern font family; improved version of default fonts with more characters. mathspec Uses OpenType fonts for math in XeLaTeX/LuaLaTeX; matches math fonts with text fonts. microtype Improves typography with microtypographic extensions like character protrusion and font expansion. natbib Advanced citation management; supports various styles and integrates with BibTeX. parskip Adds vertical space between paragraphs and removes indentation. pifont Provides access to Dingbat fonts for symbols like checkmarks and crosses. polyglossia Multilingual support for XeLaTeX/LuaLaTeX; provides language-specific typographical rules. setspace Adjusts line spacing (single, one-and-a-half, double) in documents. ulem Adds advanced underlining and strikethrough styles; allows underlines to break at line ends. upquote Ensures straight quotes in code listings; prevents conversion to curly quotes. xcolor Adds color to text and math expressions. xeCJK Typesets Chinese, Japanese, and Korean (CJK) characters in XeLaTeX; handles fonts, spacing, and line breaking."},{"location":"teacher/reference/evaluation_functions/","title":"Evaluation Functions","text":"<p>Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containerized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. </p>"},{"location":"teacher/reference/response_area_components/","title":"Response Area Components","text":"<p>Response Areas are the interactive elements your students will use to submit their answers. They check an input given by the student, and provide feedback. As React components, they admit a certain number of parameters which are described in this section.</p> <p></p>"},{"location":"teacher/reference/response_area_components/#response-area-creation","title":"Response Area creation","text":"<p>Response areas are added to the question field, and are configured for each question in the set.</p>"},{"location":"teacher/reference/response_area_components/#add-response-area","title":"Add Response Area","text":"<p>You can add any number of response areas to a question part. These may be separated by text fields. If desired, adding a double space in the field will space out response areas.</p>"},{"location":"teacher/reference/response_area_components/#duplicate","title":"Duplicate","text":"<p>You can duplicate the response area within the same part by clicking on the duplicate icon.</p> <ul> <li>What gets copied: All teacher-configured settings are duplicated. This includes the input type, evaluation function, pre/post text, the correct answer, tests, and any feedback cases you've set up.</li> <li>What is NOT copied: Any data generated during use is not carried over. This includes comments, flags, likes, and student submissions.</li> </ul>"},{"location":"teacher/reference/response_area_components/#reorder","title":"Reorder","text":"<p>It is possible to reorder response areas within the part using the \"drag and drop\" feature. It works in the same way as reordering parts.</p>"},{"location":"teacher/reference/response_area_components/#delete-response-area","title":"Delete Response Area","text":"<p>You can delete a response area without any restrictions. A pop-up message appears to confirm the deletion.</p> <p>If students have already submitted answers to the response area you are deleting, the confirmation message will warn you.</p> <p>See more information about analytics for deleted response areas in Analytics</p>"},{"location":"teacher/reference/response_area_components/#response-area-configuration","title":"Response Area configuration","text":"<p>To configure a response area, click the configure button: </p> <p></p> <p>This opens the Response Area Panel, separated into four tabs:</p> <ol> <li>Input: Define what the student sees and how they answer.</li> <li>Evaluate: Set up the logic for how the answer is checked.</li> <li>Feedback: Create customised feedback for different correct or incorrect answers.</li> <li>Test: Define correct and incorrect answers to test your configuration to ensure it works as expected.</li> </ol>"},{"location":"teacher/reference/response_area_components/#1-input","title":"1. Input","text":"<ul> <li>Select an input style for the student by scrolling or filtering. These consist of the following:<ul> <li>Matrix</li> <li>Number</li> <li>Boolean (true/false)</li> <li>Text (for short text answers)</li> <li>Table</li> <li>Multiple-choice</li> <li>MATH_SINGLE_LINE</li> <li>MATH_MULTI_LINE</li> <li>Numeric units (separate fields for a number and its units)</li> <li>Code</li> <li>Essay (for long text answers)</li> <li>Milkdown</li> </ul> </li> </ul> <p>Each input type has suitable evaluation functions. Try to choose the most suitable input style for your desired answer. See the subpages for more detailed information about each input type.</p> <ul> <li> <p>Configuration Options</p> <ul> <li>Enable live preview: Renders the typed expression in realtime. Allows to validate student input before submitting. <code>Default: TRUE</code> for the EXPRESSION input type.</li> <li>Display input symbols: Displays the symbols and associated shortcut codes that may be required for a problem beneath the input field. These are configured in the 'Evaluate' tab. <code>Default: FALSE</code>.</li> <li>Include in PDF: Only affects the PDF version. Includes pre/post response text in the PDF, with a blank space between. <code>Default: FALSE</code> except for Multiple-choice.</li> <li>Pre/post response text (optional): Add text before and after the response area to clarify to students what to input. Accepts plain text, and single-dollar-delimited latex. E.g. <code>Estimate $f(x)=$</code> is acceptable. When using fractions in this field, use <code>$\\dfrac{}{}$</code> as this is more legible.</li> <li>Response Area Answer: Enter a reference answer. This will typically be the absolute solution to a problem. When requesting a symbolic answer, you must use the variable names (codes) you define in the 'Evaluate' tab.  Configure the answer where relevant (e.g. number of rows and columns).</li> <li>Response Area Preview: Shows you what the configured response area will look like.</li> </ul> </li> </ul>"},{"location":"teacher/reference/response_area_components/#2-evaluate","title":"2. EVALUATE","text":"<p>Configure how student expressions are evaluated.</p> <p>This is a 'no code' parametric configuration. Settings will be upgraded as the system improves.</p> <ul> <li>Evaluation Function - Select an evaluation function from the list. For example:<ul> <li><code>isSimilar</code>: Best for numerical answers. It performs a basic comparison, allowing for a configurable tolerance (absolute and relative).</li> <li><code>compareExpressions</code>: Best for symbolic answers (e.g., equations).</li> </ul> </li> <li>Evaluation Function Parameters - Configure as provided, and add new parameters as required. Details depend on the selected Evaluation Function.</li> <li>Input Symbols: This is a powerful feature for defining a dictionary of accepted symbols. For each symbol, you define:<ul> <li>Symbol: The LaTeX-rendered symbol (e.g., <code>$f(x)$</code>).</li> <li>Code: The machine-readable variable name (e.g., <code>fx</code>). This is what your students will type and what the evaluation function sees.</li> <li>Alternatives: A list of other codes you want to accept for the same symbol (e.g., <code>f_x</code>, <code>f(x)</code>, <code>f</code>). This allows you to anticipate different ways students might type the same thing.</li> <li>Visibility: A <code>TRUE</code>/<code>FALSE</code> toggle. If \"Display input symbols\" is enabled in the Input tab, this setting determines whether a specific symbol is shown to the student. This allows you to show students common symbols while still accepting less common or alternative ones in the background.</li> </ul> </li> </ul>"},{"location":"teacher/reference/response_area_components/#3-feedback","title":"3. FEEDBACK","text":"<p>Here, you can go beyond a simple \"correct/incorrect\" and provide nuanced feedback by creating \"cases.\"</p> <p>Cases are an alternative answer you want to check for. You can use cases to:</p> <ul> <li>Award full marks for a different but equally valid answer.</li> <li>Identify a common mistake and provide specific, tailored feedback to help the student learn.</li> </ul> <p>For each case, you can:</p> <ul> <li>Enter an alternative answer to check for.</li> <li>Specify if the case should be marked as \"correct\" or \"incorrect.\"</li> <li>Write custom feedback text that overrides the default feedback from the evaluation function.</li> <li>Change the color of the feedback (e.g., green for correct, orange for a hint).</li> </ul> <p>Teachers typically add cases after students start submitting answers, and data about common student answers is created.</p> <p>When a student submits a response, it's evaluated against all cases. The system displays feedback for the first case that matches. The main answer you set in the 'Input' tab is treated as \"Case 0\" and is always checked first.</p> <p>Feedback fields also support LaTeX equations in both <code>$f(x)$</code> and <code>$$f(x)$$</code> formats, and Markdown inputs such as line breaks. Make sure to follow good typesetting practice in this field.</p> <p></p>"},{"location":"teacher/reference/response_area_components/#4-test","title":"4. TEST","text":"<p>Tests provide a systematic way to log what behaviour the teacher expects. You can create a list of test inputs and define the expected outcome for each.</p> <p>It provides a useful record and an efficient way to retest the bevhaiour of a response area over time (e.g. as evaluation functions evolve, or as the subject matter itself changes).</p>"},{"location":"teacher/reference/response_area_components/#restrictions-on-changing-the-input-type","title":"Restrictions on changing the input type","text":"<p>It is possible to change the input type (e.g. from Text to Number) without any restrictions until the response area is saved.</p> <p>After the response area is saved, it is still possible to change the input type, but it will result into replacing the response area by a new one. The previous response area will still exist, but only on the previous version of the question. When replacing the response area, all content (e.g. answers, feedback cases) is preserved, but any existing student submissions and analytics will remain linked only to the previous response area.</p> <p>Student answers, click events and statistics are never lost by deleting a response area. Once a question is saved (with or without publishing), then any new changes are saved into a new (draft) version of the question. So, if e.g. a response area is deleted after a question was published, then it is deleted from the draft version only. And if this draft version is later published, then the previously published version is preserved (and with it the \"deleted\" response area and linked submissions).</p> <p>The reason why the input type change is restricted is to preserve high quality data analytics as explained in the examples below.</p>"},{"location":"teacher/reference/response_area_components/#example-1-changing-input-type-on-published-response-area","title":"Example 1 - changing input type on PUBLISHED response area","text":"<ol> <li>Create and Publish: You create a question with \"Response Area 1\" and \"Response Area 2,\" both using the Number input type. You Publish the question. Let's call this <code>Version 1</code>.</li> <li>Students Submit Answers: Students begin submitting answers, which are recorded for both response areas.</li> <li>Edit the Question: You decide to edit the question. The system automatically creates a new draft, <code>Version 2</code>.</li> <li>Make Changes:<ul> <li>In \"Response Area 1,\" you add a new input symbol. This is a simple change and doesn't affect the input type.</li> <li>In \"Response Area 2,\" you change the input type from Number to Table.</li> </ul> </li> <li>The Result: Because <code>Version 1</code> was published, changing the input type triggers the replacement process. The original \"Response Area 2\" is archived (with its student data) in <code>Version 1</code>. A brand new \"Response Area 3\" (with the Table input type) is created in its place in <code>Version 2</code>.</li> <li>Publish Again: You publish <code>Version 2</code>.</li> </ol> <p>The Takeaway:</p> <ul> <li>New student submissions will be recorded for \"Response Area 1\" (Number) and \"Response Area 3\" (Table).</li> <li>The original submissions to \"Response Area 2\" are safe and preserved with <code>Version 1</code>, but you won't see them in the analytics for the currently published <code>Version 2</code>.</li> <li>If you later Revert to <code>Version 1</code>, you will once again see the analytics for \"Response Area 1\" and the original \"Response Area 2\".</li> </ul> <p>Please note: All statistics and submissions are currently displayed against the published question version only. So, though the submissions against \"Response Area 2\" are preserved, it is not currently possible to see them. We are working on the improvement to make this possible.</p>"},{"location":"teacher/reference/response_area_components/#example-2-changing-input-type-on-saved-response-area","title":"Example 2 - changing input type on SAVED response area","text":"<p>This scenario works almost identically to the one above. The key is that once a question version is Saved, the input types of its response areas are locked to protect potential future analytics. If you edit a saved question and change an input type, it will still trigger the replacement process (archiving the old and creating a new response area).</p>"},{"location":"teacher/reference/response_area_components/#example-3-adding-new-response-area-to-a-published-question","title":"Example 3 - adding new response area to a published question","text":"<ol> <li>Start with a Published Question: You have a published question (<code>Version 1</code>) with \"Response Area 1\" and \"Response Area 2.\"</li> <li>Edit the Question: You start editing, which creates a new draft (<code>Version 2</code>).</li> <li>Add a New Response Area: You add \"Response Area 3.\"</li> </ol> <p>The State of <code>Version 2</code> (Draft):</p> <ul> <li>The input types for \"Response Area 1\" and \"Response Area 2\" are locked. This is because they exist on a previously saved version (<code>Version 1</code>). To change their type, you must go through the replacement process described above.</li> <li>The input type for the new \"Response Area 3\" is unlocked. You can change it freely because it doesn't exist on any previously saved version and has no associated student data to protect. It will only become locked after you Save or Publish <code>Version 2</code>.</li> </ul>"},{"location":"teacher/reference/response_area_components/Boolean/","title":"Boolean","text":"<p>This response area allows students to select True or False. </p>"},{"location":"teacher/reference/response_area_components/Boolean/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Boolean/#isexactequal","title":"isExactEqual","text":""},{"location":"teacher/reference/response_area_components/Code/","title":"Code","text":"<p>This response area allows students to write code for a specified programming language.</p>"},{"location":"teacher/reference/response_area_components/Code/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Code/#chatgpt-experimental","title":"chatGPT (experimental)","text":""},{"location":"teacher/reference/response_area_components/Code/#comparecontsructs-experimental","title":"<code>compareContsructs (experimental)</code>","text":""},{"location":"teacher/reference/response_area_components/Essay/","title":"Essay","text":"<p>Similar to !Text, allows student's to enter any text input, whether that be prose, math, code or any other type of input, but in a longer format. </p>"},{"location":"teacher/reference/response_area_components/Essay/#evaluation-function-options","title":"Evaluation Function Options","text":"<p>As text supports any input, so most evaluation functions can be used. However, there are some text specific evaluation functions, such as shortTextAnswer and chatGPT.</p>"},{"location":"teacher/reference/response_area_components/Essay/#chatgpt-experimental","title":"chatGPT (experimental)","text":""},{"location":"teacher/reference/response_area_components/Expression/","title":"Expression","text":"<p>\u26a0\ufe0f DEPRECATED: This feature is deprecated and will be removed in a future version. Please use Math_Single_Line instead.</p>"},{"location":"teacher/reference/response_area_components/Expression/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Expression/#issimilar","title":"isSimilar","text":""},{"location":"teacher/reference/response_area_components/Expression/#symbolicequal","title":"symbolicEqual","text":""},{"location":"teacher/reference/response_area_components/Expression/#compareexpressions","title":"compareExpressions","text":""},{"location":"teacher/reference/response_area_components/Expression/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Expression/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#enable-handwriting-input","title":"Enable Handwriting Input","text":"<p>Enables a handwriting canvas in the browser, which allows a student to draw their expression, rather than type using Sympy's syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#enable-photo-upload","title":"Enable Photo Upload","text":"<p>Allows a student to upload their expression as an image, as an alternative to handwriting if the student isn't using a phone or tablet.</p>"},{"location":"teacher/reference/response_area_components/Expression/#setting-the-answer","title":"Setting The Answer","text":"<p>Type the correct answer into the 'Response Area Answer' using standard syntax. As the student enters the answer, this will be rendered using the 'live preview' feature, to ensure the correct expression has been entered.</p> <p>Use the 'Response Area Preview' to check the answer has been set correctly.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Expression/#example-student-response-area","title":"Example Student Response area","text":"<p>Correct response given </p> <p>Incorrect response given </p>"},{"location":"teacher/reference/response_area_components/Likert/","title":"Likert","text":"<p>Allows students to fill in a five point Likert (Strongly Disagree to Strongly Agree) for any number of statements.</p>"},{"location":"teacher/reference/response_area_components/Likert/#evaluation-function-options","title":"Evaluation Function Options","text":"<p>Currently, there is no supported evaluation functions for this response area.</p>"},{"location":"teacher/reference/response_area_components/Math_Multi_Line/","title":"Math_Multi_Line","text":"<p>This response area is a general handwriting input that produces markdown output. Uses the MathPix API, which includes mathematics as dollar-delimited LaTeX.</p>"},{"location":"teacher/reference/response_area_components/Math_Multi_Line/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Math_Multi_Line/#chatgpt-experimental","title":"chatGPT (experimental)","text":""},{"location":"teacher/reference/response_area_components/Math_Multi_Line/#evaluateproof-experimental","title":"evaluateProof (experimental)","text":""},{"location":"teacher/reference/response_area_components/Math_Single_Line/","title":"Math_Single_Line","text":"<p>This response area allows users to type, write or upload mathematics, and then displays how the user's response was interpreted back to them through the 'live preview' feature. </p>"},{"location":"teacher/reference/response_area_components/Math_Single_Line/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Math_Single_Line/#compareexpressions","title":"compareExpressions","text":""},{"location":"teacher/reference/response_area_components/Math_Single_Line/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Math_Single_Line/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Math_Single_Line/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Math_Single_Line/#enable-handwriting-input","title":"Enable Handwriting Input","text":"<p>Enables a handwriting canvas in the browser, which allows a student to draw their expression, rather than type using Sympy's syntax.</p>"},{"location":"teacher/reference/response_area_components/Math_Single_Line/#enable-photo-upload","title":"Enable Photo Upload","text":"<p>Allows a student to upload their expression as an image, as an alternative to handwriting if the student isn't using a phone or tablet.</p>"},{"location":"teacher/reference/response_area_components/Math_Single_Line/#handwriting-and-image-handling","title":"Handwriting and Image Handling","text":"<p>Handwriting and students photo uploads are handled with MathPix. </p> <p>Our default parameters include: - formats: <code>['text']</code> - Returns Mathpix Markdown text with math inside delimiters - include_line_data: <code>true</code> - Adds line-by-line data with geometric information about detected elements - rm_spaces: <code>true</code> - Omits spaces around LaTeX groups and other places where spaces are superfluous - rm_fonts: <code>true</code> - Omits <code>mathbb</code>, <code>mathbf</code>, <code>mathcal</code>, and <code>mathrm</code> font commands - idiomatic_braces: <code>true</code> - Returns more compact LaTeX (e.g., <code>x^2</code> instead of <code>x^{2}</code>) - numbers_default_to_math: <code>false</code> - Standalone numbers are treated as text, not automatically wrapped in math mode - math_fonts_default_to_math: <code>false</code> - Text with math fonts is not automatically converted to math mode - math_inline_delimiters: <code>['$', '$']</code> - Delimiters for inline math mode in text output - math_display_delimiters: <code>['$$', '$$']</code> - Delimiters for display/block math mode in text output</p>"},{"location":"teacher/reference/response_area_components/Matrix/","title":"Matrix","text":"<p>Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Matrix/#arrayequal","title":"ArrayEqual","text":""},{"location":"teacher/reference/response_area_components/Matrix/#arraysymbolicequal","title":"ArraySymbolicEqual","text":""},{"location":"teacher/reference/response_area_components/Matrix/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Matrix/#rows-and-cols-required","title":"<code>rows and cols</code> (required)","text":"<p>Required paramter, describes the shape of the Matrix to be displayed. </p> <p>In the 'Response area answer' section, the number of rows and columns can either be typed directly into the corresponding boxes, or adjusted using the up and down arrows, which appear once the mouse hovers over the input box.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Matrix/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#setting-the-answer","title":"Setting The Answer","text":"<p>Once the required number of rows and cols has been selected, Each element of the matrix can be entered by clicking the individual input boxes and typing in the correct numbers.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Matrix/#example-student-response","title":"Example Student Response","text":"<p>Correct response given  Incorrect response given </p>"},{"location":"teacher/reference/response_area_components/Milkdown/","title":"Milkdown","text":"<p>Similar to !Essay, allows student's to enter any text input, but with standard Markdown processing, including headers, bullet points, numbers, table, etc.</p>"},{"location":"teacher/reference/response_area_components/Milkdown/#evaluation-function-options","title":"Evaluation Function Options","text":"<p>Currently, there is no supported evaluation functions for this response area.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/","title":"MultipleChoice","text":"<p>General multiple choice response area. Features multiple options for single answer and randomising the order.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/MultipleChoice/#arrayequal","title":"ArrayEqual","text":""},{"location":"teacher/reference/response_area_components/MultipleChoice/#parameters","title":"Parameters","text":""},{"location":"teacher/reference/response_area_components/MultipleChoice/#options-required","title":"<code>options</code> (required)","text":"<p>This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the <code>parseEquations</code> function, meaning they can support markdown styling and LaTeX. </p> <p>Example</p> <pre><code>\"options\": [\n  \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\",\n  \"\\\\( \\\\pi = 3 \\\\)\",\n  \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\",\n  \"\\\\( 3 = \\\\pi \\\\)\",\n]\n</code></pre>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#randomise-optional","title":"<code>Randomise</code> (optional)","text":"<p>This is an optional boolean which will shuffle the options array on each render of this component. </p> <p>switch to 'randomise' </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-optional","title":"<code>singleAnswer</code> (optional)","text":"<p>By default, each item options is rendered using the html <code>checkbox</code> input type. Setting the <code>singleAnswer</code> boolean flag will turn those into <code>radio</code> buttons, allowing the student to select only one option at a time.</p> <p>Note Changing this flag will alter the shape of the Response Structure, and potentially require changing the grading function type and settings.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>This structure is different depending on if the <code>singleAnswer</code> option was used:</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-false-or-undefined","title":"<code>singleAnswer</code> == False (or undefined)","text":"<p>In this case, the user data is saved as an array with the same length as <code>options</code>, where each item is either a <code>1</code> or a <code>0</code> depending on if the corresponding option was selected.</p> <p>Example</p> <p>If for an instance where there are 4 options, and the first and third options were selected, the response field would be:</p> <pre><code>\"response\": [1, 0, 1, 0]\n</code></pre> <p>Example Screenshot: SingleAnswer = False  </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-true","title":"<code>singleAnswer</code> == True","text":"<p>In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. </p> <p>Example</p> <p>If for an instance where there are 4 options, and the third option was selected, the response field would be:</p> <pre><code>\"response\": 2\n</code></pre> <p>Example Screenshot: SingleAnswer = True  </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#example-student-response","title":"Example Student Response","text":"<p> This shows a response where <code>singleAnswer</code> was set to False, since each option is displayed as a <code>checkbox</code></p>"},{"location":"teacher/reference/response_area_components/Number/","title":"Number","text":"<p>Very similar to the Text response area, except the user response is parsed as a float.</p>"},{"location":"teacher/reference/response_area_components/Number/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Number/#issimilar","title":"isSimilar","text":""},{"location":"teacher/reference/response_area_components/Number/#isexactequal","title":"isExactEqual","text":""},{"location":"teacher/reference/response_area_components/Number/#setting-the-answer","title":"Setting The Answer","text":"<p>In the 'Response Area Answer' section, enter the required float into the input field. To test this, try typing correct and incorrect answers into the 'Response Area Preview' section.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Number/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Number/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Number/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Number/#grading_parameters-optional","title":"<code>grading_parameters</code> (optional)","text":"<ul> <li><code>atol</code>: Absolute tolerance parameter</li> <li><code>rtol</code>: Relative tolerance parameter</li> </ul> <p>Valid params include atol and rtol, which can be used in combination, or alone. As the comparison made is the following:</p> <p>is_correct = abs(res - ans) &lt;= (atol + rtol*abs(ans))</p>"},{"location":"teacher/reference/response_area_components/Number/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>The response is simply sent as a float, parsed from the input field using the JavaScript <code>parseFloat</code> function.</p> <p>Example</p> <pre><code>\"response\": 15.8\n</code></pre>"},{"location":"teacher/reference/response_area_components/Number/#example-student-response","title":"Example Student Response","text":"<p>Correct response: </p> <p>Incorrect response: </p>"},{"location":"teacher/reference/response_area_components/NumericUnits/","title":"NumericUnits","text":"<p>Provides two input fields with <code>Number</code> and <code>Units</code> placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). </p> <p>Note: this area will display how the user's response was interpred using the <code>interp_string</code> field provided in the feedback object returned by that function (if it exists).</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/NumericUnits/#compareexpressions","title":"compareExpressions","text":""},{"location":"teacher/reference/response_area_components/NumericUnits/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/NumericUnits/#pre_response_text-post_response_text-optional","title":"<code>pre_response_text</code> &amp; <code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left and right of the input field respectively. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#comparing-units","title":"Comparing units","text":"<p>This is done using the compareExpressions evaluation function.</p> <p>By ticking the <code>physical_quantity (boolean)</code> option in the Evaluate tab, answers with different units from the reference answer can be compared.</p> <p></p> <p>For example, if the reference answer is 100 m, the following answers will also be accepted:</p> <ul> <li>0.1 km</li> <li>10000 cm</li> <li>10^5 mm</li> </ul> <p>See here for more information</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#tolerances","title":"Tolerances","text":"<p>This is done using the compareExpressions evaluation function.</p> <p>Tolerances can be added to your answer using the <code>atol (number)</code> and <code>rtol (number)</code> fields, which denote absolute and relative tolerances respectively.</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>In this case, the response is a single string which features the user's response to both fields separated by a space.</p> <p>Example</p> <pre><code>\"response\": \"150 g\"\n</code></pre>"},{"location":"teacher/reference/response_area_components/NumericUnits/#example-screenshot","title":"Example Screenshot","text":""},{"location":"teacher/reference/response_area_components/Table/","title":"Table","text":"<p>Table response area. Will populate the component with a grid of text input fields, in order to facilitate inputing elements of a table. The number of rows and columns can be specified, along with their corresponding names.</p>"},{"location":"teacher/reference/response_area_components/Table/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Table/#arrayequal","title":"ArrayEqual","text":""},{"location":"teacher/reference/response_area_components/Table/#arraysymbolicequal","title":"ArraySymbolicEqual","text":""},{"location":"teacher/reference/response_area_components/Table/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Table/#rows","title":"<code>rows</code>","text":"<p>The number of rows required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.</p>"},{"location":"teacher/reference/response_area_components/Table/#cols","title":"<code>cols</code>","text":"<p>The number of columns required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Table/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Table/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Table/#setting-the-answer","title":"Setting The Answer","text":"<p>Once the required number of rows and cols has been inputted, The names of each row and column should be changed depending on their corresponding variables. The value of each grid element can then be entered into individual input fields. </p> <p>If the row or column names are not changed, these will appear blank in the student response area.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Table/#example-student-response","title":"Example Student Response","text":"<p>Correct response: </p> <p>Incorrect response: </p>"},{"location":"teacher/reference/response_area_components/Text/","title":"Text","text":"<p>Allows student's to enter any text input, whether that be prose, math, code or any other type of input.</p>"},{"location":"teacher/reference/response_area_components/Text/#evaluation-function-options","title":"Evaluation Function Options","text":"<p>As text supports any input, any evaluation function can be used. However, there are some text specific evaluation functions, such as shortTextAnswer and chatGPT.</p>"},{"location":"teacher/reference/response_area_components/Text/#chatgpt-experimental","title":"chatGPT (experimental)","text":""},{"location":"teacher/reference/response_area_components/Text/#shorttextanswer-experimental","title":"shortTextAnswer (experimental)","text":""},{"location":"dev_eval_function_docs/compareBoolean/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Brief description of what this evaluation function does, from the developer perspective</p>"},{"location":"dev_eval_function_docs/compareBoolean/#inputs","title":"Inputs","text":"<p>Specific input parameters which can be supplied when the <code>eval</code> command is supplied to this function.</p>"},{"location":"dev_eval_function_docs/compareBoolean/#outputs","title":"Outputs","text":"<p>Output schema/values for this function</p>"},{"location":"dev_eval_function_docs/compareBoolean/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/compareBoolean/#simple-evaluation","title":"Simple Evaluation","text":"<pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre> <pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/compareBoolean/","title":"compareBoolean","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>CODE</code></li> <li><code>MATH_SINGLE_LINE</code></li> <li><code>MATH_MULTI_LINES</code></li> </ul> <p>This function uses SymPy to test two Boolean expressions for equivalence. Expressions are considered equal if they result in the same truth table.</p> <p>When answering questions on Booleans, it is easy for students to come up with equivalent expressions but in a different form (e.g.  if using a Karnaugh map versus by inspection). compareBoolean aims to alleviate some of the frustration that may arise by accepting any response that is equivalent to the correct answer.</p>"},{"location":"user_eval_function_docs/compareBoolean/#syntax","title":"Syntax","text":"<p>The current syntax expected by this function is based on the bitwise Boolean syntax used in C, Matlab and many other programming languages.</p> Operator Meaning LaTeX <code>A | B</code> <code>A OR B</code> \\(A + B\\) <code>A &amp; B</code> <code>A AND B</code> \\(A \\cdot B\\) <code>A ^ B</code> <code>A XOR B</code> \\(A \\oplus B\\) <code>~A</code> <code>NOT A</code> \\(\\overline{A}\\) <p>The order of precedence is as follows:</p> <ol> <li>NOT</li> <li>AND</li> <li>OR/XOR</li> </ol> <p>Brackets can be used to group terms and specify the order of evaluation. For example, <code>A &amp; B | C &amp; D</code> is interpreted as <code>(A &amp; B) | (C &amp; D)</code>.</p>"},{"location":"user_eval_function_docs/compareBoolean/#examples","title":"Examples","text":"<p>The function can understand a wide variety of complex boolean expressions. Here are some examples to illustrate its capabilities. Each pair of expressions is equivalent, and would be marked as \"correct\" by compareBoolean.</p> Response Answer Comments <code>x &amp; y</code> <code>y &amp; x</code> A trivial example, but probably the most common way student responses will differ from the answer <code>(x &amp; ~y) | (y &amp; ~z)</code> <code>x ^ y</code> Both expressions are equivalent to a logical exclusive or. <code>~(~x &amp; ~y)</code> <code>x | y</code> In this example de Morgan's laws have been used to find an equivalent representation of the OR operator."},{"location":"user_eval_function_docs/compareBoolean/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/compareBoolean/#optional-parameters","title":"Optional parameters","text":"<p>There are two optional parameters that can be set: <code>enforce_expression_equality</code> and <code>disallowed</code>.</p>"},{"location":"user_eval_function_docs/compareBoolean/#enforce_expression_equality","title":"<code>enforce_expression_equality</code>","text":"<p>If this Boolean parameter is true, the response and the answer must be strictly equal, i.e in the same form.</p>"},{"location":"user_eval_function_docs/compareBoolean/#disallowed","title":"<code>disallowed</code>","text":"<p>This parameter is a list of strings (<code>\"and\"</code>, <code>\"or\"</code>, <code>\"not\"</code> or <code>\"xor\"</code>). If one of these strings is present in the list, that operation will be disallowed. For example, responding <code>A | B</code> to an answer of <code>~(~A &amp; ~ B)</code> would normally be considered correct, but if a  <code>\"disallowed\": [\"or\"]</code> parameter were added, it would be considered incorrect. This could be useful for questions on De Morgan's laws, such as  expressing a function using only NAND gates.</p>"},{"location":"user_eval_function_docs/compareBoolean/#examples-from-integration-tests","title":"Examples from Integration Tests","text":""},{"location":"user_eval_function_docs/compareBoolean/#trivial-comparisons","title":"Trivial comparisons","text":"<p>The response and answer are exactly the same, so the response should be considered correct.</p> Response Answer Correct? <code>A &amp; B</code> <code>A &amp; B</code> \u2713 <p>Multi-character variable names are also supported</p> Response Answer Correct? <code>A &amp; Test</code> <code>A &amp; Test</code> \u2713"},{"location":"user_eval_function_docs/compareBoolean/#trivial-comparisons-but-not-identical","title":"Trivial comparisons, but not identical","text":"<p>Variables can appear in any order.</p> Response Answer Correct? <code>B &amp; A</code> <code>A &amp; B</code> \u2713 <p>The wrong operator is used, so this is incorrect as the two expressions have different truth tables.</p> Response Answer Correct? <code>A | B</code> <code>A &amp; B</code> \u2717"},{"location":"user_eval_function_docs/compareBoolean/#more-complex-comparisons","title":"More complex comparisons","text":"<p>XOR can be implemented using NAND or NOR</p> <p>Using NAND:</p> Response Answer Correct? <code>~(~(A &amp; ~(A &amp; B)) &amp; ~(B &amp; ~(A &amp; B)))</code> <code>A ^ B</code> \u2713 <p>Using NOR:</p> Response Answer Correct? <code>~(~(~A | ~B) | ~(A | B))</code> <code>A ^ B</code> \u2713 <p>A few examples using de Morgan's laws:</p> Response Answer Correct? <code>~(~A &amp; ~(B &amp; C))</code> <code>A | (B &amp; C)</code> \u2713 <code>A | ~(~B | ~C)</code> <code>A | (B &amp; C)</code> \u2713"},{"location":"dev_eval_function_docs/wolframAlphaEqual/","title":"WolframAlphaEqual","text":"<p>Edit on GitHub  View Code </p> <p>This simple evaluation function uses the WolframAlpha API to compare two strings. It performs two requests in parallel:</p> <ol> <li>One to check how the user's <code>response</code> is interpreted by WolframAlpha</li> <li>One to compare the <code>response</code> to the <code>answer</code>, by submitting the following input to the api: <code>res == ans</code></li> </ol> <p>NOTE: To work, this grading script requires a valid WolframAlpha AppID! This should be stored in the <code>WOLFRAM_APPID</code> env variable.</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#inputs","title":"Inputs","text":"<p>This function doesn't need any parameters, simply two response and answer fields <pre><code>{\n  \"response\": \"&lt;string&gt;\",\n  \"answer\": \"&lt;string&gt;\"\n}\n</code></pre></p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#outputs","title":"Outputs","text":"<pre><code>{\n  \"is_correct\": \"&lt;bool or null&gt;\",\n  \"interp_string\": \"&lt;string&gt;\",\n  \"raw_comp\": \"&lt;dict&gt;\",\n  \"raw_interp\": \"&lt;dict&gt;\"\n}\n</code></pre>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#is_correct","title":"<code>is_correct</code>","text":"<p>Extracted from the second WolframAlpha call. More specifically, the <code>\"id\": \"Result\"</code> pod's first <code>subpod.plaintext</code> value.</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#interp_string","title":"<code>interp_string</code>","text":"<p>Human friendly string which indicates how WolframAlpha interpreted the user response. Extracted from the first WolframAlpha call. Corresponds also to the value of <code>plaintext</code> from the first <code>subpod</code> in the <code>\"id\" : \"Input\"</code> pod.</p> <p>For example, if the user entered <code>10 kg</code>, this might look like <code>10 kg (kilograms)</code></p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#raw_comp","title":"<code>raw_comp</code>","text":"<p>For debugging, passes the raw json result obtained from the second WolframAlpha call (comparison).</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#raw_interp","title":"<code>raw_interp</code>","text":"<p>For debugging, passes the raw json result obtained from the first WolframAlpha call (interpretation).</p> <p>Will also return <code>error</code> detailing any issues that were encountered along the way</p>"},{"location":"user_eval_function_docs/wolframAlphaEqual/","title":"WolframAlphaEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>NUMERIC_UNITS</code></li> </ul> <p>This function uses the WolframAlpha engine to compare a student's response to the correct answer. Its power can be leveraged to realise physical-units-aware evaluation, symbolic expression comparison and much more. No validation is carried out on function inputs, values are simply sent to the API in the form <code>response == answer</code>. So for example if the student provided <code>10 kilograms</code>, and the answer was defined as <code>0.01 tonnes</code>, then <code>10 kilograms == 0.01 tonnes</code> is sent to the WolframAlpha API.</p>"},{"location":"dev_eval_function_docs/compareSets/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Brief description of what this evaluation function does, from the developer perspective</p>"},{"location":"dev_eval_function_docs/compareSets/#inputs","title":"Inputs","text":"<p>Specific input parameters which can be supplied when the <code>eval</code> command is supplied to this function.</p>"},{"location":"dev_eval_function_docs/compareSets/#outputs","title":"Outputs","text":"<p>Output schema/values for this function</p>"},{"location":"dev_eval_function_docs/compareSets/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/compareSets/#simple-evaluation","title":"Simple Evaluation","text":"<pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre> <pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/compareSets/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>MATH_SINGLE_LINE</code></li> </ul> <p>Teacher-facing documentation for this function.</p>"},{"location":"dev_eval_function_docs/compareExpressions/","title":"compareExpressions","text":"<p>=======</p>"},{"location":"dev_eval_function_docs/compareExpressions/#compareexpressions","title":"CompareExpressions","text":"<p>Edit on GitHub  View Code </p> <p>This function utilises the <code>SymPy</code> to provide a maths-aware evaluation of a learner's response.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#architecture-overview","title":"Architecture overview","text":"<p>The execution of the evaluation function follows this pattern:</p> <ul> <li>Determine context</li> <li>Parse response and answer data</li> <li>Parse criteria</li> <li>Store input parameters, parsed responses in a key-value store that allows adding new fields, but not editing existing fields</li> <li>Execute generation feedback procedure provided by the context to generate written feedback and tags</li> <li>Serialise generated feedback and tags in a suitably formatted dictionary</li> </ul>"},{"location":"dev_eval_function_docs/compareExpressions/#evaluation-function","title":"Evaluation function","text":"<p>The main evaluation function is found in <code>evaluation.py</code> as has the following signature:</p> <p><code>evaluation_function(response : str, answer : str, params: dict, include_test_data=False : bool) -&gt; dict</code></p>"},{"location":"dev_eval_function_docs/compareExpressions/#input","title":"Input","text":"<p>This is the function that should be called to evaluate a response expression. - <code>response</code> is the response expression submitted by the learner - <code>answer</code> is a reference expression provided by the task author - <code>params</code> is a dictionary with optional parameters, for available parameters and their intended use, see the user documentation - <code>include_test_data</code> is a boolean that controls whether some extra data useful for testing or debugging is returned</p>"},{"location":"dev_eval_function_docs/compareExpressions/#output","title":"Output","text":"<p>The function returns result dictionary with the following fields: - <code>is_correct</code> is a boolean value that indicates whether the response is considered correct or not - <code>feedback</code> is a string that provides information about what the evaluation function found when evaluating the response that is intended to be shown to the learner - <code>tags</code> is a list of strings that encode some information about what the evaluation function has found out about the response, more consistent across similar tasks than the string output in feedback</p> <p>The returned dictionary will be referred to as the <code>result</code> in this documentation.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#overview","title":"Overview","text":"<p>The overall flow of the evaluation procedure can be described as follows:</p> <ol> <li>The function uses the parameters given in <code>params</code> to determine the context of the evaluation. What context means will be discussed in more detail in section TODO: Add section name here.</li> <li>After the context is determined the response, answer and criteria (either supplied via <code>params</code> or from the context) are analysed an necessary information is stored for future use in a dictionary with frozen valuues, i.e. a dictionary where new items can be added but existing items cannot be changed.</li> <li>The feedback generating procedure supplied by the context is used to generate feedback based on the contents of the frozen value dictionary.</li> <li>If all criteria are found to be satisfied the response is considered correct, i.e. the <code>is_correct</code> field in the result is set to true and the feedback string and list of tags generated by the feedback generation procedure are added to their respective fields.</li> </ol> <p>TODO Describe what further information is supplied when <code>include_test_data</code> is set to true.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#context","title":"Context","text":"<p>The context is a data structure that contains at least the following seven pieces of information: - <code>default_parameters</code> A dictionary where the keys are parameter names and the values are the default values that the evaluation function will use unless another value is provided together with the response. The required fields are context-dependent, currently all contexts use the default parameters found in <code>utility\\expression_utilities.py</code> and the <code>physical_quantity</code> context adds a few extra fields, see the default parameters defined in <code>context\\physical_quantity.py</code>. - <code>expression_parse</code> function that parses expressions (i.e. the <code>response</code> and <code>answer</code> inputs) into the form used by the feedback generation procedure. - <code>expression_preprocess</code> function that performs string manipulations that makes ensures that correctly written input expressions follows the conventions expected by <code>expression_parse</code>. - <code>expression_preview</code> is a function that generates a string that can be turned into a human-readable representation of how the evaluation function interpreted the response. - <code>feedback_procedure_generator</code> function that generates a function for each criteria that can be used to evaluate if the criteria is satisfied or not. The output from this function should be a list of tags that the feedback string generator can use to produce human readable feedback. - <code>feedback_string_generator</code> function that takes tags and outputs human readable feedback strings. - <code>generate_criteria_parser</code> function that generates a parser that can be used to turn the criteria (given in string form) into a form that the feedback generation procedure can use to determine if they are correct or not.</p> <p>The context can also contain other fields if necessary.</p> <p>Remark: The current implementation uses a dictionary rather than a dedicated class for ease of iteration during the initial development phase.</p> <p>There are currently two different contexts: - <code>symbolic</code>: Handles comparisons of various symbolic expressions. Defined in <code>context\\symbolic.py</code>. - <code>physical_quantity</code>: Handles comparisons of expressions involving units. Defined in <code>context\\physical_quantity.py</code>.</p> <p>Remark: Handwritten expressions are sent as latex, which requires extra preprocessing before the right context can be determined in some cases. It should be considered whether a new context, perhaps called <code>handwritten</code>, should be created for this purpose.</p> <p>TODO Describe currently available contexts in detail</p>"},{"location":"dev_eval_function_docs/compareExpressions/#symbolic-comparison-of-symbolic-expressions","title":"<code>symbolic</code> - Comparison of symbolic expressions","text":"<p>Remark: The <code>symbolic</code> context should probably be split into several smaller contexts, the following subdivision is suggested: - <code>numerical</code>: Comparison of expressions that can be evaluated to numerical values (e.g. expressions that are already numerical values or expressions only containing constants). Focuses on identifying if numerical values are greater than, less than, proportional to the expected answer or similar. - <code>symbolic</code>: Comparison of symbolic expressions that cannot be reduced to numerical values. - <code>equality</code>: Comparison of mathematical equalities (with the extra complexities that come with equivalence of equalities compared to equality of expressions). - <code>inequality</code>: Same as <code>equality</code> except for mathematical inequalities (which will require different choices when it comes to what can be considered equivalence). It might be appropriate to combine <code>equality</code> and <code>inequality</code> into one context (called <code>statements</code> or similar). - <code>collection</code>: Comparison of collections (e.g. sets, lists or intervals of the number line). Likely to consist mostly of code for handling comparison of individual elements using the other contexts, and configuring what counts as equivalence between different collections.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#symbolic-criteria-commands-and-grammar","title":"<code>symbolic</code> Criteria commands and grammar","text":"<p>Criteria</p> <p>The criteria commands uses the following productions <pre><code>    START -&gt; BOOL\n    BOOL -&gt; EQUAL\n    BOOL -&gt; ORDER\n    BOOL -&gt; EQUAL\n    BOOL -&gt; EQUAL\n    BOOL -&gt; RESERVED written as OTHER\n    BOOL -&gt; RESERVED written as RESERVED\n    BOOL -&gt; RESERVED contains OTHER\n    BOOL -&gt; RESERVED contains RESERVED\n    EQUAL_LIST -&gt; EQUAL;EQUAL\n    EQUAL_LIST -&gt; EQUAL_LIST;EQUAL\n    EQUAL -&gt; OTHER = OTHER\n    EQUAL -&gt; RESERVED = OTHER\n    EQUAL -&gt; OTHER = RESERVED\n    EQUAL -&gt; RESERVED = RESERVED\n    EQUAL -&gt; OTHER ORDER OTHER\n    EQUAL -&gt; RESERVED ORDER OTHER\n    EQUAL -&gt; OTHER ORDER RESERVED\n    EQUAL -&gt; RESERVED ORDER RESERVED\n    OTHER -&gt; RESERVED OTHER\n    OTHER -&gt; OTHER RESERVED\n    OTHER -&gt; OTHER OTHER\n</code></pre> along the the following base tokens:</p> <ul> <li><code>START</code>: Formal token used to indicate the start of an expression (in practice: any expression that can be reduced to a single <code>START</code> is a parseable criterion).</li> <li><code>END</code>: Formal token that indicates the end of a tokenized string.</li> <li><code>NULL</code>: Formal token that denotes a token without meaning, should not appear when an expression is tokenized.</li> <li><code>BOOL</code>: Expression that can be reduced to either <code>True</code> or <code>False</code>.</li> <li><code>EQUAL</code>: Token that denotes symbolic equality between the mathematical expressions.</li> <li><code>EQUALITY</code>: Token that denotes the equality operator <code>=</code>.</li> <li><code>EQUAL_LIST</code>: Token that denotes a list of equalities.</li> <li><code>RESERVED</code>: Token that denotes a formal name for a reserved name for an expression. Reserved names include <code>response</code> and <code>answer</code>.</li> <li><code>ORDER</code>: Token that denotes an order operator. Order operators include <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code> and <code>&lt;=</code>.</li> <li><code>WHERE</code>: Token that denotes the separation of a criteria and a list of equalities that describe substitutions that should be done before the criteria is checked.</li> <li><code>WRITTEN_AS</code>: Token that denotes that syntactical comparison should be done.</li> <li><code>CONTAINS</code>: Token that denotes that a mathematical expression is dependent on a symbol or subexpression.</li> <li><code>SEPARATOR</code>: Token that denotes which symbol is used to separate a the list of equalities used by <code>WHERE</code>.</li> <li><code>OTHER</code>: Token that denotes any substring that will be passed on for more context specific parsing (e.g. explicit mathematical expressions for symbolic comparisons).</li> </ul>"},{"location":"dev_eval_function_docs/compareExpressions/#examples-of-commonly-used-criteria","title":"Examples of commonly used criteria","text":"<p>TODO Add examples</p>"},{"location":"dev_eval_function_docs/compareExpressions/#physical_quantity-comparison-of-expressions-that-involve-units","title":"<code>physical_quantity</code> - Comparison of expressions that involve units","text":""},{"location":"dev_eval_function_docs/compareExpressions/#physical_quantity-criteria-commands-and-grammar","title":"<code>physical_quantity</code> Criteria commands and grammar","text":"<p>The criteria commands uses the following productions <pre><code>    START -&gt; BOOL\n    BOOL -&gt; EQUAL\n    BOOL -&gt; ORDER\n    BOOL -&gt; EQUAL where EQUAL\n    BOOL -&gt; EQUAL where EQUAL_LIST\n    BOOL -&gt; RESERVED written as OTHER\n    BOOL -&gt; RESERVED written as RESERVED\n    BOOL -&gt; RESERVED contains OTHER\n    BOOL -&gt; RESERVED contains RESERVED\n    EQUAL_LIST -&gt; EQUAL;EQUAL\n    EQUAL_LIST -&gt; EQUAL_LIST;EQUAL\n    EQUAL -&gt; OTHER = OTHER\n    EQUAL -&gt; RESERVED = OTHER\n    EQUAL -&gt; OTHER = RESERVED\n    EQUAL -&gt; RESERVED = RESERVED\n    EQUAL -&gt; OTHER ORDER OTHER\n    EQUAL -&gt; RESERVED ORDER OTHER\n    EQUAL -&gt; OTHER ORDER RESERVED\n    EQUAL -&gt; RESERVED ORDER RESERVED\n    OTHER -&gt; RESERVED OTHER\n    OTHER -&gt; OTHER RESERVED\n    OTHER -&gt; OTHER OTHER\n</code></pre> along the the following base tokens:</p> <ul> <li><code>START</code>: Formal token used to indicate the start of an expression (in practice: any expression that can be reduced to a single <code>START</code> is a parseable criterion).</li> <li><code>END</code>: Formal token that indicates the end of a tokenized string.</li> <li><code>NULL</code>: Formal token that denotes a token without meaning, should not appear when an expression is tokenized.</li> <li><code>BOOL</code>: Expression that can be reduced to either <code>True</code> or <code>False</code>.</li> <li><code>QUANTITY</code>: Token that denotes a physical quantity, that can be either given as both a value and units, only value (i.e. a dimensionless quantity) or only units.</li> <li><code>DIMENSION</code>: Token that denotes an expression only containing physical dimensions.</li> <li><code>START_DELIMITER</code>: Token that denotes a list of equalities.</li> <li><code>INPUT</code>: Token that denotes any substring that will be passed on for more context specific parsing (e.g. explicit mathematical expressions for symbolic comparisons).</li> <li><code>matches</code>: Token for operator that checks in two quantities match, i.e. if they are rewritten using the same units, are their values equal (up to chosen tolerance).</li> <li><code>dimension</code>: Token for expression only involving dimensions (i.e. no values or units).</li> <li><code>=</code>: Token for operator that checks equality (i.e. compares if value and units are identical separately)</li> <li><code>&lt;=</code>: Token for operator that checks if a quantity's value is less than or equal to another quantity's value (after both quantities are rewritten on the same units)</li> <li><code>&gt;=</code>: Token for operator that checks if a quantity's value is greater than or equal to another quantity's value (after both quantities are rewritten on the same units)</li> <li><code>&lt;</code>: Token for operator that checks if a quantity's value is less than another quantity's value (after both quantities are rewritten on the same units)</li> <li><code>&gt;</code>: Token for operator that checks if a quantity's value is greater than another quantity's value (after both quantities are rewritten on the same units)</li> </ul>"},{"location":"dev_eval_function_docs/compareExpressions/#examples-of-commonly-used-criteria_1","title":"Examples of commonly used criteria","text":"<p>TODO Add examples</p>"},{"location":"dev_eval_function_docs/compareExpressions/#code-shared-between-different-contexts","title":"Code shared between different contexts","text":""},{"location":"dev_eval_function_docs/compareExpressions/#expression-parsing","title":"Expression parsing","text":"<p>TODO Describe shared code for expression preprocessing and parsing</p> <p>TODO Describe shared code for expression parsing parameters</p>"},{"location":"dev_eval_function_docs/compareExpressions/#other-shared-code","title":"Other shared code","text":"<p>TODO Describe shared default parameters</p>"},{"location":"dev_eval_function_docs/compareExpressions/#feedback-and-tag-generation","title":"Feedback and tag generation","text":"<ul> <li>Generate feedback procedures from criteria, each procedure return a boolean that indicates whether the corresponding criterion is satisfied or not, a string intended to be shown to the student, and a list of tags indicating what was found when checking the criteria</li> <li>For each criterion; run the corresponding procedure and store the result, the feedback string and the list of tags</li> <li>If all criteria are found to be true, then the response is considered correct</li> </ul>"},{"location":"dev_eval_function_docs/compareExpressions/#tag-conventions","title":"Tag conventions","text":"<p>The feedback procedures consists of a series of function calls, the specifics are determined by the particular criteria, that each return a list of strings (called tags). Each tag then indicates what further function calls must be performed to continue the evaluation, as well as what feedback string (if any) should be generated. When there are no remaining function calls the feedback procedure is completed. The tags are formatted according as criteria<code>_</code>name of function call outcome. For tags that are not connected to a specific criteria (e.g. tags that indicate an issue with expression parsing) the criteria name and underscore is omitted.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#returning-final-results","title":"Returning final results","text":"<p>The function returns result dictionary with the following fields: - <code>is_correct</code> is a boolean value that is set to <code>True</code> is all criteria are satisfied - <code>feedback</code> is a string that is created by joining all strings generated by the feedback procedures with a line break between each string. - <code>tags</code> is a list of strings that is generated by joining all lists of tags generated by feedback procedures and removing duplicates.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#preview-function","title":"Preview function","text":"<p>When the evaluation function preview is called the code in <code>preview.py</code> will be executed. Since different contexts interpret responses in different ways they also have their own preview functions. The context-specific preview functions can be found in <code>preview_implementations</code>.</p> <p>Remark: Since it is likely that there will be significant overlap between the response preview and the response evaluation (e.g. code for parsing and interpreting the response), it is good practice if they can share as much code as possible to ensure consistency. For this reason it might be better to move the preview functions fully inside the context (either by making a <code>preview</code> subfolder in the <code>context</code> folder, or by moving the implementation of the preview function inside the context files themselves). In this case the <code>preview.py</code> and <code>evaluation.py</code> could also share the same code for determining the right context to use.</p> <p><code>E</code> is treated as <code>E</code> in when <code>is_latex</code> is enabled. As latex2sympy2 does not parse E correctly. More details can be found ont the PR: https://github.com/lambda-feedback/compareExpressions/pull/235</p>"},{"location":"dev_eval_function_docs/compareExpressions/#tests","title":"Tests","text":"<p>There are two main groups of tests, evaluation tests and preview tests. The evaluation test can be run by calling <code>evaluation_tests.py</code></p>"},{"location":"user_eval_function_docs/compareExpressions/","title":"CompareExpressions","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>NUMERIC_UNITS</code></li> <li><code>CODE</code></li> <li><code>ESSAY</code></li> <li><code>MATH_SINGLE_LINE</code></li> </ul> <p>This function utilises the <code>SymPy</code> to provide a maths-aware comparison of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p> <p>Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equalities as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/compareExpressions/#optional-parameters","title":"Optional parameters","text":"<p>There are 15 optional parameters that can be set: <code>atol</code>, <code>complexNumbers</code>, <code>convention</code>, <code>criteria</code>, <code>elementary_functions</code>, <code>feedback_for_incorrect_response</code>, <code>multiple_answers_criteria</code>, <code>physical_quantity</code>, <code>plus_minus</code>/<code>minus_plus</code>, <code>rtol</code>, <code>specialFunctions</code>, <code>strict_syntax</code>, <code>strictness</code>, <code>symbol_assumptions</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#atol","title":"<code>atol</code>","text":"<p>Sets the absolute tolerance, \\(e_a\\), i.e. if the answer, \\(x\\), and response, \\(\\tilde{x}\\), are numerical values then the response is considered equal to the answer if $|x-\\tilde{x}| \\leq e_aBy default <code>atol</code> is set to <code>0</code>, which means the comparison will be done with as high accuracy as possible. If either the answer or the response aren't numerical expressions this parameter is ignored.</p>"},{"location":"user_eval_function_docs/compareExpressions/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"user_eval_function_docs/compareExpressions/#convention","title":"<code>convention</code>","text":"<p>Changes the implicit multiplication convention. If unset it will default to <code>equal_precedence</code>.</p> <p>If set to <code>implicit_higher_precedence</code> then implicit multiplication will have higher precedence than explicit multiplication, i.e. <code>1/ab</code> will be equal to <code>1/(ab)</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p> <p>If set to <code>equal_precedence</code> then implicit multiplication will have the same precedence than explicit multiplication, i.e. both <code>1/ab</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#criteria","title":"<code>criteria</code>","text":"<p>The <code>criteria</code> parameter can be used to customize the comparison performed by the evaluation function. If unset the evaluation function will default to checking if the answer and response are symbolically equal.</p> <p>The <code>criteria</code> parameter takes a string that defines a set of (comma separated) mathematical statements. If all statements in the list are true the response is considered correct.</p> <p>The <code>criteria</code> parameter reserves <code>response</code> and <code>answer</code> as keywords that will be replaced y the response and answer respectively when the criteria is checked. Setting <code>criteria</code> to <code>answer=response</code> is gives the same behaviour as leaving <code>criteria</code> unset.</p> <p>Note: Currently the <code>criteria</code> parameter is ignored if <code>physical_quantity</code> is set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#available-criteria","title":"Available criteria","text":"<p>Note: In the table below EXPRESSION is used to denote some mathematical expression, i.e. a string that contains mathematical symbols and operators, but no equal signs <code>=</code> or inequality signs <code>&gt;</code>, '&lt;'.</p> Name Syntax Description Example EQUAL <code>EXPRESSION = EXPRESSION</code> Checks if the expressions are equal <code>answer = response</code> - Default way to check equality of expressions ORDER <code>EXPRESSION ORDER EXPRESSION</code> Checks if the expressions have the given order. ORDER operators can be <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> <code>answer &gt; response</code> - Checks if the answer is greater than the response WHERE <code>EXPRESSION = EXPRESSION where EXPRESSION = EXPRESSION; ... ; EXPRESSION = EXPRESSION</code> Checks if the equality on the left side of <code>where</code> are equal if the equalities in the comma-separated list on the right side of <code>where</code> <code>answer = response where x = 0</code> - Checks if the curves given by the answer and the response intersect when \\(x=0\\). WRITTEN_AS <code>EXPRESSION written as EXPRESSION</code> Syntactical comparison, checks if the two expressions are written the same way. <code>response written as answer</code> - Checks if the response is written in the same for as the answer (e.g. if answer is <code>(x+1)(x+2)</code> then the response <code>x^2+3x+2</code> will not satisfy the criteria but <code>(x+3)(x+4)</code> will). PROPORTIONAL <code>EXPRESSION proportional to EXPRESSION</code> Checks if one expression can be written is equivalent to the other expression multiplied by some constant. <code>answer proportional to response</code> CONTAINS <code>EXPRESSION contains EXPRESSION</code> Checks if the left expression has the right expression as a subexpression. <code>response contains x</code> - Checks if the response contains the symbol x"},{"location":"user_eval_function_docs/compareExpressions/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with multiple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#feedback_for_incorrect_response","title":"<code>feedback_for_incorrect_response</code>","text":"<p>All feedback for all incorrect responses will be replaced with the string that this parameter is set to.</p>"},{"location":"user_eval_function_docs/compareExpressions/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"user_eval_function_docs/compareExpressions/#physical_quantity","title":"<code>physical_quantity</code>","text":"<p>If unset, <code>physical_quantity</code> will default to <code>false</code>.</p> <p>If <code>physical_quantity</code> is set to <code>true</code> the answer and response will interpreted as a physical quantity using units and conventions decided by the <code>strictness</code> and <code>units_string</code> parameters.</p> <p>Remark: Setting <code>physical_quantity</code> to <code>true</code> will also mean that comparisons will be done numerically. If neither the <code>atol</code> nor <code>rtol</code> parameters are set, the evaluation function will choose a relative error based on the number of significant digits given in the answer.</p> <p>When <code>physical_quantity</code> the evaluation function will generate feedback based on the flowchart below.</p> <p>TODO: Generate new flowchart for updated physical quantity feedback generation procedure.</p>"},{"location":"user_eval_function_docs/compareExpressions/#rtol","title":"<code>rtol</code>","text":"<p>Sets the relative tolerance, \\(e_r\\), i.e. if the answer, \\(x\\), and response, \\(\\tilde{x}\\), are numerical values then the response is considered equal to the answer if \\(\\left|\\frac{x-\\tilde{x}}{x}\\right| \\leq e_r\\). By default <code>rtol</code> is set to <code>0</code>, which means the comparison will be done with as high accuracy as possible. If either the answer or the response aren't numerical expressions this parameter is ignored.</p>"},{"location":"user_eval_function_docs/compareExpressions/#strictness","title":"<code>strictness</code>","text":"<p>Controls the conventions used when parsing physical quantities.</p> <p>Remark: If <code>physical_quantity</code> is set to <code>false</code>, this parameter will be ignored.</p> <p>There are three possible values: <code>strict</code>, <code>natural</code> and <code>legacy</code>. If <code>strict</code> is chosen then quantities will be parsed according to the conventions described in 5.1, 5.2, 5.3.2, 5.3.3 in The International System of Units (SI), 8th edition and 5.2, 5.3, 5.4.2 and 5.4.3 in The International System of Units (SI), 9th edition. If <code>natural</code> is chosen then less restrictive conventions are used.</p> <p>Remark: The default setting is <code>natural</code>.</p> <p>Remark: The <code>legacy</code> setting should not be used and is only there to allow compatibility with content designed for use with older versions of the evaluation function. If you encounter a question using the <code>legacy</code> setting is recommended that it is changed to another setting and the answer is redefined to match the chosen conventions.</p>"},{"location":"user_eval_function_docs/compareExpressions/#units_string","title":"<code>units_string</code>","text":"<p>Controls what sets of units are used. There are three values <code>SI</code>, <code>common</code> and <code>imperial</code>.</p> <p>If <code>SI</code> is chosen then only units from the tables <code>Base SI units</code> and <code>Derived SI units</code> (below) are allowed (in combinations with prefixes). If <code>common</code> is chosen then all the units allowed by <code>SI</code> as well as those listed in the tables for <code>Common non-SI units</code>. If <code>imperial</code> is chosen the base SI units and the units listed in the <code>Imperial units</code> table are allowed.</p> <p>Remark: The different settings can also be combined, e.g. <code>SI common imperial</code> will allow all units.</p> <p>The default setting is to allow all units, i.e. <code>units_string</code> is set to <code>SI common imperial</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#notation-and-definition-of-units","title":"Notation and definition of units","text":""},{"location":"user_eval_function_docs/compareExpressions/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units based on Table 2 in The International System of Units (SI), 9th edition.</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m <code>length</code> gram g <code>mass</code> second s <code>time</code> ampere A <code>electric_current</code> kelvin k <code>temperature</code> mole mol <code>amount_of_substance</code> candela cd <code>luminous_intensity</code>"},{"location":"user_eval_function_docs/compareExpressions/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI prefixes based on Table 7 in The International System of Units (SI), 9th edition.</p> SI Prefix Symbol Factor SI Prefix Symbol Factor quetta Q \\(10^{30}\\) deci d \\(10^{-1}\\) ronna R \\(10^{27}\\) centi c \\(10^{-2}\\) yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI based on Table 4 in The International System of Units (SI), 9th edition.</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r \\((2\\pi)^{-1}\\) steradian sr \\((4\\pi)^{-1}\\) hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre} \\mathrm{kilogram} \\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1} \\mathrm{kilogram} \\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-2}\\) watt W \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-3}\\) coulomb C \\(\\mathrm{second ampere}\\) volt V \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-3} \\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2} \\mathrm{kilogram}^{-1} \\mathrm{second}^4 \\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-3} \\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2} \\mathrm{kilogram}^{-1} \\mathrm{second}^3 \\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-2} \\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2 \\mathrm{kilogram second}^{-2} \\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2} \\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2 \\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2 \\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole second}^{-1}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units based on Table 8 in The International System of Units (SI), 9th edition and Tables 7 and 8 in The International System of Units (SI), 8th edition. Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not, this is done to minimize ambiguities when writing units.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from Wikipedia, Imperial Units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"user_eval_function_docs/compareExpressions/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"user_eval_function_docs/compareExpressions/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"user_eval_function_docs/compareExpressions/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumptions are: <code>constant</code>, <code>function</code> as well as those listed here:  SymPy Assumption Predicates</p> <p>Note: Writing a symbol which denotes a function without its arguments, e.g. <code>T</code> instead of <code>T(x,t)</code>, is prone to cause errors.</p>"},{"location":"user_eval_function_docs/compareExpressions/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/compareExpressions/#setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","title":"Setting input symbols to be assumed positive to avoid issues with fractional powers","text":"<p>In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a &gt; 0\\) and \\(b &gt; 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\). The same is true for other fractional powers.</p> <p>So if expressions like these are expected in the answer and/or response then it is a good idea to use the <code>symbol_assumptions</code> parameter to note that \\(a &gt; 0\\) and \\(b &gt; 0\\). This can be done by setting <code>symbol_assumptions</code> to <code>('a','positive') ('b','positive')</code>.</p> <p>The example given in the example problem set uses two EXPRESSION response areas. Both response areas uses <code>compareExpression</code> with answer <code>sqrt(a/b)</code>, as well as <code>strict_syntax</code> set to false and <code>elementary_functions</code> set to true.</p> <p>The first response area leaves <code>symbol_assumptions</code> unset. Since \\(\\sqrt{\\frac{a}{b}} = \\frac{\\sqrt{a}}{\\sqrt{b}}\\) is only guaranteed to be true if both \\(a\\) and \\(b\\) are positive. The response area does not inform the evaluation function that it should assume that \\(a\\) and \\(b\\) are positive, thus it will not consider <code>sqrt(a)/sqrt(b)</code> \\(\\frac{\\sqrt{a}}{\\sqrt{b}}\\) equivalent to the answer given by the teacher.</p> <p>The second response area sets <code>symbol_assumptions</code> to <code>('a','positive') ('b','positive')</code>. Some examples of expressions that are now accepted as correct with \\(a\\) and \\(b\\) assumed to be positive: <code>sqrt(a)/sqrt(b)</code> \\(\\frac{\\sqrt{a}}{\\sqrt{b}}\\), <code>(a/b)^(1/2)</code> \\(\\left(\\frac{a}{b}\\right)^{\\frac{1}{2}}\\), <code>a^(1/2)/b^(1/2)</code> \\(\\frac{a^{\\frac{1}{2}}}{b^{\\frac{1}{2}}}\\), <code>(a/b)^(0.5)</code> \\(\\left(\\frac{a}{b}\\right)^{0.5}\\), <code>a^(0.5)/b^(0.5)</code> \\(\\frac{a^{0.5}}{b^{0.5}}\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-plusminus-symbols","title":"Using plus/minus symbols","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol.</p> <p>Note: symbol replacement is brittle and can have unintended consequences.</p> <p>It is considered good practice to make sure that the appropriate notation for \\(\\pm\\) and \\(\\mp\\) are added and displayed as input symbols in order to minimize confusion.</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>plus_minus x^2 + minus_plus y^2</code> \\(\\pm x^2 + (\\mp y^2)\\), <code>strict_syntax</code> set to false and <code>elementary_function</code> set to true. Some examples of expressions that are accepted as correct: <code>plus_minus x^2 + minus_plus y^2</code> \\(\\pm x^2 + (\\mp y^2)\\), <code>- minus_plus x^2 + minus_plus y^2</code> \\(-\\pm x^2 + (\\mp y^2)\\), <code>plus_minus x^2 minus_plus y^2</code> \\(\\pm x^2 \\mp y^2\\), <code>- minus_plus x^2 - plus_minus y^2</code> \\(-\\mp x^2 - \\pm y^2\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#equalities-in-the-answer-and-response","title":"Equalities in the answer and response","text":"<p>There is (limited) support for using equalities in the response and answer. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\).</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>x^2-5\\*y^2-7=0</code> \\(x^2-5y^2-7=0\\) as well as <code>strict_syntax</code> set to false and <code>elementary_functions</code> set to true. Any equality of the form \\(f(x) = g(x)\\) such that \\(f(x)-g(x) = k \\cdot (x^2-5y^2-7)\\) where \\(k\\) is a non-zero constant should be accepted.</p> <p>Some examples of expressions that are accepted as correct: <code>x^2-5\\*y^2-7=0</code> \\(x^2-5y^2-7=0\\), <code>x^2 = 5y^2+7</code> \\(x^2=5y^2+7\\), <code>2x^2 = 10y^2+14</code> \\(2x^2=10y^2+14=0\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#checking-the-value-of-an-expression-or-a-physical-quantity","title":"Checking the value of an expression or a physical quantity","text":"<p>If the parameter <code>physical_quantity</code> is set to true, the evaluation function can handle expressions that describe physical quantities. Which units are permitted and how they should be written depends on the <code>units_string</code> and <code>strictness</code> parameters respectively.</p> <p>There are three examples in the example problem set. Each examples uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>strict_syntax</code> set to false and <code>physical_quantity</code> set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#example-a","title":"Example (a)","text":"<p>The response area below has answer <code>2.00 km/h</code> \\(2.00 \\frac{\\mathrm{kilometre}}{\\mathrm{hour}}\\) .</p> <p>There are many possible correct responses, e.g. <code>2.00 kilometre/hour</code>, <code>2 km/h</code>, <code>2000 m/h</code>, <code>0.556 meter/second</code>, <code>2 metre/millihour</code>.</p> <p>Note that the answer is given with 2 correct decimals, so responses such as <code>1.996 km/h</code> and <code>2.004 km/h</code> are also accepted.</p> <p>If responding in other units the evaluation function by default assumes that the same relative error is acceptable so <code>0.556 m/s</code> is considered correct but <code>0.56 m/s</code> is not.</p>"},{"location":"user_eval_function_docs/compareExpressions/#example-b","title":"Example (b)","text":"<p>Here the answer is <code>2.00 km/h</code>. To restrict the answers to SI units <code>strictness</code> is set to <code>strict</code> and <code>units_string</code> is set to <code>SI</code>. Some examples of accepted responses are: <code>0.556 metre/second</code>, <code>5.56 dm/s</code>, <code>55.6 centimetre second^(-1)</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#example-c","title":"Example (c)","text":"<p>Here the answer is <code>2.00 km/h</code>. To restrict the answers to imperial units <code>strictness</code> is set to <code>strict</code> and <code>units_string</code> is set to <code>imperial common</code>. Examples of accepted responses: <code>1.24 mile/hour</code>, <code>1.82 foot/second</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#example-d","title":"Example (d)","text":"<p>The values of physical quantities are always tested with some numerical tolerance, by default it is assumed that the the <code>answer</code> is given with the correct number of significant digit, e.g. <code>1 m</code> means 0.5 meter tolerance, while <code>1.0 m</code> means 0.05 metre tolerance.</p> <p>The relative tolerance can also be set explicitly using the <code>rtol</code> parameter.</p> <p>Here the answer is set to <code>1 N/cm</code>, while <code>physical_quantity</code> is set to <code>true</code> and <code>rtol</code> is set to <code>0.1</code> (i.e. a 10% relative error tolerance). It will accept any response between \\(0.9 \\frac{\\mathrm{N}}{\\mathrm{cm}}\\) and \\(1.1 \\frac{\\mathrm{N}}{\\mathrm{cm}}\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#changing-convention-for-precedence-for-implicit-and-explicit-multiplication","title":"Changing convention for precedence for implicit and explicit multiplication","text":"<p>You can change what convention is used for precedence for explicit and implicit multiplication by setting the <code>convention</code> parameter.</p> <p>There are two supported conventions:</p> <p><code>equal_precedence</code> Implicit multiplication have the same precedence as explicit multiplication, i.e. both <code>1/ab</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p> <p><code>implicit_higher_precedence</code> Implicit multiplication have higher precedence than explicit multiplication, i.e. <code>1/ab</code> will be equal to <code>1/(ab)</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p> <p>In the examples set there is one response area for each of the two conventions, both with answer <code>1/ab</code>. Try <code>1/ab</code>, <code>(1/a)*b</code> and <code>1/(a*b)</code> in both response areas and note the differences in behaviour.</p>"},{"location":"user_eval_function_docs/compareExpressions/#setting-absolute-or-relative-tolerances-for-numerical-comparison","title":"Setting absolute or relative tolerances for numerical comparison","text":"<p><code>compareExpressions</code> can be used both for symbolic and numerical comparisons. The default is symbolic comparisons.</p> <p>The evaluation function can be configured to do numerical comparisons in the following ways:</p> <ul> <li> <p>Setting the absolute error tolerance: This is done by setting the parameter <code>atol</code>. When <code>atol</code> is set to any positive value, then <code>response</code> will be considered correct if \\(|\\)<code>answer</code>\\(-\\)<code>response</code>$| &lt; $<code>atol</code>.</p> </li> <li> <p>Setting the relative error tolerance: This is done by setting the parameter <code>rtol</code>. When <code>rtol</code> is set to any positive value, then <code>response</code> will be considered correct if \\(|\\)<code>answer</code>\\(-\\)<code>response</code>\\(|/|\\)<code>answer</code>$| &lt; $<code>rtol</code>.</p> </li> </ul> <p>Note: If the response area is used to compare physical quantities (i.e. the parameter <code>physical_quantity</code> is set to <code>true</code>) comparisons will be numerical be default (since unit conversion has finite precision).</p>"},{"location":"user_eval_function_docs/compareExpressions/#setting-the-absolute-tolerance","title":"Setting the absolute tolerance","text":"<p>In the following response area the absolute tolerance (<code>atol</code>) has been set to \\(5\\), <code>strict_syntax</code> is set to false and <code>elementary_function</code> is set to true.</p> <p>The response area is set up with three correct answers: \\(\\sqrt{47}+\\pi\\) (approx. 9.9997), \\(\\frac{13}{3}^{\\pi}\\) (approx. 99.228) and \\(9^{e+\\ln(1.5305)}\\) (approx. 1000.05). Any answer within the set absolute tolerance (i.e. \\(5&lt;\\)<code>response</code>\\(&lt;15\\), \\(95&lt;\\)<code>response</code>\\(&lt;105\\) or \\(995&lt;\\)<code>response</code>\\(&lt;1005\\)) of any answer will be accepted.</p>"},{"location":"user_eval_function_docs/compareExpressions/#setting-the-relative-tolerance","title":"Setting the relative tolerance","text":"<p>In the following response area the absolute tolerance (<code>atol</code>) has been set to \\(0.5\\).</p> <p>The response area is set up with three correct answers: \\(\\sqrt{47}+\\pi\\) (approx. 9.9997), \\(\\frac{13}{3}^{\\pi}\\) (approx. 99.228) and \\(9^{e+\\ln(1.5305)}\\) (approx. 1000.05). Any answer within the set absolute tolerance (i.e. \\(5&lt;\\)<code>response</code>\\(&lt;15\\), \\(95&lt;\\)<code>response</code>\\(&lt;105\\) or \\(995&lt;\\)<code>response</code>\\(&lt;1005\\)) of either answer will be considered correct.</p>"},{"location":"user_eval_function_docs/compareExpressions/#customizing-comparisons-using-criteria","title":"Customizing comparisons using criteria","text":"<p>For a description of how criteria works, see section Inputs: Optional parameters: <code>criteria</code>.</p> <p>For these examples all response areas will have <code>strict_syntax</code> set to false and <code>elementary_functions</code> set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#default-criteria-checking-if-two-expressions-are-equal","title":"Default criteria - checking if two expressions are equal","text":"<p>If the <code>criteria</code> parameter is unset the evaluation function defaults to checking if the response is equal to the answer. It also attempts to recognise whether the input criteria is equivalent to checking if the response and answer are equal.</p> <p>As an example the response area below has answer \\(5x\\) and <code>criteria</code> set to <code>answer-response = 0, response/answer = 1</code> which are two different criteria equivalent to <code>response=answer</code> (assuming the answer is non-zero) so here the set criteria will not change the response area behaviour.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-order-operators-instead-of-equality","title":"Using order operators instead of equality","text":"<p>Equality, \\(=\\) (<code>=</code>), is not the only available comparison operator, criteria can also handle the typical order operators, \\(&gt;\\) (<code>&gt;</code>), \\(&lt;\\) (<code>&lt;</code>), \\(\\leq\\) (<code>&gt;=</code>), \\(\\geq\\) (<code>&lt;=</code>).</p> <p>Here the answer is set to \\(2x^2\\) and <code>criteria</code> is set to <code>answer &lt;= response, 2\\*answer &gt; response</code>. Thus any response is greater than or equal to \\(2x^2\\) but less than \\(2x^2+2\\) will be accepted.</p>"},{"location":"user_eval_function_docs/compareExpressions/#criteria-that-analyses-the-response-without-comparing-to-the-answer","title":"Criteria that analyses the response without comparing to the answer","text":"<p>This is an example of a question where the answer is not used in the comparison.</p> <p>The response area accepts any root of the polynomial \\(x^3-6x^2+11x-6\\).</p> <p>This is achieved by setting the <code>criteria</code> parameter to <code>response^3-6*response^2+11*response-6=0</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-derivatives-in-criteria","title":"Using derivatives in criteria","text":"<p>Derivatives can be used in criteria, \\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\) can be written as either <code>diff(f,x)</code> or <code>Derivative(f,x)</code>.</p> <p>Note: To use abstract function (e.g. <code>f</code> is some arbitrary function that depends on <code>x</code>) or symbols that should be considered constant it is necessary to use the <code>function</code> and <code>constant</code> assumptions, see question \"Using the `constant` and <code>function</code> assumptions\".</p> <p>Here <code>answer</code> is set to <code>sin(x)</code> and <code>criteria</code> is set to <code>diff(response,x)=answer</code>. Thus any expression in the form \\(c-\\cos(x)\\), where \\(c\\) is a constant value will be considered correct.</p> <p>Note: It will be assumed that any undefined symbol is a constant.</p> <p>More complicated expressions can also be used in criteria, e.g. we can check if the response is a solution to a differential equation.</p> <p>Suppose we want to check if the response is a solution to the logistic differential equation, \\(\\frac{\\mathrm{d}y}{\\mathrm{d}x}=\\lambda y (1-y)\\). In other words, any expression in the form \\(y(x) = \\frac{c e^{\\lambda x}}{1+c e^{\\lambda x}}\\) will be considered correct.</p> <p>To get this behaviour, set <code>criteria</code> to <code>diff(response,x)=lambda*response*(1-response)</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-where-to-substitute-symbols-in-criteria","title":"Using <code>where</code> to substitute symbols in criteria","text":"<p>One way to customize the comparison of two expressions is to substitute some symbol in the expression with either a specific number, or another symbol. This can be done using the <code>where</code> criteria.</p> <p>The first response are will either accept \\(A \\cos(\\omega t)\\) or \\(A\\cos(\\omega t + \\phi)\\) as a correct answer. This is achieved by setting the answer to <code>A*cos(omega*t+phi)</code> and the  <code>criteria</code> parameter to <code>response=answer where phi=2*pi</code>.</p> <p>To substitute a symbol with several different values, one criteria for each substitution must be created.</p> <p>The second response area below will accept any function \\(f(x)\\) whose curve passes through the points \\((0,0)\\), \\((1,1)\\) and \\((2,2)\\). This is achieved by setting the answer to <code>x</code> and the <code>criteria</code> parameter to <code>response=answer where x=0, response=answer where x=1, response=answer where x=2</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-contains-to-check-if-response-contains-a-certain-symbol","title":"Using <code>contains</code> to check if response contains a certain symbol","text":"<p>To create a criteria that checks if an expressions contains a certain symbol we use <code>contains</code>.</p> <p>Suppose we expect the response to be an expression that can be written in many equivalent (but not equal) ways, for example, if we ask for an expression that gives all numbers, \\(x\\), such that \\(\\sin(x)=0\\), then \\(\\pi n\\) and \\(\\pi+\\pi n\\) (where \\(n \\in \\mathbb{Z}\\)) are valid responses. Thus comparison with a reference expression is not enough. We can use the criteria <code>sin(response)=0</code> but this will only determine if the expression is an example of a number with the desired property, not a general expression. If we add another criteria, <code>response contains n</code>, we can also check that the response contains some dependence on an arbitrary integer.</p> <p>The response area below has answer set to \\(0\\), <code>criteria</code> set to <code>sin(response)=0, response contains n</code> and <code>symbol_assumptions</code> is set to <code>('n', integer)</code> (see Setting input symbols to be assumed positive to avoid issues with fractional powers and Using the <code>constant</code> and <code>function</code> assumptions for more information on how assumptions can be used). There is also an input symbol with code <code>N</code> and several alternatives that are commonly used to denote integers.</p> <p>Note: There is currently a bug in the interaction between criteria and so only upper-case letters can be used for input symbols for this particular response area.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-proportional-to-to-check-if-the-ratio-between-two-expressions-is-constant","title":"Using <code>proportional to</code> to check if the ratio between two expressions is constant","text":"<p>Criteria can be easily used to check if two expression differ by a certain factor, for example <code>response = 2*answer</code> checks if the response is twice the expected value. To check if two expressions differ with some arbitrary factor use <code>proportional to</code>.</p> <p>In the response area below any (non-zero) multiple of \\(a+b+c\\) will be accepted. It has <code>criteria</code> set to <code>response proportional to answer</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-written-as-to-syntactical-comparison","title":"Using <code>written as</code> to syntactical comparison","text":"<p>The evaluation function can automatically detect some patterns and do syntactical comparison (comparing if expressions are written in the same way instead of checking if they are mathematically equivalent) using the <code>syntactical_comparison</code> parameter. To check for more specific patterns the <code>written as</code> criteria can be used.</p> <p>For example, one method to find the solutions to a quadratic equation is to Complete the square. Part of this method is to rewrite a second degree polynomial in the form \\((x+p)^2+q\\) where \\(p\\) and \\(q\\) are constants. This square form is not among the preprogrammed forms that the evaluation function is aware of, so if we want to make a response are where a student practices this rewrite step specifically it could be set up as the one below.</p> <p>Suppose the task is to express \\(x^2-8x+11\\) in the form \\((x+p)^2+q\\) where \\(p\\) and \\(q\\) are constants. The rewritten form in this case is \\((x-4)^2-5\\). In the response area below we have set the answer to <code>(x-4)^2-5</code> and <code>criteria</code> to <code>response = answer, response written as answer</code>. Thus <code>x^2-8x+11</code> will not be considered valid, but <code>(x-4)^2-5</code> will.</p> <p>Note that the syntactical comparison done by <code>written as</code> is relatively crude, for example <code>-5+(x-4)^2</code> or <code>(x+(-4))^2+(-5)</code> will not be accepted. Some examples of how to improve this can be found in part (i).</p>"},{"location":"user_eval_function_docs/compareExpressions/#customising-feedback-based-on-satisfied-criteria","title":"Customising feedback based on satisfied criteria","text":"<p>The procedure for creating feedback on a student response can be briefly described as follows:</p> <ul> <li>Take student response and interpret it appropriately (exactly how depends on how various parameters are set)</li> <li>Analyse interpreted response and make a list of identified properties (what properties the evaluation function looks for depends on various parameters)</li> <li>For each identified property generate a string of text with appropriate feedback (note that in many cases, including the common case where the response is equal to the answer, this string will be empty)</li> </ul> <p>When a property of the response is identified a corresponding tag is generated, and feedback strings are then generated for each string independently. The feedback for a particular tag can be overridden by the task author. Note: Since the possible set of tags depends on the several different parameters, it is not easy to know what parameters are available. Currently, the only method of finding out is for the task author to experiment using the test panel for a response area, where the tags can be seen as part of the Raw test response when a test fails. If a response are uses some custom criteria then for each of those criteria the evaluation function can usually output (at least) three tags that consist if the string for the corresponding criterion with either <code>_TRUE</code>, <code>_FALSE</code> or <code>_UNKNOWN</code> appended, e.g. if <code>criteria</code> is , <code>diff(response) = answer, response contains x</code>, then the table below shows the possible tags:</p> Criterion Tags <code>diff(response) = answer</code> <code>diff(response) = answer_TRUE</code>, <code>diff(response) = answer_FALSE</code>, <code>diff(response) = answer_UNKNOWN</code> <code>response contains x</code> <code>response contains x_TRUE</code>, <code>response contains x_FALSE</code>, <code>response contains x_UNKNOWN</code> <p>To change the feedback generated for a particular tag the <code>custom_feedback</code> parameter can be used. The parameter needs to be given a JSON object literal whose keys are tag strings and the values are feedback string.</p> <p>As an example we can use the second response area from part (e). There the <code>criteria</code> parameter was set to <code>response=answer where x=0, response=answer where x=1, response=answer where x=2</code> creating a response area that accepts any function \\(f(x)\\) whose curve passes through the points \\((0,0)\\), \\((1,1)\\) and \\((2,2)\\).</p> <p>For these criteria the table of tags looks like this:</p> Criterion Tags <code>response=answer where x=0</code> <code>response=answer where x=0_TRUE</code>, <code>response=answer where x=0_FALSE</code>, <code>response=answer where x=0_UNKNOWN</code> <code>response=answer where x=1</code> <code>response=answer where x=1_TRUE</code>, <code>response=answer where x=1_FALSE</code>, <code>response=answer where x=1_UNKNOWN</code> <code>response=answer where x=2</code> <code>response=answer where x=2_TRUE</code>, <code>response=answer where x=2_FALSE</code>, <code>response=answer where x=2_UNKNOWN</code> <p>To set custom feedback for the cases where the graph misses any of the three points we can set the <code>custom_feedback</code> parameter as follows:</p> <pre><code>{\n \"response=answer where x=0_FALSE\": \"$f(x)$ gives the wrong value when $x=0$.\",\n \"response=answer where x=1_FALSE\": \"The curve does not pass through the point $(1,1)$.\",\n \"response=answer where x=2_FALSE\": \"Missed the third point.\",\n}\n</code></pre> <p>Note: The evaluation function can generate the tags (and the feedback strings) in any order, thus it is best practice to customise the feedback in such a way that the feedback for each tag can be read independently of the others.</p> <p>Note: The following list of responses can be useful to see the the results of this particular feedback customisation: \\(x\\)</p> <p>\\(x^3-3x^2+3x\\)</p> <p>\\(x^2-2x+2\\)</p> <p>\\(x^2-x\\)</p> <p>\\(x^2\\)</p> <p>\\(0\\)</p> <p>\\(1\\)</p> <p>\\(2\\)</p> <p>\\(3\\)</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-complex-numbers","title":"Using complex numbers","text":"<p>If the parameter <code>complexNumbers</code> is set to <code>true</code> then <code>I</code> will be interpreted as the imaginary constant \\(i\\).</p> <p>Note: When <code>i</code> is used to denote the imaginary constant, then it can end up forming reserved keywords when used with other symbols, e.g. <code>xi</code> will be interpreted as \\(\\xi\\) instead of \\(x \\cdot i\\). To see how to avoid this kind of issue, see Working with Reserved Characters.</p> <p>In this example, <code>complexNumbers</code> is set to <code>true</code> and the answer is <code>2+I</code>. An input symbols has also been added so that <code>I</code> can be replaced with <code>i</code> or <code>j</code>.</p> <p>Any response that is mathematically equivalent \\(2+i\\) to will be accepted, e.g. <code>2+I</code>, <code>2+(-1)^(1/2)</code>, <code>conjugate(2-I)</code>, <code>2sqrt(2)e^(I\\*pi/4)+e^(I\\*3\\*pi/2)</code> or <code>re(2-I)-im(2-I)\\*I</code>.</p> <p>Note: If the particular way that the answer is written matter, e.g. only answers on cartesian form should be accepted, then that requires further configuration, see the example Syntactical comparison.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-constant-and-function-assumptions","title":"Using <code>constant</code> and <code>function</code> assumptions","text":"<p>Examples of how to use the <code>constant</code> and <code>function</code> assumptions for symbols.</p>"},{"location":"user_eval_function_docs/compareExpressions/#comparing-equalities-involving-partial-derivatives-using-the-constant-and-function-assumptions","title":"Comparing equalities involving partial derivatives - Using the <code>constant</code> and <code>function</code> assumptions","text":"<p>The response should be some expression that is equivalent to this equation (i.e. <code>answer</code>):</p> \\[\\frac{\\partial^2 T}{\\partial x^2}+\\frac{\\dot{q}}{k}=\\frac{1}{\\alpha}\\frac{\\partial T}{\\partial t}\\] <p>Here is an example of another valid response:</p> \\[\\alpha k \\frac{\\partial^2 T}{\\partial x^2}+ \\alpha \\dot{q} = k \\frac{\\partial T}{\\partial t}\\] <p>In general, if both the response and the answer are equalities, i.e. the response is \\(a=b\\), and the answer is \\(c=d\\), <code>compareExpressions</code> compares the answer and respose by checking if \\(\\dfrac{a-b}{c-d}\\) is a constant, i.e. it is assumed that \\(a-b\\) and \\(c-d\\) are not simplifyable to zero without assuming \\(a-b=c-d=0\\).</p> <p>By default <code>compareExpressions</code> assumes that symbols are independent of each other, a consequence of this is that derivatives will become zero, e.g. \\(\\dfrac{\\mathrm{d}T}{\\mathrm{d}t} = 0\\). This can be prevented by assuming that some symbols are functions (a symbol assumed to be a function is assumed to depend on all other symbols). In this example we want to take derivatives of \\(T\\) and \\(q\\) so we add <code>('T','function') ('q','function')</code> to the <code>symbol_assumptions</code> parameter.</p> <p>Taking the ratio of the given answer and the example response gives: $$ \\frac{\\frac{\\partial^2 T}{\\partial x^2}+\\frac{\\dot{q}}{k} - \\frac{1}{\\alpha}\\frac{\\partial T}{\\partial t}}{\\alpha k \\frac{\\partial^2 T}{\\partial x^2}+ \\alpha \\dot{q} - k \\frac{\\partial T}{\\partial t}} = \\alpha k $$</p> <p>By default \\(\\alpha\\) and \\(k\\) are assumed to be variables so the ratio is not seen as a constant. This can be fixed by adding <code>('alpha','constant') ('k','constant')</code> to the <code>symbol_asssumptions</code> parameter.</p> <p>The make it simpler and more intuitive to write valid responses we add the following input symbols:</p> Symbol Code Alternatives \\(\\dot{q}\\) <code>Derivative(q(x,t),t)</code> <code>q_{dot}, q_dot</code> \\(\\dfrac{\\mathrm{d}T}{\\mathrm{d}t}\\) <code>Derivative(T(x,t),t)</code> <code>dT/dt</code> \\(\\dfrac{\\mathrm{d}T}{\\mathrm{d}x}\\) <code>Derivative(T(x,t),x)</code> <code>dT/dx</code> \\(\\dfrac{\\mathrm{d}^2 T}{\\mathrm{d}x^2}\\) <code>Derivative(T(x,t),x,x)</code> <code>(d^2T)/(dx^2),  d^2T/dx^2</code> <p>Suggestions of correct responses to try:</p> <p><code>Derivative(T(x,t),x,x) + Derivative(q(x,t),t)/k = 1/alpha*Derivative(T(x,t),t)</code></p> <p><code>alpha*k*(d^2T)/(dx^2) = k*(dT/dt) - alpha*q_dot</code></p> <p><code>d^2T/dx^2 + q_dot/k = 1/alpha*(dT/dt)</code></p> <p><code>(d^2T)/(dx^2) + q_dot/k = 1/alpha*(dT/dt)</code></p> <p>A simple example of an incorrect expression:</p> <p><code>k*alpha*(d^2T)/(dx^2) = k*(dT/dt) + alpha*q_dot</code></p> <p>Note that omitting the arguments of functions (when not using an alias) can cause errors.</p> <p>Compare what happens with response</p> <p><code>Derivative(T(x,t),x,x) + Derivative(q(x,t),t)/k = 1/alpha*Derivative(T(x,t),t)</code></p> <p>and</p> <p><code>Derivative(T,x,x) + Derivative(q,t)/k = 1/alpha*Derivative(T,t)</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#syntactical-comparison","title":"Syntactical comparison","text":"<p>Typically <code>compareExpressions</code> only checks if the response is mathematically equivalent to the answer. If we want to require that the answer is written in a certain way, e.g. Cartesian form vs. exponential form of a complex number or standard form vs factorized form of a polynomial, further comparisons need to be done. There are some built in standard forms that can be detected, as well as a method that tries to match the way that the response is written in a limited fashion. Either method can be activated either by setting the flag <code>syntactical_comparison</code> to <code>true</code>, or by using the criteria <code>response written as answer</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#standard-forms-for-complex-numbers","title":"Standard forms for complex numbers","text":"<p>For complex numbers there are two known forms, Cartesian form, \\(a+bi\\), and exponential form, \\(ae^{bi}\\).</p> <p>For either form the pattern detection only works if \\(a\\) and \\(b\\) are written as numbers on decimal form, i.e. no fractions or other mathematical expressions.</p> <p>The example set has two response areas, one with an answer written in Cartesian form (\\(2+2i\\)) and one with the answer written in exponential form (\\(2e^{2i}\\)). For both response areas the parameter <code>complexNumbers</code> is set to <code>true</code> (so that <code>I</code> will be treated as the imaginary constant) and <code>syntactical_comparison</code> is set to <code>true</code> (to activate the syntactical comparison). There is also an input symbol that makes <code>I</code>, <code>i</code> and <code>j</code> all be interpreted as the imaginary constant \\(i\\). The evaluation function automatically detects the form of the answer and uses it as the basis of the comparison.</p>"},{"location":"user_eval_function_docs/compareExpressions/#arbitrary-syntactical-comparison-by-comparing-the-form-of-the-response-to-the-form-of-the-answer","title":"Arbitrary syntactical comparison by comparing the form of the response to the form of the answer","text":"<p>The criteria keyword <code>written as</code> can be used for more general syntactical comparison, see Examples: Customizing comparisons using criteria: Using <code>written as</code> to syntactical comparison for details.</p>"},{"location":"user_eval_function_docs/compareExpressions/#custom-feedback-based-on-response-evaluation-results","title":"Custom feedback based on response evaluation results","text":"<p>For incorrect responses the feedback generated by the evaluation function can be replaced. Either the same feedback can be shown for any incorrect answer or the feedback generated for specific properties for the given response.</p> <p>To give custom feedback for any incorrect response, set the parameter <code>feedback_for_incorrect_response</code> to the desired feedback string.</p> <p>For the response area below <code>feedback_for_incorrect_response</code> is set to \"This message will be displayed for any incorrect response.\" and the answer is \\(x\\).</p> <p>Some more feedback customisation is shown in Examples: Customizing comparison using criteria: Customising feedback based on satisfied criteria.</p>"},{"location":"user_eval_function_docs/compareExpressions/#using-integrals","title":"Using integrals","text":"<p>The evaluation function can handle one-dimensional definite integrals, i.e. expression in the form \\(\\int_a^b f(x) \\mathrm{d}x\\), if the <code>elementary_functions</code> parameter is set to true. The integrand and the boundary values can be symbolic.</p> <p>Note: Indefinite integrals (expression in the form \\(\\int f(x) \\mathrm{d}x\\)), contour integrals (\\(\\oint f(x) \\mathrm{d}x\\)) and integrals based on abstract measures (\\(\\int_A f(x) \\mathrm{d}\\mu\\)) are not supported.</p> <p>The expression \\(\\int_a^b f(x) \\mathrm{d}x\\) can be written <code>Integral(f(x), (x, a, b))</code>. The syntax works as follows: the integral sign corresponds to <code>Integral</code> (the short form <code>int</code> can also be used), which must be followed by two argument, first is the integrand (the function that is integrated), the second is a triple containing; the variable to be integrated over and the two boundary values.</p> <p>Here is an example of an integral that can be fully evaluated, more specifically \\(\\int_0^2 3xy \\mathrm{d}x = 6y\\). If the answer is set to <code>Integral(3xy, (x, 0, 2))</code> then response area will accept both integral expressions, e.g. <code>int(3*y*x, (x, 0, 2))</code>, and computed expressions, e.g. <code>6y</code>.</p> <p>The boundary and function does not need to be defined explicitly. As an example of a more abstract integral we can consider \\(\\int_a^b f(x)+g(x) \\mathrm{d}x\\). If the answer is set to <code>Integral(f(x)+g(x), (x, a, b))</code> then, for example, <code>int(g(x)+f(x), (x, a, b))</code> \\(\\int_a^b g(x)+f(x) \\mathrm{d}x\\) and <code>int(f(x), (x, a, b)) + int(g(x), (x, a, b))</code> \\(\\int_a^b f(x) \\mathrm{d}x+\\int_a^b g(x) \\mathrm{d}x\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#working-with-reserved-characters","title":"Working with Reserved Characters","text":"<p>SymPy recommends that you not use I, E, S, N, C, O, or Q for variable or symbol names, as those are used for the imaginary unit (\\(i\\)), the base of the natural logarithm (\\(e\\)), the sympify function (<code>S()</code>), numeric evaluation (<code>N()</code>), the big O order symbol (as in \\(O(n)\\)), and the assumptions object that holds a list of supported ask keys (such as <code>Q.real</code>), respectively. You can use the mnemonic OSINEQ to remember what Symbols are defined by default in SymPy. Or better yet, always use lowercase letters for Symbol names. Python will not prevent you from overriding default SymPy names or functions, so be careful.\" For more information checkout the SymPy Docs</p> <p>If you want to use a symbol that is usually reserved for some reserved character, e.g. \u2018E\u2019, do as follows: 1. Create an input symbol where the code is different that the symbol you want to use, e.g. \u2018Ef\u2019 or 'Euler' instead of \u2018E\u2019 2. Add the symbol you want to use as an alternative, e.g. the alternatives could be set to \u2018E\u2019</p>"},{"location":"user_eval_function_docs/compareExpressions/#example","title":"Example:","text":"<p>For the answer: \\(A/(\\epsilon*l)\\), <code>e</code> is reserved as Euler's number, so we replace <code>e</code> with <code>ef</code> or any other character(s) that are not reserved or used in the expression and provide alternatives as input symbols:</p> <p>Symbol: \\(\\epsilon\\)</p> <p>Code: <code>ef</code></p> <p>Alternatives: <code>\u03f5,\u03b5,E,e,Ep</code></p> <p>Here the answer \\(A/(ef*l)\\) is marked as correct, and so are the alternatives: - \\(A/(\u03f5*l)\\) - \\(A/(\u03b5*l)\\) - \\(A/(E*l)\\) - \\(A/(e*l)\\) - \\(A/(Ep*l)\\)</p>"},{"location":"user_eval_function_docs/compareExpressions/#overriding-greek-letters-or-other-reserved-symbols-with-input-symbols","title":"Overriding greek letters or other reserved symbols with input symbols","text":"<p>Sometimes there can be ambiguities in the expected responses. For example <code>xi</code> in a response could either be interpreted as the greek letter \\(\\xi\\) or as the multiplication \\(x \\cdot i\\).</p> <p>If there is an ambiguity the parser will choose the longest corresponding string, in the example above that means that <code>xi</code> will be interpreted as \\(\\xi\\) rather than \\(x \\cdot i\\).</p> <p>This behaviour can in many cases be overridden using input symbols, by letting the meaning with the longer corresponding string be an alternative to the interpretation consisting of several shorter strings. In the above example adding an input symbol with code <code>x\\*i</code> and alternative <code>xi</code> will ensure that <code>xi</code> will be interpreted as  \\(x \\cdot i\\) rather than \\(\\xi\\).</p> <p>In this example the answer is set to \\(5 x \\mathbf{i}-12 y \\mathbf{j}\\), while <code>strict_syntax</code> is set to false and <code>elementary_functions</code> is set to true.</p> <p>In order for the response <code>5xi-12yj</code> to be interpreted as \\(5 x \\mathbf{i}-12 y \\mathbf{j}\\) instead of \\(5 \\xi-12 y \\mathbf{j}\\) the following input symbols have been added:</p> Symbol Code Alternatives <code>\\$x\\$</code> x <code>\\$\\\\mathbf{i}\\$</code> i <code>\\$x \\\\mathbf{i}\\$</code> x*i xi <code>\\$y\\$</code> y <code>\\$\\\\mathbf{j}\\$</code> j"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/","title":"ComparePhysicalQuantities","text":"<p>Edit on GitHub  View Code </p> <p>Evaluation function which proveds some basic some dimensional analysis functionality.</p> <ul> <li>DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></li> <li>Substitutions of symbols before comparison of expressions is done</li> <li>Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem</li> </ul> <p>Note: When the <code>quantities</code> grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example <code>Nm/s</code> or <code>Newton*metre/SECOND</code> will not be handled correctly, but <code>newton*metre/second</code> will.</p> <p>Note: Prefixes have lower precedence than exponentiation, e.g. <code>10*cm**2</code> will be interpreted as \\(10 \\cdot 10^{-2} \\mathrm{metre}^2\\) rather than \\(10 (10^(-2)\\mathrm{metre})^2\\).</p> <p>Note: This function allows omitting <code>*</code> and using <code>^</code> instead of <code>**</code> if the grading parameter <code>strict_syntax</code> is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities).</p> <p>Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g.  e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. <code>m</code>) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Longer short form symbols take precedence over shorter short forms, e.g. <code>sr</code> will be interpreted as <code>steradian</code> instead of <code>second radian</code>.</p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p> <p>Note: When running the unit test some tests are expected to take much longer than the other. These tests can be skipped by adding <code>skip_resource_intensive_tests</code> as a command line argument to improve iteration times.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#changing-default-feedback-messages","title":"Changing default feedback messages","text":"<p>The feedback messages can be set on a per-task basis (see description of the <code>custom_feedback</code> input parameter).</p> <p>The default feedback messages are defined in <code>feedback_responses_list</code> defined near the top of <code>evaulation.py</code>, which contains a list of dictionaries of feedback responses that are used througout the code. All feedback messages visible to learners are defined in these dictionaries. The entries in the dictionaries are either be string of functions that return strings.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#inputs","title":"Inputs","text":"<p>All input parameters need to be supplied via the Grading parameters panel.</p> <p>There are seven optional parameters that can be set: <code>elementary_functions</code>, <code>substitutions</code>, <code>quantities</code>, <code>strict_syntax</code>, <code>rtol</code>, <code>atol</code> and <code>comparison</code>.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#custom_feedback","title":"<code>custom_feedback</code>","text":"<p>Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback.</p> <p>The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","title":"Feedback tags for all comparisons","text":"<ul> <li><code>PARSE_ERROR_WARNING</code> Response cannot be parsed as an expression or physical quantity.</li> <li><code>PER_FOR_DIVISION</code> Warns about risk of ambiguity when using <code>per</code> instead <code>/</code> for division.</li> <li><code>STRICT_SYNTAX_EXPONENTIATION</code> Warns that <code>^</code> cannot be used for exponentiation when <code>strict_syntax</code> is set to <code>true</code>.</li> <li><code>QUANTITIES_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of quantities could not be parsed.</li> <li><code>SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of substitutions could not be parsed.</li> </ul>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","title":"Feedback tags for <code>buckinghamPi</code> comparison","text":"<ul> <li><code>VALID_CANDIDATE_SET</code> Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text.</li> <li><code>NOT_DIMENSIONLESS</code> Message displayed when at least one groups is not dimensionless.</li> <li><code>MORE_GROUPS_THAN_REFERENCE_SET</code> Message displayed when the response contains more groups than necessary.</li> <li><code>CANDIDATE_GROUPS_NOT_INDEPENDENT</code> Message displayed when the groups in the response are not independent.</li> <li><code>TOO_FEW_INDEPENDENT_GROUPS</code> Message displayed when the response contains fewer groups than necessary.</li> <li><code>UNKNOWN_SYMBOL</code> Message displayed when the response contains some undefined symbol.</li> <li><code>SUM_WITH_INDEPENDENT_TERMS</code>  Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.</li> </ul>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#substitutions","title":"<code>substitutions</code>","text":"<p>String that lists all substitutions that should be done to the answer and response inputs before processing.</p> <p>Each substitution should be written in the form <code>('original string','substitution string')</code> and all pairs concatenated into a single string. Substitutions can be grouped by adding <code>|</code> between two substitutions. Then all substitutions before <code>|</code> will be performed before the substitutions after <code>|</code>.</p> <p>The input can contain an arbitrary number of substitutions and <code>|</code> symbols.</p> <p>Note that using substitutions will replace all default definitions of quantities and dimensions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#quantities","title":"<code>quantities</code>","text":"<p>String that lists all quantities that can be used in the answer and response.</p> <p>Each quantity should be written in the form <code>('quantity name','(units)')</code> and all pairs concatenated into a single string. See tables below for available default units.</p> <p>Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities.</p> <p>NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question.</p> <p>If the <code>comparison</code> parameter is set to <code>dimensions</code>, it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions.</p> <p>If the <code>comparison</code> parameter is set to <code>buckinghamPi</code>, then <code>quantities</code> should be set in a different way. See the detailed description of <code>buckinghamPi</code> further down.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html</p> SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that degrees Celsius is omitted.</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html</p> <p>Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*kilo*metre/second**2</code> is accepted but <code>10 kilometre/second^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#rtol","title":"<code>rtol</code>","text":"<p>Maximum relative error allowed when comparing expressions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#atol","title":"<code>atol</code>","text":"<p>Maximum absolute error allowed when comparing expressions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#comparison","title":"<code>comparison</code>","text":"<p>Parameter that determines what kind of comparison is done. There are four possible options:</p> <ul> <li><code>expression</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the <code>atol</code> and <code>rtol</code> parameters).</li> <li><code>expressionExact</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible.</li> <li><code>dimensions</code> Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities.</li> <li><code>buckinghamPi</code> Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</li> </ul> <p>For more details on each options see the description below and the corresponding examples.</p> <p>If <code>comparison</code> is not specified it defaults to <code>expression</code>.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expression","title":"<code>expression</code>","text":"<p>Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close.</p> <p>How big the difference is between the value of the answer and the value of the response is decided by the <code>rtol</code> and <code>atol</code> parameters. If neither <code>atol</code> nor <code>rtol</code> is specified the function will allow a relative error of \\(10^{-12}\\). If <code>atol</code> is specified its value will be interpreted as the maximum allowed absolute error. If <code>rtol</code> is specified its value will be interpreted as the maximum allowed relative error. If both <code>atol</code> and <code>rtol</code> the function will check both the absolute and relative error.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expressionexact","title":"<code>expressionExact</code>","text":"<p>Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#dimensions","title":"<code>dimensions</code>","text":"<p>Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities.</p> <p>With this option the quantities (specified by the <code>quantities</code> parameter) can be given either dimension only, or units.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#buckinghampi","title":"<code>buckinghamPi</code>","text":"<p>Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</p> <p>There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and set answer to <code>-</code>. The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response.</p> <p>Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/","title":"ComparePhysicalQuantities","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>NUMERIC_UNITS</code></li> <li><code>MATH_SINGLE_LINE</code></li> </ul> <p>Evaluation function which proveds some basic some dimensional analysis functionality.</p> <ul> <li>DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></li> <li>Substitutions of symbols before comparison of expressions is done</li> <li>Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem</li> </ul> <p>Note: When the <code>quantities</code> grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example <code>Nm/s</code> or <code>Newton*metre/SECOND</code> will not be handled correctly, but <code>newton*metre/second</code> will.</p> <p>Note: Prefixes have lower precedence than exponentiation, e.g. <code>10*cm**2</code> will be interpreted as \\(10 \\cdot 10^{-2}~\\mathrm{metre}^2\\) rather than \\(10 \\cdot (10^{-2}~\\mathrm{metre})^2\\).</p> <p>Note: This function allows omitting <code>*</code> and using <code>^</code> instead of <code>**</code> if the grading parameter <code>strict_syntax</code> is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities).</p> <p>Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g.  e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. <code>mug</code> will be interpreted as <code>micro*gram</code> instead <code>metre*astronomicalunit*gram</code>. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. <code>m</code>) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Longer short form symbols take precedence over shorter short forms, e.g. <code>sr</code> will be interpreted as <code>steradian</code> instead of <code>second radian</code>.</p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#inputs","title":"Inputs","text":"<p>All input parameters need to be supplied via the Grading parameters panel.</p> <p>There are seven optional parameters that can be set: <code>elementary_functions</code>, <code>substitutions</code>, <code>quantities</code>, <code>strict_syntax</code>, <code>rtol</code>, <code>atol</code> and <code>comparison</code>.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#custom_feedback","title":"<code>custom_feedback</code>","text":"<p>Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback.</p> <p>The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","title":"Feedback tags for all comparisons","text":"<ul> <li><code>PARSE_ERROR_WARNING</code> Response cannot be parsed as an expression or physical quantity.</li> <li><code>PER_FOR_DIVISION</code> Warns about risk of ambiguity when using <code>per</code> instead <code>/</code> for division.</li> <li><code>STRICT_SYNTAX_EXPONENTIATION</code> Warns that <code>^</code> cannot be used for exponentiation when <code>strict_syntax</code> is set to <code>true</code>.</li> <li><code>QUANTITIES_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of quantities could not be parsed.</li> <li><code>SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of substitutions could not be parsed.</li> </ul>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","title":"Feedback tags for <code>buckinghamPi</code> comparison","text":"<ul> <li><code>VALID_CANDIDATE_SET</code> Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text.</li> <li><code>NOT_DIMENSIONLESS</code> Message displayed when at least one groups is not dimensionless.</li> <li><code>MORE_GROUPS_THAN_REFERENCE_SET</code> Message displayed when the response contains more groups than necessary.</li> <li><code>CANDIDATE_GROUPS_NOT_INDEPENDENT</code> Message displayed when the groups in the response are not independent.</li> <li><code>TOO_FEW_INDEPENDENT_GROUPS</code> Message displayed when the response contains fewer groups than necessary.</li> <li><code>UNKNOWN_SYMBOL</code> Message displayed when the response contains some undefined symbol.</li> <li><code>SUM_WITH_INDEPENDENT_TERMS</code>  Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.</li> </ul>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#substitutions","title":"<code>substitutions</code>","text":"<p>String that lists all substitutions that should be done to the answer and response inputs before processing.</p> <p>Each substitution should be written in the form <code>('original string','substitution string')</code> and all pairs concatenated into a single string. Substitutions can be grouped by adding <code>|</code> between two substitutions. Then all substitutions before <code>|</code> will be performed before the substitutions after <code>|</code>.</p> <p>The input can contain an arbitrary number of substitutions and <code>|</code> symbols.</p> <p>Note that using substitutions will replace all default definitions of quantities and dimensions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#quantities","title":"<code>quantities</code>","text":"<p>String that lists all quantities that can be used in the answer and response.</p> <p>Each quantity should be written in the form <code>('quantity name','(units)')</code> and all pairs concatenated into a single string. See tables below for available default units.</p> <p>Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities.</p> <p>NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question.</p> <p>If the <code>comparison</code> parameter is set to <code>dimensions</code>, it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions.</p> <p>If the <code>comparison</code> parameter is set to <code>buckinghamPi</code>, then <code>quantities</code> should be set in a different way. See the detailed description of <code>buckinghamPi</code> further down.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html</p> SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that degrees Celsius is omitted.</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html</p> <p>Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*kilo*metre/second**2</code> is accepted but <code>10 kilometre/second^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#rtol","title":"<code>rtol</code>","text":"<p>Maximum relative error allowed when comparing expressions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#atol","title":"<code>atol</code>","text":"<p>Maximum absolute error allowed when comparing expressions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#comparison","title":"<code>comparison</code>","text":"<p>Parameter that determines what kind of comparison is done. There are four possible options:</p> <ul> <li><code>expression</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the <code>atol</code> and <code>rtol</code> parameters).</li> <li><code>expressionExact</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible.</li> <li><code>dimensions</code> Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities.</li> <li><code>buckinghamPi</code> Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</li> </ul> <p>For more details on each options see the description below and the corresponding examples.</p> <p>If <code>comparison</code> is not specified it defaults to <code>expression</code>.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expression","title":"<code>expression</code>","text":"<p>Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close.</p> <p>How big the difference is between the value of the answer and the value of the response is decided by the <code>rtol</code> and <code>atol</code> parameters. If neither <code>atol</code> nor <code>rtol</code> is specified the function will allow a relative error of \\(10^{-12}\\). If <code>atol</code> is specified its value will be interpreted as the maximum allowed absolute error. If <code>rtol</code> is specified its value will be interpreted as the maximum allowed relative error. If both <code>atol</code> and <code>rtol</code> the function will check both the absolute and relative error.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expressionexact","title":"<code>expressionExact</code>","text":"<p>Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#dimensions","title":"<code>dimensions</code>","text":"<p>Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities.</p> <p>With this option the quantities (specified by the <code>quantities</code> parameter) can be given either dimension only, or units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#buckinghampi","title":"<code>buckinghamPi</code>","text":"<p>Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</p> <p>There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and set answer to <code>-</code>. The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response.</p> <p>Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#1-checking-the-dimensions-of-an-expression-or-physical-quantity","title":"1 Checking the dimensions of an expression or physical quantity","text":"<p>DEPRECATED</p> <p>RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></p> <p>This example will check if the response has dimensions \\(\\frac{\\mathrm{length}^2}{\\mathrm{time}^2}\\).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a","title":"a)","text":"<p>To check an expression there needs to be some predefined quantities that can be used in the expression. Since only dimensions will be checked units are not necessary (but could be used as well).</p> <p>Here a response area with input type <code>TEXT</code> and two grading parameters, <code>quantities</code> and <code>comparison</code>, will be used.</p> <p><code>quantities</code> is defined as follows: <pre><code>('d','(length)') ('t','(time)') ('v','(length/time)')\n</code></pre></p> <p><code>comparison</code> is set to <code>dimensions</code>.</p> <p>The answer is set two some expression with the right dimensions, e.g. <code>v**2</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>v**2</code> <code>v^2</code> <code>5*v**2</code> <code>5v^2</code> <code>(d/t)**2+v**2</code> <code>(d/t)^2+v^2</code> <code>d**2/t**2</code> <code>d^2/t^2</code> <code>d**2*t**(-2)</code> <code>d^2 t^(-2)</code> <code>d/t*v</code> <code>vd/t</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b","title":"b)","text":"<p>Checking the dimensions of a quantity directly, i.e. the dimensions of an expression of the form <code>number*units</code>, no predefined quantities are necessary.</p> <p>Here a response area with input type <code>TEXT</code> and one grading parameter,<code>comparison</code>, will be used.</p> <p><code>comparison</code> is set to <code>dimensions</code>.</p> <p>The answer is set two some expression with the right dimensions, e.g. <code>length**2/time**2</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected in the answer we do not need to set any input symbols.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>metre**2/second**2</code> <code>metre^2/second^2</code> <code>m^2/s^2</code> <code>(centi*metre)**2/hour**2</code> <code>(centimetre)^2/h^2</code> <code>(cm)^2/h^2</code> <code>246*ohm/(kilo*gram)*coulomb**2/second</code> <code>246 ohm/(kilogram) coulomb^2/second</code> <code>246 O/(kg) c^2/s</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#2-checking-the-value-of-an-expression-or-a-physical-quantity","title":"2 Checking the value of an expression or a physical quantity","text":"<p>DEPRECATED</p> <p>RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></p> <p>This examples checks if your expression is equal to \\(2~\\frac{\\mathrm{kilometre}}{\\mathrm{hour}}\\).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_1","title":"a)","text":"<p>Here an expression with predefined quantities is checked as exactly as possible. This is done with a TEXT response area with the following parameters: <code>quantities</code> is set to: <pre><code>('d','(length)') ('t','(time)') ('v','(length/time)')\n</code></pre> Note that short form symbols cannot be used when defining quantities.</p> <p><code>comparison</code> is set to <code>expressionExact</code>.</p> <p>The response area answer is set to <code>2*v</code> but there are many other expressions that would work just as well. Note that we cannot write <code>2*kilo*metre/second</code> as response or answer since the predefined quantity <code>t</code> will substitute the <code>t</code> in <code>metre</code> which results in unparseable input.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units and single character symbols are expected in the answer we will not set the grading parameter <code>symbols</code>.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>2*v</code> <code>2v</code> <code>2000/3600*d/t</code> <code>2000/3600 d/t</code> <code>1/1.8*d/t</code> <code>d/(1.8t)</code> <code>v+1/3.6*d/t</code> <code>v+d/(3.6t)</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_1","title":"b)","text":"<p>Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed absolute tolerance of \\(0.05 \\frac{metre}{second}\\) can be done with a TEXT response area with <code>atol</code> set to <code>0.05</code> and the answer set to <code>2*kilo*metre/hour</code>.</p> <p>Note: <code>atol</code> is always assumed to be given in the base SI units version of the expression. This is likely to change in future versions of the function.</p> <p>The <code>comparison</code> parameter could also be set to <code>expression</code> but since this is the default it is not necessary.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected in the answer no input symbols are necessary.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>0.556*metre/second</code> <code>0.556 metre/second</code> <code>0.556 m/s</code> <code>0.560*metre/second</code> <code>0.560 metre/second</code> <code>0.560 m/s</code> <code>0.6*metre/second</code> <code>0.6 metre/second</code> <code>0.6 m/s</code> <code>2*kilo*metre/hour</code> <code>2 kilometre/hour</code> <code>2 km/h</code> <code>1.9*kilo*metre/hour</code> <code>1.9 kilometre/hour</code> <code>1.9 km/h</code> <code>2.1*kilo*metre/hour</code> <code>2.1 kilometre/hour</code> <code>2.1 km/h</code> <p>In the example given in the example problem set, the following responses are tested and evaluated as incorrect:</p> Strict syntax Relaxed syntax Using symbols <code>0.61*metre/second</code> <code>0.61 metre/second</code> <code>0.61 m/s</code> <code>2.2*kilo*metre/hour</code> <code>2.2 kilometre/hour</code> <code>2.2 km/h</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c","title":"c)","text":"<p>Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed relative tolerance of \\(0.05\\) can be done with a TEXT response area with <code>rtol</code> set to <code>0.05</code> and the answer set to <code>2*kilo*metre/hour</code>.</p> <p>The <code>comparison</code> parameter could also be set to <code>expression</code> but since this is the default it is not necessary.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>0.533*metre/second</code> <code>0.533 metre/second</code> <code>0.533 m/s</code> <code>2.08*kilo*metre/hour</code> <code>2.08 kilometre/hour</code> <code>2.08 km/h</code> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected it is not necessary to set any input symbols.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as incorrect:</p> Strict syntax Relaxed syntax Using symbols <code>0.522*metre/second</code> <code>0.522 metre/second</code> <code>0.522 m/s</code> <code>2.11*kilo*metre/hour</code> <code>2.11 kilometre/hour</code> <code>2.11 km/h</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#3-checking-if-a-set-of-quantities-match-the-buckingham-pi-theorem","title":"3 Checking if a set of quantities match the Buckingham pi theorem","text":""},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_2","title":"a)","text":"<p>In this example the task is: Given \\(U\\), \\(L\\) and \\(\\nu\\), suggest a dimensionless group.</p> <p>For this problem we do not need to predefine any quantities and give exact dimensions. The algorithm assumes that all symbols in the answer (that are not numbers or predefined constants such as \\(\\pi\\)) are quantities and that there are no other quantities that should appear in the answer.</p> <p>Note: This means that the algorithm does not in any way check that the stated answer is dimensionless, ensuring that that is left to the problem author.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code> and answer set to <code>['U*L/nu']</code>. It is not necessary to use this specific answer, any example of a correct dimensionless group should work.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_2","title":"b)","text":"<p>In this example the task is: Given \\(U\\), \\(L\\), \\(\\nu\\) and \\(f\\), determine the necessary number of dimensionless groups and give one example of possible expressions for them.</p> <p>This task is similar to example a) with two significant differences. First, adding \\(f\\) means that there are now two groups required, and second the problem will constructed by defining the quantities and let the function compute the rest on its own instead of supplying a reference example.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code>, <code>quantities</code> set to <code>('U','(length/time)') ('L','(length)') ('nu','(length**2/time)') ('f','(1/time)')</code> and <code>answer</code> set to <code>-</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c_1","title":"c)","text":"<p>In this example the task is: Suppose we are studying water waves that move under the influence of gravity. We suppose that the variables of interest are the acceleration in free fall \\(g\\), the velocity of the wave \\(v\\), the height of the wave \\(h\\) and the wave length \\(\\ell\\). We also suppose that they are related by a dimensionally consistent equation \\(f(g,v,h,l) = 0\\). Determine the minimum number of dimensionless \\(\\pi\\)-variables needed to describe this problem according to the Buckingham pi-theorem and give one example of possible expressions for the dimensionless quantities.</p> <p>For this problem two dimensionless groups are needed, see the worked solution for a terse solution that gives the general form of the dimensionless quantities.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code> and then give a list of correct group expressions formatted as the code for a python list. For this example the answer <code>['g**(-2)*v**4*h*l**3', 'g**(-2)*v**4*h**2*l**4']</code> was used (this corresponds to \\(p_1 = 1\\), \\(p_2 = 2\\), \\(q_1 = 3\\), \\(q_2 = 4\\) in the worked solution). The feedback was costumized by setting the <code>custom_feedback</code> parameter too: <code>\"custom_feedback\": { \"VALID_CANDIDATE_SET\": \"Your list of power products satisfies the Buckingham Pi theorem.\", \"NOT_DIMENSIONLESS\": \"At least one power product is not dimensionless.\", \"MORE_GROUPS_THAN_REFERENCE_SET\": \"Response has more power products than necessary.\", \"CANDIDATE_GROUPS_NOT_INDEPENDENT\": \"Power products in response are not independent.\", \"TOO_FEW_INDEPENDENT_GROUPS\": \"Candidate set contains too few independent groups.\", \"UNKNOWN_SYMBOL\": \"One of the prower products contains an unkown symbol.\", \"SUM_WITH_INDEPENDENT_TERMS\": \"The candidate set contains an expression which contains more independent terms that there are groups in total. The candidate set should ideally only contain expressions written as power products.\" }</code></p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#4-defining-costum-sets-of-units","title":"4 Defining costum sets of units","text":"<p>In this problem it is demonstrated how to use <code>substitutions</code> to define costum units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_3","title":"a)","text":"<p>In this problem currencies will be us as units, and thus the quantities will no longer be physical.</p> <p>Here the <code>substitutions</code> parameter will be set so that the evaluation function can be used to compare. Note that using <code>substitutions</code> this way means that the default SI units can no longer be used.</p> <p>The following exchange rates (from Bank of England 1 August 2022) will be used:</p> Currency Exchange rate \\(1\\) EUR \\(1.1957\\) GBP \\(1\\) USD \\(1.2283\\) GBP \\(1\\) CNY \\(8.3104\\) GBP \\(1\\) INR \\(96.943\\) GBP <p>To compare prices written in different currencies a reference currency needs to be chosen. In this case GBP will be used. To substitute other currencies for their corresponding value in GBP the following grading parameter can be used: <pre><code>\"substitutions\":\"('EUR','(1/1.1957)*GBP') ('USD','(1/1.2283)*GBP') ('CNY','(1/8.3104)*GBP') ('INR','(1/96.9430)*GBP')\"\n</code></pre> Since these conversion are not exact and for practical purposes prices are often not gives with more than two decimals of precision we also want to set the absolute tolerance, <code>atol</code>, to \\(0.05\\).</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. By setting the grading parameter <code>strict_syntax</code> to false the <code>*</code> can be omitted and <code>^</code> can be used instead of <code>**</code>. To ensure that this works correctly it is necessary to list the multicharacter symbols that are expected to appear in the answer and response as input symbols. For this example this means setting <code>EUR</code>, <code>USD</code>, <code>CNY</code> and <code>INR</code> as codes for inut symbols.</p> <p>In the example given in the example problem set, the answer set to <code>10*GBP</code> and the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>11.96*EUR</code> <code>11.96 EUR</code> <code>12.28*USD</code> <code>12.28 USD</code> <code>83.10*CNY</code> <code>83.10 CNY</code> <code>969.43*INR</code> <code>969.43 INR</code>"},{"location":"dev_eval_function_docs/symbolicEqual/","title":"SymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Evaluates the equality between two symbolic expressions using the python <code>SymPy</code> package.</p> <p>Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#inputs","title":"Inputs","text":""},{"location":"dev_eval_function_docs/symbolicEqual/#optional-grading-parameters","title":"Optional grading parameters","text":"<p>There are eight optional parameters that can be set: <code>complexNumbers</code>, <code>elementary_functions</code>, <code>specialFunctions</code>, <code>strict_syntax</code>,  <code>symbol_assumptions</code>, <code>multiple_answers_criteria</code>, <code>plus_minus</code> and <code>minus_plus</code>.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"dev_eval_function_docs/symbolicEqual/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumption names can be found in this list:  <code>SymPy Assumption Predicates</code></p>"},{"location":"dev_eval_function_docs/symbolicEqual/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>eval</code> command will feature:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"response_latex\": \"&lt;str&gt;\",\n    \"response_simplified\": \"&lt;str&gt;\",\n    \"level\": \"&lt;int&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/symbolicEqual/#response_latex","title":"<code>response_latex</code>","text":"<p>This is a latex string, indicating how the user's <code>response</code> was understood by SymPy. It can be used to provide feedback in the front-end.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#level","title":"<code>level</code>","text":"<p>The function tests equality using three levels, of increasing complexity. This parameter indicates the level at which equality was found. It is not present if the result is incorrect.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#response_simplified","title":"<code>response_simplified</code>","text":"<p>This is a math-simplified string of the given response. All mathematically-equivalent expressions will yield identical strings under this field. This can be used by teacher dashboards when aggregating common student errors.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/symbolicEqual/","title":"SymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>MATH_SINGLE_LINE</code></li> </ul> <p>The function has been deprecated.</p> <p>compareExpressions is a second-generation evaluation function that is backwards compatible with symbolicEqual and provides more advanced capabilities.</p> <p>This function utilises the <code>SymPy</code> to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p> <p>Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equations as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\).</p>"},{"location":"user_eval_function_docs/symbolicEqual/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/symbolicEqual/#optional-grading-parameters","title":"Optional grading parameters","text":"<p>There are eight optional parameters that can be set: <code>complexNumbers</code>, <code>elementary_functions</code>, <code>specialFunctions</code>, <code>strict_syntax</code>,  <code>symbol_assumptions</code>, <code>multiple_answers_criteria</code>, <code>plus_minus</code> and <code>minus_plus</code>.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"user_eval_function_docs/symbolicEqual/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumption names can be found in this list:  <code>SymPy Assumption Predicates</code></p>"},{"location":"user_eval_function_docs/symbolicEqual/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#1-setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","title":"1 Setting input symbols to be assumed positive to avoid issues with fractional powers","text":"<p>In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a &gt; 0\\) and \\(b &gt; 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\). The same is true for other fractional powers.</p> <p>So if expression like these are expected in the answer and/or response then it is a good idea to use the <code>symbol_assumptions</code> parameter to note that \\(a &gt; 0\\) and \\(b &gt; 0\\). This can be done by setting <code>symbol_assumptions</code> to <code>('a','positive') ('b','positive')</code>.</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>SymbolicEqual</code> with answer <code>sqrt(a/b)</code>, <code>strict_syntax</code> set to false and <code>symbol_assumptions</code> set as above. Some examples of expressions that are accepted as correct: <code>sqrt(a)/sqrt(b)</code>, <code>(a/b)**(1/2)</code>, <code>a**(1/2)/b**(1/2)</code>, <code>(a/b)^(0.5)</code>, <code>a^(0.5)/b^(0.5)</code></p>"},{"location":"dev_eval_function_docs/shortTextAnswer/","title":"ShortTextAnswer","text":"<p>Edit on GitHub  View Code </p> <p>This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#inputs","title":"Inputs","text":"<p><code>keystrings</code> - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#keystring-object","title":"<code>keystring</code> object","text":"<p>The <code>keystring</code> object contains several fields which affect how it will be interpreted:</p> <ul> <li><code>string</code> - Required. The actual keystring being searched for.</li> <li><code>exact_match</code> - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to <code>false</code></li> <li><code>should_contain</code> - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to <code>true</code>. Setting this flag to false indicates that a correct response will not contain the specified keystring.</li> <li><code>custom_feedback</code> - Optional. A feedback string to be returned if the <code>string</code> was not found (or if it was, in case <code>should_contain</code> was set to <code>false</code>). Defaults to <code>None</code>, in which case a generic response will be generated containing the string searched for.</li> </ul>"},{"location":"dev_eval_function_docs/shortTextAnswer/#outputs","title":"Outputs","text":"<p>The function will return an object with 3 fields of interest. the <code>is_correct</code> and <code>feedback</code> fields are required by LambdaFeedback to present feedback to the user. The <code>result</code> field is only used for development. <pre><code>{\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"result\": {\n        \"response\": \"&lt;string&gt;\",\n        \"processing_time\": \"&lt;double&gt;\",\n    },\n    \"feedback\": \"string\"\n}\n</code></pre></p> <ul> <li><code>response</code> - The student answer. USed for debugging purposes.</li> <li><code>processing_time</code> - The time it took for the function to evaluate</li> </ul> <p>If the function identified a problematic keystring, the result object will have an additional field: * <code>keystring-scores</code> - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer.</p> <p>Otherwise, it will have the additional fields: * <code>method</code> - string. Either \"w2v\" or \"BOW vector similarity\". * <code>similarity_value</code> - double. The similarity value between the response and the answer.</p> <p>If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#initial-setup","title":"Initial SetUp","text":"<p>Follow Docker Image instructions and run  <code>docker build -t &lt;image_name&gt; .</code> in app/</p> <p>Otherwise if setup locally: 1. create a venv 2. in the venv <code>pip install -r app/requirements.txt</code> 3. if errors encountered with nltk packages, follow <code>testing_nltk.py</code> instructions</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","title":"Example simple input, no keystring","text":"<p>Input <pre><code>{\n    \"response\": \"Density, velocity, viscosity, length\",\n    \"answer\": \"Density, speed, Viscosity, Length\",\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': True, \n    'result': {\n        'response': 'Density, speed, Viscosity, Length',\n        'processing_time': 0.022912219000000178, \n        'method': 'w2v', \n        'similarity_value': 0.9326027035713196}, \n    'feedback': 'Confidence: 0.933%'\n}\n</code></pre></p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-keystring-input","title":"Example keystring input","text":"<p>Input <pre><code>{\n    \"response\": \"Molecules are made out of atoms\",\n    \"answer\": \"Many atoms form a molecule\",\n    'keystrings': [\n        {'string': 'molecule'}, \n        {'string': 'proton', 'exact_match': True}\n    ]\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': False, \n    'result': {\n        'response': 'Molecules are made out of atoms', \n        'processing_time': 0.30640586500000033, \n        'keystring-scores': [\n            ('molecule', 0.990715997949492), \n            ('proton', 0.9186190596675989) # Searched for with exact match, therefore not a match.\n        ]\n    }, \n    'feedback': \"Cannot determine if the answer is correct. Please provide more information about 'proton'\"}\n</code></pre></p>"},{"location":"user_eval_function_docs/shortTextAnswer/","title":"ShortTextAnswer","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> </ul> <p>This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#inputs","title":"Inputs","text":"<p><code>keystrings</code> - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#keystring-object","title":"<code>keystring</code> object","text":"<p>The <code>keystring</code> object contains several fields which affect how it will be interpreted:</p> <ul> <li><code>string</code> - Required. The actual keystring being searched for.</li> <li><code>exact_match</code> - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to <code>false</code></li> <li><code>should_contain</code> - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to <code>true</code>. Setting this flag to false indicates that a correct response will not contain the specified keystring.</li> <li><code>custom_feedback</code> - Optional. A feedback string to be returned if the <code>string</code> was not found (or if it was, in case <code>should_contain</code> was set to <code>false</code>). Defaults to <code>None</code>, in which case a generic response will be generated containing the string searched for.</li> </ul>"},{"location":"user_eval_function_docs/shortTextAnswer/#outputs","title":"Outputs","text":"<p>The function will return an object with 3 fields of interest. the <code>is_correct</code> and <code>feedback</code> fields are required by LambdaFeedback to present feedback to the user. The <code>result</code> field is only used for development. <pre><code>{\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"result\": {\n        \"response\": \"&lt;string&gt;\",\n        \"processing_time\": \"&lt;double&gt;\",\n    },\n    \"feedback\": \"string\"\n}\n</code></pre></p> <ul> <li><code>response</code> - The student answer. USed for debugging purposes.</li> <li><code>processing_time</code> - The time it took for the function to evaluate</li> </ul> <p>If the function identified a problematic keystring, the result object will have an additional field: * <code>keystring-scores</code> - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer.</p> <p>Otherwise, it will have the additional fields: * <code>method</code> - string. Either \"w2v\" or \"BOW vector similarity\". * <code>similarity_value</code> - double. The similarity value between the response and the answer.</p> <p>If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","title":"Example simple input, no keystring","text":"<p>Input <pre><code>{\n    \"response\": \"Density, velocity, viscosity, length\",\n    \"answer\": \"Density, speed, Viscosity, Length\",\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': True, \n    'result': {\n        'response': 'Density, speed, Viscosity, Length',\n        'processing_time': 0.022912219000000178, \n        'method': 'w2v', \n        'similarity_value': 0.9326027035713196}, \n    'feedback': 'Confidence: 0.933%'\n}\n</code></pre></p>"},{"location":"user_eval_function_docs/shortTextAnswer/#example-keystring-input","title":"Example keystring input","text":"<p>Input <pre><code>{\n    \"response\": \"Molecules are made out of atoms\",\n    \"answer\": \"Many atoms form a molecule\",\n    'keystrings': [\n        {'string': 'molecule'}, \n        {'string': 'proton', 'exact_match': True}\n    ]\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': False, \n    'result': {\n        'response': 'Molecules are made out of atoms', \n        'processing_time': 0.30640586500000033, \n        'keystring-scores': [\n            ('molecule', 0.990715997949492), \n            ('proton', 0.9186190596675989) # Searched for with exact match, therefore not a match.\n        ]\n    }, \n    'feedback': \"Cannot determine if the answer is correct. Please provide more information about 'proton'\"}\n</code></pre></p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/","title":"ArraySymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>This evaluation function can take any level of nesting for \"response\" and \"answer\" fields, as comparison is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymbolicEqual function, called using the experimental EvaluationFunctionClient from the evaluation-function-utils library.</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#inputs","title":"Inputs","text":"<p>This compares cells using the <code>symbolicEqual</code> function. Please consult that function's documentation for details on it's allowable parameters, as the ones provided to this function are fed through as they are.</p> <pre><code>{\n  \"response\": \"&lt;array (of arrays) of strings&gt;\",\n  \"answer\": \"&lt;array (of arrays) of strings&gt;\",\n  \"params\": {\n            \"Any params accepted by symbolicEqual\"\n    }\n}\n</code></pre> <p>Note: <code>response</code> and <code>answer</code> arrays should ultimately have string elements, even though they can have any level of nesting.</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>grade</code> command look like the following:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"detailed_feedback\": [\n      {\n        \"is_correct\": \"&lt;bool&gt;\",\n        \"level\": \"&lt;sympy correctness level&gt;\"\n      },\n      {\n        \"...\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Note: The <code>detailed_feedback</code> result field is of the same shape as the answer, giving specific information for the correctness of each cell in the evaluated array</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#examples","title":"Examples","text":""},{"location":"dev_eval_function_docs/arraySymbolicEqual/#simple-arrays","title":"Simple Arrays","text":"<p>Correct behaviour Input <pre><code>{\n  \"response\": [\"a\", \"b + c\"],\n  \"answer\": [\"a\", \"c + b\"]\n}\n</code></pre></p> <p>Output <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": true,\n        \"detailed_feedback\": [\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            },\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            }\n        ]\n    }\n}\n</code></pre></p> <p>Incorrect behaviour Input <pre><code>{\n  \"response\": [\"a\", \"b + 2*c\"],\n  \"answer\": [\"a\", \"c + b\"]\n}\n</code></pre></p> <p>Output <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": false,\n        \"detailed_feedback\": [\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            },\n            {\n                \"is_correct\": false\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"user_eval_function_docs/arraySymbolicEqual/","title":"ArraySymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>MATRIX</code></li> <li><code>NUMBER</code></li> <li><code>TABLE</code></li> </ul> <p>This function compares two symbolic expression arrays, with any level of nesting (2D, 3D, irregular shape, ...). Each cell is compared using the <code>SymbolicEqual</code> evaluation function.</p>"},{"location":"user_eval_function_docs/arraySymbolicEqual/#inputs","title":"Inputs","text":"<p>This function support all input parameters that <code>SymbolicEqual</code> uses.</p>"},{"location":"dev_eval_function_docs/arrayEqual/","title":"ArrayEqual","text":"<p>Edit on GitHub  View Code </p> <p>Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.</p>"},{"location":"dev_eval_function_docs/arrayEqual/#inputs","title":"Inputs","text":"<p>Valid params include <code>atol</code> and <code>rtol</code>, which can be used in combination, or alone. (just like the <code>IsSimilar</code> grading function)</p> <pre><code>{\n  \"response\": \"&lt;array&gt;\",\n  \"answer\": \"&lt;array&gt;\",\n  \"params\": {\n    \"atol\": \"&lt;number&gt;\",\n    \"rtol\": \"&lt;number&gt;\"\n  }\n}\n</code></pre> <p>Note: <code>response</code> and <code>answer</code> arrays are parsed using <code>np.array(dtype=np.float32)</code>, any errors this causes are returned and the comparison fails.</p>"},{"location":"dev_eval_function_docs/arrayEqual/#atol","title":"<code>atol</code>","text":"<p>Absolute tolerance parameter</p>"},{"location":"dev_eval_function_docs/arrayEqual/#rtol","title":"<code>rtol</code>","text":"<p>Relative tolerance parameter</p>"},{"location":"dev_eval_function_docs/arrayEqual/#outputs","title":"Outputs","text":""},{"location":"dev_eval_function_docs/arrayEqual/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/arrayEqual/","title":"ArrayEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>MATRIX</code></li> <li><code>TABLE</code></li> <li><code>MULTIPLE_CHOICE</code></li> </ul> <p>This function is used to compare two number arrays/vectors/matrices, provided absolute and/or relative tolerance parameters <code>rtol</code> and <code>atol</code>. This is carried out using the numpy.allclose function.</p> <p>If the answer is not an array of numbers an exception is raised. If the response is not an array of numbers, a feedback message that informs the user that only numbers are accepte will be generated.</p>"},{"location":"user_eval_function_docs/arrayEqual/#optional-parameters","title":"Optional parameters","text":"<p>There is one optional parameter: <code>feedback_for_incorrect_response</code>.</p>"},{"location":"user_eval_function_docs/arrayEqual/#feedback_for_incorrect_response","title":"<code>feedback_for_incorrect_response</code>","text":"<p>All feedback for all incorrect responses will be replaced with the string that this parameter is set to.</p>"},{"location":"dev_eval_function_docs/isExactEqual/","title":"IsExactEqual","text":"<p>Edit on GitHub  View Code </p> <p>Could be qualified as the simplest form of evaluation function, testing exact equality. This function will use the default python <code>==</code> test to compare answer and responses. It doesn't infer any types - meaning it requires a <code>params.type</code> to be supplied.</p>"},{"location":"dev_eval_function_docs/isExactEqual/#inputs","title":"Inputs","text":"<p>This function requires a parameter to function properly: <pre><code>{ \n  \"params\": {\n    \"type\": \"&lt;string&gt;\" (any of [\"int\", \"float\", \"str\", \"dict\"])\n  }\n  \"response\": &lt;&gt;,\n  \"answer\": &lt;&gt;\n}\n</code></pre></p> <p>When a student submits a response to a response area the number of previously submitted responses submitted to the same response area byt the same student will be sent to the evaluation function. The following format is used: <pre><code>{\n    \"submission_context\": {\n        \"submissions_per_student_per_response_area\": # non-negative integer that represent the number of previously processed responses\n    }\n}\n</code></pre></p> <p>The total number of submitted responses (i.e. the number of processed response + 1) can be displayed in the feedback by setting adding a field named <code>display_submission_count</code> to <code>params</code> and set its value to true.</p>"},{"location":"dev_eval_function_docs/isExactEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>grade</code> command will feature:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isExactEqual/#examples","title":"Examples","text":""},{"location":"dev_eval_function_docs/isExactEqual/#simple-string-comparison","title":"Simple String Comparison","text":"<pre><code>{\n  \"answer\": \"hydrophobic\",\n  \"response\": \"hydrophobic\",\n  \"params\": {\n    \"type\": \"str\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isExactEqual/#displaying-number-of-submitted-responses-in-the-feedback","title":"Displaying number of submitted responses in the feedback","text":"<pre><code>{\n  \"answer\": 1,\n  \"response\": 1,\n  \"params\": {\n    \"type\": \"int\",\n    \"display_submission_count\": true\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/isExactEqual/","title":"IsExactEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>NUMBER</code></li> <li><code>BOOLEAN</code></li> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>MATH_SINGLE_LINE</code></li> </ul> <p>Use this function to check the exact equality between the student response and answer. This function requires a 'type' parameter which specifies how each of the two inputs should be cast before direct comparison in python.</p>"},{"location":"dev_eval_function_docs/isSimilar/","title":"IsSimilar","text":"<p>Edit on GitHub  View Code </p> <p>This simple evaluation function checks if the supplied response is within a tolerance range defined in <code>params</code>. Works exactly like the numpy.isclose function.</p> <p>Valid params include <code>atol</code> and <code>rtol</code>, which can be used in combination, or alone. As the comparison made is the following:</p> <pre><code>is_correct = abs(res - ans) &lt;= (atol + rtol*abs(ans))\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#inputs","title":"Inputs","text":"<pre><code>{\n  \"response\": \"&lt;number&gt;\",\n  \"answer\": \"&lt;number&gt;\",\n  \"params\": {\n    \"atol\": \"&lt;number&gt;\",\n    \"rtol\": \"&lt;number&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#atol","title":"<code>atol</code>","text":"<p>Absolute tolerance parameter</p>"},{"location":"dev_eval_function_docs/isSimilar/#rtol","title":"<code>rtol</code>","text":"<p>Relative tolerance parameter</p>"},{"location":"dev_eval_function_docs/isSimilar/#outputs","title":"Outputs","text":"<pre><code>{\n  \"is_correct\": \"&lt;bool&gt;\",\n  \"real_diff\": \"&lt;number&gt;\",\n  \"allowed_diff\": \"&lt;number&gt;\",\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#real_diff","title":"<code>real_diff</code>","text":"<p>Real difference between the given answer and response</p>"},{"location":"dev_eval_function_docs/isSimilar/#allowed_diff","title":"<code>allowed_diff</code>","text":"<p>Allowed difference between answer and response, calculated using the supplied <code>atol</code> and <code>rtol</code> parameters</p>"},{"location":"dev_eval_function_docs/isSimilar/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/isSimilar/","title":"IsSimilar","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>NUMBER</code></li> <li><code>TEXT</code></li> </ul> <p>Use this evaluation function to check if the student's reponse is within a tolerance range defined in <code>params</code>. Works exactly like the numpy.isclose function. Valid params include <code>atol</code> and <code>rtol</code> (absolute and relative tolerances) which can be used in combination, or alone.</p> <p>Note: If the answer is not a number, all responses will generate an error.</p> <p>Note: If the response is not a number, a feedback message asking the user to submit a number will be returned.</p>"}]}