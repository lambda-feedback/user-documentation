{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lambda Feedback!","text":"Students: Teachers: <ul><li>Accessible content - in the browser and on PDF. </li><li>Feedback - express mathematical ideas naturally and get instant feedback. </li><li>Analytics - track your progress, manage your time</li><li>Social - connect with someone doing the same thing as you, now!</li></ul> <ul><li>Curate content - edit and publish your content in one place.</li><li>Share - customise content from others; share your own content and get kudos.</li><li>Analytics - curate automated feedback to meet student needs; arrange in-person lessons according to student needs.</li></ul> <p>This project began at Imperial College in 2022. The project itself, and the documentation, is currently under construction.</p> <p>Lambda Feedback is a homework platform providing automated formative feedback. We are initially targeting STEM subjects in Higher Education, with a particular focus on mathematical subjects (such as mathematical methods or mechanics). The value proposition is given briefly here.</p> <p>In these docs you can find:</p> <p><li> Student guide </li><li> Teacher guide </li><li> Advanced teachers. <p>Check out Lambda Feedback by clicking on the button below!</p> <p>Visit Lambda Feedback</p> <p>Articles relating to Lambda Feedback:</p> <ul> <li>Value proposition: Computers make us human</li> <li>Student experience of self-study: Getting stuck</li> <li>The role of challenge in self-study: Friction in the ideal learning process</li> <li>Configuring online learning: Worked solutions: when and how to use them?</li> <li>Articles on evaluation function algorithms, and on data analytics, are in preparation.</li> </ul>"},{"location":"opportunities/","title":"Opportunities","text":"<p>2023/06/05: there are no vacancees at the moment. The positions below are now filled.</p>"},{"location":"opportunities/#principal-full-stack-developer","title":"Principal Full Stack Developer","text":"<p>We are recruiting a Principal Full Stack Developer to work on the Lambda Feedback project.</p> <p>Work is by contract for up to 40 days per year. The contractor will implement new features, review work by software engineers on the project, and advise on architecture decisions.</p> <p>The developer should be senior with significant experience taking responsibility for a full stack application, and ready to 'hit the ground running'. The tech stack is AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js). The project is all built in Typescript. Infrastructure, testing, and deployment is all automated. We use test driven development, and CI/CD using CircleCI.</p> <p></p> <p>We are a diverse team of educators, students, and software professionals developing an application in-situ with our users. This is a motivating application in education and uses exciting new technologies. We are a flexible team that values creativity and redefining problems before solving them.</p> <p></p> <p>We have work that needs to be completed by July, and will have potential follow on work and commercialisation opportunities if the initial work is successful.</p> <p>Work is by contract (outside IR35) and we pay for deliverables. Prices for deliverables are agreed based on a day rate of \u00a3500-\u00a3800 depending on competence. We prefer to write and agree detailed completion criteria before agreeing to work. All work is quality assured, including testing with users, before approval.</p> <p>The Lambda Feedback project is owned and operated by Imperial College London. We work on campus in South Kensington, London. The position of contractor can optionally be fulfilled remotely if the holder prefers but we do expect occasional visits in-person. We are very flexible with timings and work regimes - our priority is to get the deliverables; we know that people work differently, and that people work best when they work in their own way.</p> <p>If you are interested, please enquire by email with peter [dot] johnson [at] ic.ac.uk.</p>"},{"location":"opportunities/#summer-placements","title":"Summer placements","text":"<p>Bursary: 8 positions, 8 weeks each (flexible), full-time. Bursary: \u00a3365 per week.</p> <p>Who should apply: Students from Imperial College London from any cohort who study in the following subjects: Aeronautics, Chemical Engineering, Computing, Design Engineering, Materials Science, Mathematics, Mechanical Engineering, Physics.</p> <p>Campus/Location: South Kensington, with opportunities to be remote later in the project.</p> <p>How to apply: apply here. Applications should be 300-500 words.</p> <p>Deadline: 31st March</p> <p>Contact details: Peter [dot] Johnson [at] ic.ac.uk</p> <p>More details on the project:</p>"},{"location":"opportunities/#lambda-feedback-software-a-place-to-do-homework","title":"Lambda Feedback software, a place to do homework","text":"<p>Lambda Feedback is a web platform that hosts homework, with a focus on mathematical subjects. The platform hosts\u202fquestion content both in the browser and in traditional PDF format. Online step-by-step solutions are also provided and are particularly popular with students.</p> <p>In addition to content delivery, the platform provides automated feedback on student responses. The long-term vision is rich, timely, personalised, feedback to students at the time of doing their homework.</p> <p>The software is being developed within Imperial. This year is our first academic year in \u2018alpha\u2019 version which hosted 9 modules across 8 departments and 2 faculties, with over 1,000 student users. We are now working to widen the availability of the software and to improve the functionality.</p> <p>More information about the software can be found here:\u202f Article: https://teachingengineers.wordpress.com/2022/07/18/computers-make-us-human/\u202f Presentation: click here</p> <p>We have 8 StudentShapers positions in summer 2023 each with the following purpose:\u202f \u2022 In partnership with an academic staff member, curate their content on Lambda Feedback. Key aspects include content transfer and editing; setting up automated feedback; improving the content.\u202f \u2022 As part of the wider team of summer students, develop the software more broadly. Key aspects include documenting good practice, testing new features, designing new features, and designing a broader vision for the future software \u2013 for example curating positive learning communities on the platform, identifying key analytics to serve students and teachers, or developing study aids.</p> <p>Essential skills and experience that we are looking for:\u202f \u2022 A passion for and knowledge of your own subject\u202f \u2022 A deep appreciation for the student experience in your subject, and the key needs of students\u202f \u2022 A keen interest in content management, including typesetting  (markdown, LaTeX, images; learn as you go!)\u202f \u2022 A vision for digital education where software serves the needs of today\u2019s students</p> <p>Additional ways you can add value to the project if you have the skills:\u202f \u2022 Mathematical computing skills, e.g. in Python, to help develop our evaluation functions (more info here: https://lambda-feedback.github.io/user-documentation/advanced/)\u202f \u2022 Data science skills to analyse our growing data set, for example skills in SQL queries, data analysis e.g. in Pandas, data visualisation, machine learning.\u202f \u2022 Graphic design, UI/UX design, vision development for online products\u202f \u2022 Web development skills in any part of the following stack: AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js) / Typescript / CircleCI</p>"},{"location":"terminology/","title":"Terminology","text":""},{"location":"terminology/#general-information","title":"General Information","text":"<p>LambdaFeedback is a place to study online. Teachers curate content that students can access. Content is available in the browser and in PDF. Automated feedback on final answers and detailed worked solutions are provided.</p>"},{"location":"terminology/#terminology-used-and-definitions","title":"Terminology Used and Definitions","text":"<p>Here, the fundamental structure and terminology will be laid out.</p>"},{"location":"terminology/#evaluation-functions","title":"Evaluation Functions","text":"<p>An evaluation function is an algorithm that is applied to a response area, whereby the student's response is evaluated and checked. The types of evaluation functions correspond to the types of response areas.</p>"},{"location":"terminology/#final-answer","title":"Final Answer","text":"<p>This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. It serves as a simple container for the final answer, so that the student may compare results.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#modules","title":"Modules","text":"<p>A module is a set programme of taught material. In a university setting, this would correspond to a course module.</p>"},{"location":"terminology/#questions","title":"Questions","text":"<p>A question is a problem within a set, and it may contain any number of parts.</p>"},{"location":"terminology/#response-areas","title":"Response Areas","text":"<p>A response area is an interactive element. Student enters a response and receives feedback. There are different types of response area (text, numerical, array etc.), and these correspond to the different types of information the student is required to input.</p>"},{"location":"terminology/#sets","title":"Sets","text":"<p>We use the word 'Set' to refer to a group of questionscontent. In a university setting, a Set typically corresponds to an individual homework/tutorial sheet.</p>"},{"location":"terminology/#structured-tutorial","title":"Structured Tutorial","text":"<p>Content providing a structure with which to approach a problem - but not to give the full details away. It is encouraged to be the first piece of guidance for the student before they look at the \"Worked Solution\", or \"Final Answer\".</p> <p>This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#student-learner","title":"Student (learner)","text":"<p>From the perspective of Lambda Feedback, a student is someone who accesses and responds to problem sets. A student only has permissions to view and respond to problem sets (not to edit them).</p>"},{"location":"terminology/#teacher","title":"Teacher","text":"<p>From the perspective of Lambda Feedback, a teacher is someone who creates and manages content. A teacher's account has permissions to create, edit, and delete content within a Module.</p>"},{"location":"terminology/#worked-solution","title":"Worked Solution","text":"<p>The stages of working that lead to a final answer. It may be split into multiple steps which the student can reveal sequentially. This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button.</p> <p>This is an optional section, and so does not have to be included in any question.</p>"},{"location":"terminology/#workspace","title":"Workspace","text":"<p>On the Question page, the students has access to their own workspace tab. Here they can find the \"Canvas\", for handwriting notes, and the \"Chat\", for conversing with an AI Chatbot on the question materials.</p>"},{"location":"advanced/","title":"Advanced users","text":"<p>Advanced users can develop their own evaluation functions.</p>"},{"location":"advanced/#evaluation-functions","title":"Evaluation functions","text":"<p>Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created.</p> <p>Evaluation functions - Quickstart Guide</p>"},{"location":"advanced/#response-areas","title":"Response areas","text":"<p>Response areas are components in the frontend where student users can enter a response. The response is sent to the evaluation function, which returns feedback to the response area. In the alpha version response areas are built into the software (rather than being modular) so are not straightforward to redevelop. This website catalogues the basic behaviour of response areas, to inform developers of evaluation functions.</p> <p>Response areas - overview</p>"},{"location":"advanced/#ai-chatbot-agents","title":"AI Chatbot Agents","text":"<p>Chatbot agents are AI Assistants that students can chat with to ask for help or further explanations regarding the Question that they are working on. Each Agent has its own personality and approach to assisting the students.</p> <p>Chatbot Agents - Quickstart guide</p>"},{"location":"advanced/#system-architecture","title":"System Architecture","text":"<ul> <li>Technologies</li> <li>Deployment pipelines</li> <li>Hierarchy</li> </ul>"},{"location":"advanced/#future-features","title":"Future Features","text":""},{"location":"advanced/placeholder/","title":"coming soon","text":""},{"location":"advanced/chatbot_agents/local/","title":"Running and Testing Agents Locally","text":"<p>You can run the Python function for your agent itself by writing a <code>main()</code> function, or you can call the <code>testbench_prompts.py</code> script that runs a similar pipeline to the <code>module.py</code>.</p> <pre><code>python src/agents/utils/testbench_prompts.py\n</code></pre> <p>You can also use the <code>test_prompts.py</code> script to test the agents with example inputs from Lambda Feedback questions and synthetic conversations. <pre><code>python src/agents/utils/test_prompts.py\n</code></pre></p>"},{"location":"advanced/chatbot_agents/local/#testing-using-the-docker-image","title":"Testing using the Docker Image","text":"<p>You can also build and run the docker pipeline for the agents. The chatbot agents are deployed onto a AWS Lambda serverless cloud function using the docker image. Hence, for final testing of your chatbots, we recommend completing those steps.</p>"},{"location":"advanced/chatbot_agents/local/#build-the-docker-image","title":"Build the Docker Image","text":"<p>To build the Docker image, run the following command in the root folder of the project (where the Dockerfile is located):</p> <pre><code>docker build -t llm_chat .\n</code></pre>"},{"location":"advanced/chatbot_agents/local/#running-the-docker-image","title":"Running the Docker Image","text":"<p>To run the Docker image, use the following command:</p>"},{"location":"advanced/chatbot_agents/local/#without-env-file","title":"Without .env file:","text":"<pre><code>docker run -e OPENAI_API_KEY={your key} -e OPENAI_MODEL={your LLM chosen model name} -p 8080:8080 llm_chat\n</code></pre>"},{"location":"advanced/chatbot_agents/local/#with-container-name-for-interaction-eg-copying-file-from-inside-the-docker-container","title":"With container name (for interaction, e.g. copying file from inside the docker container):","text":"<pre><code>docker run --env-file .env -it --name my-lambda-container -p 8080:8080 llm_chat\n</code></pre> <p>This will start the evaluation function and expose it on port <code>8080</code> and it will be open to be curl:</p> <pre><code>curl --location 'http://localhost:8080/2015-03-31/functions/function/invocations' --header 'Content-Type: application/json' --data '{\"message\":\"hi\",\"params\":{\"conversation_id\":\"12345Test\",\"conversation_history\": [{\"type\":\"user\",\"content\":\"hi\"}]}}'\n</code></pre>"},{"location":"advanced/chatbot_agents/local/#call-docker-container-from-postman","title":"Call Docker Container From Postman","text":"<p>POST URL:</p> <pre><code>http://localhost:8080/2015-03-31/functions/function/invocations\n</code></pre> <p>Body:</p> <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n        \"conversation_id\":\"12345Test\",\n        \"conversation_history\": [{\"type\":\"user\",\"content\":\"hi\"}]\n    }\n}\n</code></pre> <p>Body with optional Params: <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n        \"conversation_id\":\"12345Test\",\n        \"conversation_history\":[{\"type\":\"user\",\"content\":\"hi\"}],\n        \"summary\":\" \",\n        \"conversational_style\":\" \",\n        \"question_response_details\": \"\",\n        \"include_test_data\": true,\n        \"agent_type\": {agent_name}\n    }\n}\n</code></pre></p>"},{"location":"advanced/chatbot_agents/quickstart/","title":"Developing Chat Agents: Getting Started","text":""},{"location":"advanced/chatbot_agents/quickstart/#what-is-a-chat-agent","title":"What is a Chat Agent?","text":"<p>It's a function which calls Large Language Models (LLMs) to respond to the student's messages given contxtual data:</p> <ul> <li>question data</li> <li>user data such as past responses to the problem   Chatbot Agents capture and automate the process of assisting students during their learning process when outside of classroom.</li> </ul>"},{"location":"advanced/chatbot_agents/quickstart/#getting-setup-for-development","title":"Getting Setup for Development","text":"<ol> <li> <p>Get the code on your local machine (Using github desktop or the <code>git</code> cli)</p> <ul> <li> <p>For new functions: clone the main repo for lambda-chat and create a new branch. Then go under <code>scr/agents</code> and copy the <code>base_agent</code> folder.</p> </li> <li> <p>For existing functions: please make your changes on a new separate branch</p> </li> </ul> </li> <li> <p>If you are creating a new chatbot agent, you'll need to set it's name as the folder name in <code>scr/agents</code> and its corresponding files.</p> </li> <li> <p>You are now ready to start making changes and implementing features by editing each of the three main function-logic files:</p> <ol> <li> <p><code>scr/agents/{base_agent}/{base}_agent.py</code>: This file contains the main LLM pipeline using LangGraph and LangChain.</p> </li> <li> <p>the agent expects the following inputs when it being called:</p> </li> </ol> <p>Body with necessary Params:</p> <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n            \"conversation_id\":\"12345Test\",\n            \"conversation_history\": [{\"type\":\"user\",\"content\":\"hi\"}]\n    }\n}\n</code></pre> <p>Body with optional Params:</p> <pre><code>{\n    \"message\":\"hi\",\n    \"params\":{\n            \"conversation_id\":\"12345Test\",\n            \"conversation_history\":[{\"type\":\"user\",\"content\":\"hi\"}],\n            \"summary\":\" \",\n            \"conversational_style\":\" \",\n            \"question_response_details\": \"\",\n            \"include_test_data\": true,\n            \"agent_type\": {agent_name}\n    }\n}\n</code></pre> </li> <li> <p><code>scr/agents/{base_agent}/{base}_prompts.py</code>: This is where you can write the system prompts that describe how your AI Assistant should behave and respond to the user.</p> </li> <li> <p>Make sure to add your agent <code>invoke()</code> function to the <code>module.py</code> file.</p> </li> <li> <p>Please add a <code>README.md</code> file to describe the use and behaviour of your agent.</p> </li> <li> <p>Changes can be tested locally by running the pipeline tests using:     <pre><code>pytest src/module_test.py\n</code></pre> Running and Testing Agents Locally</p> </li> <li> <p>Merge commits into any branch (except main) will trigger the <code>dev.yml</code> workflow, which will build the docker image, push it to a shared <code>dev</code> ECR repository to make the function available from the <code>dev</code> and <code>localhost</code> client app.</p> </li> <li> <p>In order to make your new chatbot available on the LambdaFeedback platform, you will have to get in contact with the ADMINS on the platform.</p> </li> </ol>"},{"location":"advanced/evaluation_functions/","title":"Deployed Evaluation Functions","text":"<p>Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader.</p> <p>If you can't see any documentation files under this section, please contact an admin.</p>"},{"location":"advanced/evaluation_functions/alternate_languages/","title":"Alternate Evaluation Function Languages","text":""},{"location":"advanced/evaluation_functions/alternate_languages/#lambda-compatible-images","title":"Lambda-Compatible Images","text":""},{"location":"advanced/evaluation_functions/alternate_languages/#extending-a-pre-built-lambda-image","title":"Extending a pre-built Lambda image","text":"<ul> <li>Available for: Node.js, Python, Java, .NET, Go, Ruby</li> <li>Docs</li> <li>Repo</li> <li>These base images are regularly updated, and the most widely used (more docs)</li> <li>They also come with pre-packaged runtime interface clients - a HTTP interface for runtimes to receive invocation events and respond<ul> <li>Good for local development</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#creating-custom-base-images","title":"Creating custom base images","text":"<ul> <li>Using the lambda/provided image<ul> <li>This \"contains all the required components to run functions packaged as container images on Lambda\"</li> </ul> </li> <li>Building a custom runtime from scratch <ul> <li>Custom AWS Lambda runtimes</li> <li>Runtimes walkthrough tutorial</li> </ul> </li> <li>Emulate execution locally? <p>Lambda provides a runtime interface emulator (RIE) for you to test your function locally. The AWS base images for Lambda and base images for custom runtimes include the RIE. For other base images, you can download the\u00a0Runtime interface emulator\u00a0from the AWS GitHub repository.</p> </li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#misc-notessources","title":"Misc Notes/Sources","text":"<ul> <li>The Lambda Execution Environment</li> <li>Create Images from Alternative base images</li> </ul>"},{"location":"advanced/evaluation_functions/alternate_languages/#development-philosophy","title":"Development Philosophy","text":"<p>Ultimately we want to call a function made by a user in any language. Two ways to do this:</p> <ul> <li> <p>We write and provide runtime in all the different languages. This means that all the logic happens in that language. We write the code that actually receives the requests from lambda function events. In this case, the user function can be imported from those handlers.</p> <ul> <li>Writing handlers in each of those languages requires time and extensive knowledge (in order to write robust code)</li> <li>Handler code needs to:<ul> <li>Have clean and reliable error catching</li> </ul> </li> </ul> </li> <li> <p>We write a global runtime, which makes a call to their function via a sub-process. We call their script, which must recieve the payload as a commandline argument.</p> <ul> <li>User has to write more code <ul> <li>For allowing cmdline arguments, and parsing of inputs</li> </ul> </li> <li>Might be slower than in other languages. Since another script has to be executed.</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/feedback/","title":"Base Layer Feedback Implementation","text":"<p>Input structure:</p> <pre><code>{\n    \"response\": \"user input\",\n    \"answer\": \"original answer\",\n    \"params\": {\n        \"cases\": [\n            {\n                \"answer\": \"same shape as original answer\",\n                \"feedback\": \"feedback string\",\n                \"params\": {...} # Any parameters to set or override\n            },\n            ...\n        ]\n    }\n}\n</code></pre>"},{"location":"advanced/evaluation_functions/feedback/#execution-logic-for-the-eval-command","title":"Execution Logic for the <code>eval</code> command","text":"<ol> <li>First <code>evaluation_function</code> is called using the response, answer and params</li> <li>If evaluation threw an error, then return the error message</li> <li>If evaluation was successful, check for matching cases<ol> <li>If \"params\" contains a non-empty list of \"cases\", determine the correct feedback, add it to the result and return the block (Logic for this is described in the next section) </li> <li>If \"params\" doesn't contain a list of cases, simply return the result</li> </ol> </li> </ol>"},{"location":"advanced/evaluation_functions/feedback/#determining-the-correct-feedback-case","title":"Determining the correct feedback case","text":"<ol> <li>Iterate through each case in the list of <code>cases</code>:<ol> <li>Validate the case has an 'answer' and 'feedback'</li> <li>If the case contains 'params', then merge them with the original 'params', overwriting values if they already exist</li> <li>Call <code>evaluation_function</code> with the student \"response\", case \"answer\" and merged \"params\"<ol> <li>If the function returns \"is_correct: true\", we have a match, store case and feedback returned from the evaluation function</li> <li>If the function returns an error, catch it and add it to a list of warnings</li> </ol> </li> </ol> </li> <li>If no matches were found, don't return any feedback </li> <li>If exactly one match was found, check if <code>override_eval_feedback</code> is in parameters<ol> <li>If <code>override_eval_feedback</code> is set to true, return the case feedback</li> <li>If <code>override_eval_feedback</code> is not set or set to false, append the evaluation function feedback to the case feedback, separated by a linebreak and the return the result</li> </ol> </li> <li>If more than one matches were found, return the first one (using the same procedure as if only one match was found) and add a warning explaining which cases matched, and why only the first was selected.</li> </ol>"},{"location":"advanced/evaluation_functions/local/","title":"Running and Testing Functions Locally","text":""},{"location":"advanced/evaluation_functions/local/#simple","title":"Simple","text":""},{"location":"advanced/evaluation_functions/local/#using-docker","title":"Using Docker","text":"<p>This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image, the container contains a HTTP client which can be used to locally simulate Lambda execution events. </p> <p>Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as <code>docs</code> and <code>healthcheck</code> can be tested.</p> <ol> <li> <p>Install Docker on your machine</p> </li> <li> <p>Navigate to the root directory of your function</p> </li> <li> <p>Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it <code>eval-tmp</code>.     <pre><code>docker image build -t eval-tmp app\n</code></pre></p> </li> <li> <p>Spin up a container using the image built in the previous step.     <pre><code>docker run --rm -d --name eval-function -p 9000:8080 eval-tmp \n</code></pre></p> </li> <li> <p>You can now simulate requests to the function using any request client (like Insomnia or Postman). By default, the url you can hit is:     <pre><code>http://localhost:9000/2015-03-31/functions/function/invocations\n</code></pre></p> Warning <p>When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway. This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function.</p> <p>Specifically, this means structuring requests in the following way: <pre><code>{\n  \"headers\": {\n    \"command\": \"eval\"\n  },\n  \"body\": {\n    \"response\": \"a\",\n    \"answer\": \"a\",\n    \"params\": {\n      \"garlic\": \"moreish\"\n    }\n  }\n}\n</code></pre></p> <p>The main difference is that <code>headers</code> and <code>body</code> are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the <code>command</code> field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the <code>response</code>, <code>answer</code> and <code>params</code> fields.</p> </li> <li> <p>(Optional) The <code>run</code> command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run:     <pre><code>docker container logs -f eval-function \n</code></pre></p> </li> </ol> Tip <p>You will very rarely need this, but you can peek into the running container by opening a shell within it using:</p> <pre><code>docker exec -it eval-function bash\n</code></pre>"},{"location":"advanced/evaluation_functions/local/#useful-links","title":"Useful Links","text":""},{"location":"advanced/evaluation_functions/module/","title":"evaluation-function-utils Package","text":"<ul> <li>Error Reporting </li> <li>Schema validation</li> <li>Local testing</li> </ul>"},{"location":"advanced/evaluation_functions/module/#errors","title":"Errors","text":"<p>Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors.</p>"},{"location":"advanced/evaluation_functions/module/#class-evaluationexception","title":"class <code>EvaluationException</code>","text":"<p>This class extends the usual python <code>Exception</code>, with additional functionality. It can be used to package additional fields and values to errors thrown and returned by evaluation functions.</p> <p>Example</p> <p>If at some point in the execution of the <code>evaluation_function</code>, an error is thrown:</p> <pre><code>from evaluation_function_utils.errors import EvaluationException\n\nif isinstance(input, str):\n    raise EvaluationException(\n        \"The input must not be a string\", \n        valid_types=[\"int\", \"float\", \"array\"],\n    )\n</code></pre> <p>Then the output generated by the lambda function will look like:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"error\": {\n    \"message\": \"The input must not be a string\",\n    \"valid_types\": [\n      \"int\", \"float\", \"array\"\n    ]\n  }\n}\n</code></pre> <p>This class contains an error_dict property, which packages the additional arguments given to the Exception instance into a JSON-serializable object. It does so in an error-safe way, also reporting serialization errors if they occur.</p>"},{"location":"advanced/evaluation_functions/module/#client","title":"Client","text":"<p>This submodule contains a custom <code>EvaluationFunctionClient</code>, which can be used to call other deployed evaluation functions.</p>"},{"location":"advanced/evaluation_functions/module/#class-evaluationfunctionclient","title":"class <code>EvaluationFunctionClient</code>","text":"<p>Client wrapped around the botocore.client.Lambda, for invoking deployed evaluation functions. On initialisation, it fetches credentials from environment variables \"INVOKER_KEY\", \"INVOKER_ID\" and \"INVOKER_REGION\", or from an optional environment file prescrived by <code>env_path</code>. </p> <p>Example</p> <pre><code>from evaluation_function_utils.client import EvaluationFunctionClient\nclient = EvaluationFunctionClient()\n\ndef evaluation_function(response, answer, params): \n    return client.invoke('isExactEqual', response, answer, params)\n</code></pre> <p>In this example, the evaluation_function completely offloads grading to the deployed 'isExactEqual' function. </p> <p>Note: The <code>EvaluationFunctionClient.invoke</code> method was designed to behave exactly as if the <code>evaluation_function</code> function defined in the targeted deployed function was called directly. This means that if errors are encountered an <code>EvaluationException</code> is raised.</p>"},{"location":"advanced/evaluation_functions/quickstart/","title":"Developing Evaluation Functions: Getting Started","text":""},{"location":"advanced/evaluation_functions/quickstart/#what-is-an-evaluation-function","title":"What is an Evaluation Function?","text":"<p>It's a cloud function which performs some computation given some user input (the response), a problem-specific source of truth (the answer), and some optional parameters (params). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer. However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined. </p>"},{"location":"advanced/evaluation_functions/quickstart/#getting-setup-for-development","title":"Getting Setup for Development","text":"<ol> <li>Get the code on your local machine (Using github desktop or the <code>git</code> cli)<ul> <li>For new functions: create and clone a new repository using the boilerplate template. Make sure the new repository is set to public (it needs access to organisation secrets).</li> <li>For existing functions: please make your changes on a new separate branch </li> </ul> </li> <li>If you are creating a new function, you'll need to set it's name (as it will be deployed) in the <code>config.json</code> file, available in the root directory.<ul> <li>The name must be unique. To view existing grading functions, go to:<ul> <li>Staging API Gateway Integrations</li> <li>Production API Gateway Integrations</li> </ul> </li> </ul> </li> <li> <p>You are now ready to start making changes and implementing features by editing each of the three main function-logic files:</p> <ol> <li> <p><code>app/evaluation.py</code>: This file contains the main <code>evaluation_function</code> function, which ultimately gets called to compare a response to an answer. </p> <p><code>evaluation.py</code> Specification</p> </li> <li> <p><code>app/evaluation_tests.py</code>: This is where you can test the logic in <code>evaluation.py</code>, following the standard <code>unittest</code> format. </p> <p><code>evaluation_tests.py</code> Specification</p> </li> <li> <p>Documentation files:</p> <ul> <li> <p><code>app/docs/dev.md</code>: This file should be edited to reflect any changes/features implemented, following a developer perspective. It is baked into the function's image to be pulled by this documentation website under the deployed functions section.</p> </li> <li> <p><code>app/docs/user.md</code>: This file documents how the function can be used by a teacher user, from the perspective of editing content on the LambdaFeedback platform. This time, files are collated and displayed in the Teacher section.</p> </li> </ul> </li> </ol> </li> <li> <p>Changes can be tested locally by running the tests you've written using: <pre><code>python -m unittest app/evaluation_tests.py\n</code></pre> Running and Testing Functions Locally</p> </li> <li> <p>Merge commits into the default branch will trigger the <code>test-and-deploy.yml</code> workflow, which will build the docker image, push it to a shared ECR repository, then call the backend <code>grading-function/ensure</code> route to build the necessary infrastructure to make the function available from the client app.</p> </li> <li> <p>You can now test the deployed evaluation function using your prefered request client (such as Insomnia or Postman or simply <code>curl</code> from a terminal). Functions are made available at:     <pre><code>https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/&lt;function name as defined in config.json&gt;\n</code></pre></p> <p>Example Request to SymbolicEqual</p> <pre><code>``` \ncurl --request GET \\\n    --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/symbolicEqual \\\n    --header 'Content-Type: application/json' \\\n    --header 'command: eval' \\\n    --data '{\"response\": \"x + x\", \"answer\": \"2*x\"}'\n```\n</code></pre> </li> <li> <p>In order to make your new function available on the LambdaFeedback platform, you have to register it via the Admin Panel. This is done by supplying its name, url (the same as the one above) and supported response types. </p> </li> </ol>"},{"location":"advanced/evaluation_functions/quickstart/#more-info","title":"More Info","text":"<ul> <li> <p>General Function Specification and Behaviour</p> <ul> <li>Function philosophy including deployment strategy</li> <li>Request/Response schemas and communication spec </li> <li>Base layer logic, properties and behaviour</li> </ul> </li> <li> <p>EvaluationFunctionUtils (python package)</p> <ul> <li>Error Reporting </li> <li>Schema validation</li> <li>Local testing</li> </ul> </li> </ul>"},{"location":"advanced/evaluation_functions/specification/","title":"Evaluation Function Specification","text":""},{"location":"advanced/evaluation_functions/specification/#introduction-and-philosophy","title":"Introduction and Philosophy","text":"<p>Functionality for each evaluation function is split up as follows:</p> <p>Universal function behaviour applicable to every function, such as the ability to run tests, return documentation and execute the evaluation is handled by the Base Layer. This is the docker image which is extended by every developed evaluation function.</p> <p>Functionality that may be required in more than one function (but not necessarily all), such as the ability to call already deployed functions and error reporting is handled by the evaluation_function_utils python package. This package comes pre-installed in the base layer, and can optionally be imported and called from the evaluation_function.</p> <p>Finally, specific comparison logic and handling of bespoke evaluation parameters is done in the custom evaluation_function, unique to each deployed instance. This is the logic that differenciates each function (comparing numbers, matrices, images, equations, graphs, text, tables, etc ...).</p>"},{"location":"advanced/evaluation_functions/specification/#commands","title":"Commands","text":"<p>Commands are handled by the base layer. They define a unified interface for interacting with all deployed evaluation functions on the web. Practically, these are specified in the \"command\" request header.</p> <p>Example</p> <p>To execute the <code>docs-user</code> command for a function, the following header would be specified alonside the http request made to the endpoint on which the function is made available:</p> <pre><code>```bash\ncurl --request GET \\\n--url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/isExactEqual \\\n--header 'command: docs-user'\n```\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#eval","title":"<code>eval</code>","text":"<p>This is the default command, used to compare a student's <code>response</code> and correct <code>answer</code>, given certain <code>params</code>. Outputs for this command depend on the success of the execution of the user-defined <code>evaluation_function</code>. If an error was thrown during execution, it is caught by the main handler and an error block is returned - otherwise, successful execution outputs are supplied under a <code>result</code> field.</p> <p>Output Structure: Successful evaluation</p> <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": \"&lt;bool&gt;\",\n\n        # Optional fields added by feedback generation (1)\n        \"feedback\": \"&lt;string&gt;\",\n        \"warnings\": \"&lt;array&gt;\"\n\n        # This output can also contain any number of fields given by `evaluation_function`\n    }\n}\n</code></pre> <ol> <li>See the Feedback Page for more information</li> </ol> <p>Output Structure: Error thrown during Execution</p> <pre><code>{\n    \"command\": \"eval\",\n    \"error\": {\n        \"message\": \"&lt;string&gt;\", # Always present\n\n        # This object can contain other number of additional fields\n        # passed through by the EvaluationException (1) for debugging e.g.:\n        \"serialization_errors\": [],\n        \"culprit\": \"user\",\n        \"detail\": \"...\"\n    }\n}\n</code></pre> <ol> <li>This is a custom error class from the evaluation-function-utils package, which developers are encouraged to use in order to output richer errors. See the Error handling section for more information.</li> </ol>"},{"location":"advanced/evaluation_functions/specification/#preview","title":"<code>preview</code>","text":"<p>This command is similar to <code>eval</code>, except it doesn't return whether an answer is correct or provide feedback. Instead, <code>preview</code> provides a way for students view their response after some pre-processing, e.g. as rendered LaTeX when using Sympy for symbolic algebra.</p> <p>This should be faster to compute than <code>eval</code>, allowing students to get live preview of their response.</p>"},{"location":"advanced/evaluation_functions/specification/#healthcheck","title":"<code>healthcheck</code>","text":"<p>This command runs and returns a summary three testing suites: requests, responses and evaluation. Request and response tests check that inputs and outputs to the function work correctly, and follow the correct syntax. Evaluation tests are unique to each evaluation function and test the actual comparison logic.</p>"},{"location":"advanced/evaluation_functions/specification/#docs-user","title":"<code>docs-user</code>","text":"<p>Command returns the <code>docs/user.md</code> file (base64 encoded)</p>"},{"location":"advanced/evaluation_functions/specification/#docs-dev","title":"<code>docs-dev</code>","text":"<p>Command returns the <code>docs/dev.md</code> file (base64 encoded)</p>"},{"location":"advanced/evaluation_functions/specification/#base-layer","title":"Base Layer","text":""},{"location":"advanced/evaluation_functions/specification/#file-structure","title":"File Structure","text":"<p>A standard evaluation function repository based on the provided boilerplate will have the following file structure:</p> <pre><code>app/\n    __init__.py\n    evaluation.py # Script containing the main evaluation_function\n    evaluation_tests.py # Unittests for the main evaluation_function\n    requirements.txt # list of packages needed for algorithm.py\n    Dockerfile # for building whole image to deploy to AWS\n\n    docs/ # Documentation pages for this function (required)\n        dev.md # Developer-oriented documentation\n        user.md # LambdaFeedback content author documentation\n\n.github/\n    workflows/\n        test-and-deploy.yml # Testing and deployment pipeline\n\nconfig.json # Specify the name of the evaluation function in this file\nREADME.md\n.gitignore\n</code></pre> <p>Warning</p> <p>If you want to split up function logic into different files, these must be added to the <code>Dockerfile</code>. This is so they are packaged with the built image when deployed. For example, if <code>evaluation.py</code> imports functionality from an <code>app/utils.py</code> file, then the following line must be added:</p> <pre><code>RUN pip3 install -r requirements.txt\n\n# Copy the evaluation and testing scripts\nCOPY evaluation.py ./app/\nCOPY evaluation_tests.py ./app/\n\n# Copy additional files\nCOPY utils.py ./app/\n\n# Copy Documentation\nCOPY docs/dev.md ./app/docs/dev.md\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#evaluationpy","title":"<code>evaluation.py</code>","text":"<p>The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the <code>evaluation_function</code> function within it, to be more precise.</p>"},{"location":"advanced/evaluation_functions/specification/#the-evaluation_function","title":"The <code>evaluation_function</code>","text":""},{"location":"advanced/evaluation_functions/specification/#inputs","title":"Inputs","text":"<p>All evaluation functions are passed three arguments:</p> <ul> <li><code>response</code>: Data input by the user</li> <li><code>answer</code>: Data to compare user input to (could be from a DB of answers, or pre-generated by other functions)</li> <li><code>params</code>: Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...)</li> </ul> <p>For evaluation functions that use Sympy or LaTeX for mathematical expressions, it's not always possible for a student to type the correct symbols. Instead we need to use simpler symbols. For example, \\(\\overline{U_{ij}}\\) cannot be written using standard sympy syntax, and therefore has to be substituted for something else, such as <code>\"u\"</code> or <code>\"U\"</code>.</p> <p>Therefore, evaluation functions using mathematical expressions should be able to handle multiple symbols to represent the same variable. To achieve this, every evaluation function is passed a <code>symbols</code> entry in <code>params</code>, to allow functions to convert a student's response:</p> <pre><code>{\n    \"response\": \"user input\",\n    \"answer\": \"model response to compare against\",\n    \"params\": {\n        \"symbols\": {...},\n        ... # params set by the teacher\n    }\n}\n</code></pre> <p><code>symbols</code> is a dictionary, where each key represents the main Sympy symbol (known as the <code>code</code>), and has two entries:</p> <ul> <li><code>latex</code>: the string used for rendering the symbol in LaTeX</li> <li><code>aliases</code>: a list of alternative Sympy symbols that can be used by the student to represent the <code>code</code>.</li> </ul> <p>For the example above with \\(\\overline{U_{ij}}\\), <code>symbols</code> would have the form:</p> <pre><code>{\n    ...\n    \"params\": {\n        \"symbols\": {\n            \"u\": {\n                \"latex\": \"\\\\overline{U_{ij}}\",\n                \"aliases\": [\"U\"]\n            }\n        }\n    }\n}\n</code></pre> <p>Note that in JSON, special characters need to be escaped, so the latex symbol above will have a double-backslash instead.</p> <p>Currently, the backend only supports one LaTeX symbol for multiple Sympy symbols. In future, this will be a many-to-many relationship.</p>"},{"location":"advanced/evaluation_functions/specification/#context","title":"Context","text":"<p>When a student submits a response to a response area the number of previously submitted responses submitted to the same response area byt the same student will be sent to the evaluation function. The following format is used: <pre><code>    {\n        \"submission_context\": {\n            \"submissions_per_student_per_response_area\": # non-negative integer that represent the nubmer of previously processed responses\n        }\n    }\n</code></pre></p>"},{"location":"advanced/evaluation_functions/specification/#outputs","title":"Outputs","text":"<p>The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed:</p> <p><code>is_correct: &lt;bool&gt;</code>: Boolean parameter indicate whether the comparison between <code>response</code> and <code>answer</code> was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red).</p> <p>Info</p> <p>More standardised function outputs that the frontend can consume are to come</p>"},{"location":"advanced/evaluation_functions/specification/#error-handling","title":"Error Handling","text":"<p>Error reporting should follow a specific approach for all evaluation functions. If the <code>evaluation_function</code> you've written doesn't throw any errors, then it's output is returned under the <code>result</code> field - and assumed to have worked properly. This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways:</p> <p>Letting <code>evaluation_function</code> fail: On the request handler in the Base Layer, the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the <code>error.detail</code> field.</p> <p>Custom errors: If you want to report more detailed errors from your function, use the <code>EvaluationException</code> class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end.</p> <p>Example</p> <p>It is discouraged to do the following in the evaluation code: <code>python     if something.bad.happened():         return {             \"error\": {                 \"message\": \"Some important message\",                 \"other\": \"details\",             }         }</code></p> <pre><code>As this causes the actual function output (by the AWS lambda function) to be:\n```json\n{\n    \"command\": \"eval\",\n    \"result\": {\n        \"error\": {\n            \"message\": \"Some important message\",\n            \"other\": \"details\"\n        }\n    }\n}\n```\n\nInstead, use custom exceptions from the [evaluation-function-utils](module.md#errors) package.\n```python\nif something.bad.happened():\n    raise EvaluationException(message=\"Some important message\", other='details')\n```\n\nAs the actual function output will look like:\n```json\n{\n    \"command\": \"eval\",\n    \"error\": {\n        \"message\": \"Some important message\",\n        \"other\": \"details\"\n    }\n}\n```\n\nThis immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed.\n</code></pre>"},{"location":"advanced/evaluation_functions/specification/#evaluation_testspy","title":"<code>evaluation_tests.py</code>","text":"<p>This file is intended to contain unit tests for the <code>evaluation_function</code>. Python's built-in <code>unittest</code> framework is used. These tests are run by Github Actions whenever changes are pushed to the main branch, and the evaluation function is not deployed unless all the tests pass.</p> <p>Example</p> <p>A minimal example of a test: <pre><code>import unittest\nfrom .evaluation import evaluation_function\n\n# Tests are functions beginning with \"test_\" in \n# a class that inherits from unittest.TestCase\nclass TestEvaluationFunction(unittest.TestCase):\n    def test_trivial(self):\n        result = evaluation_function(\"a + b\", \"a + b\", {})\n        self.assertTrue(result[\"is_correct\"])\n</code></pre> Tests can be run locally using  <pre><code>$ python -m unittest app.evaluation_tests\n</code></pre></p>"},{"location":"advanced/evaluation_functions/specification/#autotests","title":"Autotests","text":"<p>For writing simple tests, it may be easier to write the tests in a config file and have them run on the evaluation function automatically. This can be achieved using the autotests library, which can easily be integrated into an existing project by adding a decorator to the test class. See the autotests README for more information.</p> <p>Another benefit of this approach is that the tool that collects evaluation function documentation (EvalDocsLoader) can read this file and auto-generate examples of correct and incorrect responses. This can help new users understand the capabilities of your evaluation function.</p> <p>For an example of how this looks, see the user docs for compareBoolean.</p>"},{"location":"advanced/evaluation_functions/specification/#documentation","title":"Documentation","text":"<p>Evaluation function documentation is stored in two files, which contain documentation for developers and users respectively. These files are fetched by  EvalDocsLoader, which integrates them into this documentation site. </p> <p>In order for EvalDocsLoader to find your docs, your evaluation function must:</p> <ol> <li>be deployed to the production site;</li> <li>belong to the lambda-feedback organisation on Github;</li> <li>have a topic called <code>evaluation-function</code>.</li> </ol> <p>Once these requirements are met, the docs you write should appear on the documentation site.</p>"},{"location":"advanced/evaluation_functions/specification/#docsdevmd","title":"<code>docs/dev.md</code>","text":"<p>This should contain documentation that would be useful for new developers working on your function.</p>"},{"location":"advanced/evaluation_functions/specification/#docsusermd","title":"<code>docs/user.md</code>","text":"<p>This should contain information for non-technical users, such as an overview of capabilities, examples, and a description of parameters.</p>"},{"location":"advanced/response_areas/overview/","title":"Overview of response areas","text":""},{"location":"advanced/response_areas/overview/#list-of-response-areas","title":"List of response areas","text":"<p>The list of response areas is maintained in the Teacher section here. In the Developer area (here), the behaviour of the response areas is documented.</p>"},{"location":"advanced/response_areas/overview/#data-types-when-submitting-empty-responses-default-behaviour","title":"Data types when submitting empty responses (default behaviour)","text":"<p>If a user submits a response without inputing a value, the response areas convert the responses as follows before passing them to the evaluation functions:</p> Input Value Type Default Value Comment number undefined [Behaviour needs updating] string undefined [Behaviour needs updating] MATRIX <code>\" \"</code> empty string in all cells TABLE <code>\" \"</code> empty string in all cells MULTIPLE CHOICE <code>False</code> all choices set to false"},{"location":"releases/","title":"Index","text":""},{"location":"releases/#release-20250402","title":"Release 2025/04/02","text":"<ul> <li>b690-lexdown-legacy-content - updated Lexdown to ensure all legacy content is correctly displayed</li> <li>b676-user-permissions-tweaks-and-bugs - added access restrictions to the question Stats tab, specifically limiting access to Explore and Configure functionality</li> </ul>"},{"location":"releases/#release-20250327","title":"Release 2025/03/27","text":"<ul> <li>b647-fix-tab-numbers - corrected tab to display the correct number of activities</li> <li>b561-comments-buttons - simplified button arrangement for posting comments</li> <li>b680-lexdown-insert-image-triggers-submit-event - updated Lexical text editor implementation to prevent incorrect triggering of submit events</li> <li>b650-reactions-scroll-under-tabs-in-teacher-edit-mode - improved UI so that reaction and flag icons scroll under the top panel instead of over it</li> <li>b599-teacher-set-timings-statistics - added a graph to show statistics on how students access and work on sets</li> </ul>"},{"location":"releases/#release-20250324","title":"Release 2025/03/24","text":"<ul> <li>b531-teacher-modules-page-table-filtering-and-sorting - improved sorting of the module list in the teacher view</li> <li>b642-update-manually-hidden-in-sets-page-immediately-after-updating-in-settings - fixed the issue with the \"manually hidden\" switch not updating immediately</li> <li>b649-chatbot-message-reactions - introduced reactions and flagging features to allow students to comment on chatbot responses</li> <li>b657-breadcrumbs-should-show-module-name - added module name to the breadcrumbs</li> <li>b671-admin-chat-flags-page - added a new admin page to view student flags on chatbot responses</li> <li>b660-global-tag-attributes - added teacher email address as an attribute to the global tag to link the global tag with the corresponding teacher</li> </ul>"},{"location":"releases/#release-20250314","title":"Release 2025/03/14","text":"<ul> <li>b655-small-chat-improvements - Minor chat enhancements, primarily focused on providing suggestions.</li> <li>b663-milkup-editor-improvements-ii - Text editor tweaks.</li> <li>b659-style-of-chats-to-match-the-lexical-text-editor - Updated chat styling.</li> </ul>"},{"location":"releases/#release-20250311","title":"Release 2025/03/11","text":"<ul> <li>b658-milkup-editor-improvements-i - Text editor upgrades, including handling tables, centering images in PDFs, handling modals, and improving step-by-step display in worked solutions and tutorials.</li> </ul>"},{"location":"releases/#release-20250305","title":"Release 2025/03/05","text":"<ul> <li>b652-introducing-milkup-editor - Implemented a new Milkup editor (a Lexical-based editor with extensions developed by a student group) to replace the existing Milkdown editor.</li> <li>b627-add-a-switch-for-text-editors - Introduced switch to activate either Mildown editor or Milkup editor.</li> </ul>"},{"location":"releases/#release-20250225","title":"Release 2025/02/25","text":"<ul> <li>b373-show-number-of-unresolved-activities-on-tab-header - numbers added to tabs in TEACHER mode.</li> <li>b645-fix-question-version-duplicate-statistics - Fixed statistics to avoid duplicate counting.</li> <li>b634-teacher-modules-progress-bar-tooltip-fix - Removed module IDs from the progress bar tooltip.</li> <li>b601-stats-switch-to-edit - New switch in TEACHER mode from response area stats to edit.</li> <li>b635-chat-improvements-more-question-info - Additional question data provided to chatbots.</li> <li>b630-stats-not-refreshed-after-enrolling-or-deleting-a-student - stats refreshed immediatley when a student is added/removed to/from the module.</li> </ul>"},{"location":"releases/#release-20250219","title":"Release 2025/02/19","text":"<ul> <li>631-chat-welcome-message-for-new-conversations - Added a welcome message for students using the chatbot feature for the first time.</li> </ul>"},{"location":"releases/#release-20250213","title":"Release 2025/02/13","text":"<ul> <li>b574-teacher-roles - Introduced teacher roles to manage permissions and access levels.</li> <li>b583-student-view-should-be-the-same-for-teacher-and-admin - Admin users now have the same view in student mode as teachers.</li> <li>b624-teacher-modules-stats-performances - Improved performance when retrieving data for statistics and graphs in teacher modules.</li> <li>b637-chat-functions-upsert-service - Standardised chatbot function deployment to use the same mechanism as evaluation functions.</li> </ul>"},{"location":"releases/#release-20250207","title":"Release 2025/02/07","text":"<ul> <li>b600-teacher-set-ra-statistics - Set statistics for teachers</li> <li>b555-firefox-expression-writing-and-scanning-misaligned - Upgraded canvas library to make sure canvas is working with Firefox</li> <li>b568-header-no-drop-down-if-only-one-instance - The module instance to be displayed as a text (instead of dropdown) in the header if there is only one module instance</li> </ul>"},{"location":"releases/#release-20250205","title":"Release 2025/02/05","text":"<ul> <li>b559-instances-in-order - corrected order of module instances in the header for both, teacher and student view, to be in descending chronological order</li> <li>b598-query-for-admin-dashboard-evaluation-functions-needs-optimisation - optiised query for evaluation function statistics in admin dashboard</li> <li>b604-teacher-modules-stats - Show set's activity and progress statistics on teacher's modules list page</li> <li>b607-duplicate-notifications - preventing email notifications to be sent twice</li> </ul>"},{"location":"releases/#release-20250129","title":"Release 2025/01/29","text":"<ul> <li>b602-teacher-sets-overview-stats - Show set's activity and progress statistics on teacher's module overview page</li> <li>b603-teacher-sets-list-stats - Show set's activity and progress statistics on teacher's module content page</li> <li>b615-adjust-workspace-size-on-open - Ensure workspace width always stays in given boundaries</li> <li>b589-teacher-set-statistics-improvements - various improvements to the question statistics chart</li> </ul>"},{"location":"releases/#release-20250129_1","title":"Release 2025/01/29","text":"<ul> <li>b591-workspace-split-panes-improvements - Reworked the workspace split panes</li> </ul>"},{"location":"releases/#release-20250123","title":"Release 2025/01/23","text":"<ul> <li>b543-lost-canvas-snapshot - improve robustness of canvas saving</li> <li>b522-latex-edit-box-over-displayed-latex - milkdown UI fixes</li> </ul>"},{"location":"releases/#release-20250121","title":"Release 2025/01/21","text":"<ul> <li>b65-enhanced-stats - chart added in the question STATS tab</li> <li>b590-chat-canvas-documentation - chatbot documentation added</li> <li>b598-query-for-admin-dashboard-evaluation-functions-needs-optimisation - part 1 - evaluation function statistics disabled. Query will be optimised.</li> </ul>"},{"location":"releases/#release-20250115","title":"Release 2025/01/15","text":"<ul> <li>b524-modular-chatbot-workspace - new chatbot functionality</li> <li>b540-chatbot-switches - chatbot toggles</li> <li>b548-chat-mcqs-information-conversion - parse MCQ respones into a user-readable format for chatbots</li> <li>b552-pulumi-chat-infra-dev-staging-prod  - infrastructure for chatbots</li> <li>b578-inconsistent-lambda-function-number-of-errors - resolved inconsistencies in eval function error statistics</li> </ul>"},{"location":"releases/#release-20250110","title":"Release 2025/01/10","text":"<ul> <li>b584-PDF-tables - ensure tables compile in PDFs</li> </ul>"},{"location":"releases/#release-20250109","title":"Release 2025/01/09","text":"<ul> <li>b75-add-labels-to-users-to-allow-filtering-in-analytics-examples-of-useful-labels-guest-msc-group2-personaltutor-etc-user-type - added 'none' option to student filter</li> </ul>"},{"location":"releases/#release-20250108","title":"Release 2025/01/08","text":"<ul> <li>b75-add-labels-to-users-to-allow-filtering-in-analytics-examples-of-useful-labels-guest-msc-group2-personaltutor-etc-user-type - student admin categories and student module tags  </li> <li>b562-single-multiple-choice-toggle-not-clear - tidied up multiple choice toggle in the response area configuration panel</li> </ul>"},{"location":"releases/#release-20241212","title":"Release 2024/12/12","text":"<ul> <li>b563-misc-frontend-changes - client-side setup improvements</li> </ul>"},{"location":"releases/#release-20241206","title":"Release 2024/12/06","text":"<ul> <li>b556-Solutions-PDF---Lambda-step-error - PDF generator in backend - fix inline LaTeX (only in structured tutorial/final answer/worked solutions)</li> </ul>"},{"location":"releases/#release-20241209","title":"Release 2024/12/09","text":"<ul> <li>b440-notifications-in-ui - add notifications feed for teachers</li> </ul>"},{"location":"releases/#release-20241206_1","title":"Release 2024/12/06","text":"<ul> <li>b556-Solutions-PDF---Lambda-step-error - fix to PDF generator Lambda to handle steps (e.g. in worked solutions)</li> </ul>"},{"location":"releases/#release-20241202","title":"Release 2024/12/02","text":"<ul> <li>b533-teacher-header-eager-auth - Prevent header/wrapper requests to module without a user</li> <li>fix-week-bounds - Fix weekly recap bounds including Sunday</li> </ul>"},{"location":"releases/#release-20241129","title":"Release 2024/11/29","text":"<ul> <li>b460-ui-tweaks-iii - additional tweaks to the user interface</li> <li>b541-module-cloning-should-copy-over-support-material-settings - module cloning carries over settings for visibility of structured tutorials, final answers, and worked solutions.</li> </ul>"},{"location":"releases/#release-20241125","title":"Release 2024/11/25","text":"<ul> <li>b533-firebase-auth-idtoken-is-required - no change to UX. Fixed issue \"Error: idToken is required\" caused by a premature query before the token was available</li> <li>b537-a-gap-between-title-and-list-of-modules - fix: remove unintended gap between the page title and content when the canvas feature was opened</li> </ul>"},{"location":"releases/#release-20241123","title":"Release 2024/11/23","text":"<ul> <li>b544-eval-function-schema-403 - evaluation functions base layer: schema included to avoid 403 errors when retrieving</li> </ul>"},{"location":"releases/#release-20241120","title":"Release 2024/11/20","text":"<ul> <li>b526-studentgetmodules-query-is-slow - optimized data retrieval, sorting, and filtering for the student modules page</li> </ul>"},{"location":"releases/#release-20241115","title":"Release 2024/11/15","text":"<ul> <li>b463-save-button-for-all-response-types - save button to save work before submission. Configurable per response type at ADMIN level, and per response area at TEACHER.</li> <li>b468-email-updates-settings - introduced an admin feature allowing changes to the recap schedule setting for each teacher</li> <li>b503-the-list-of-errors-and-flags-in-teacher-view-to-contain-info-about-the-part - enhanced the teacher view by including details about which part each flag and error was created against</li> <li>b527-numericunits-displays-answers-incorrectly-in-the-configure-panel - resolved an issue with numeric units that previously displayed incorrectly when spaces were present</li> <li>b528-upgrade-next-from-1423-to-1424 - updated Next.js library</li> <li>b529-update-branches-info-in-readmemd - updated the README.md to provide developers with the latest information about the ticket board and testing processes</li> </ul>"},{"location":"releases/#release-20241113","title":"Release 2024/11/13","text":"<ul> <li>b506-add-authentication-with-google - replaced MSAL (Microsoft Authentication Library) with Firebase Authentication to allow sign-in using both Microsoft and Google accounts</li> </ul>"},{"location":"releases/#release-20241031","title":"Release 2024/10/31","text":"<ul> <li>b465-do-not-remove-whitespace-from-input-symbols - fix: remove spaces from input symbols only at the beginning and end, preserving spaces in the middle</li> <li>b477-set-json-generation-returns-403-if-a-media-is-not-accessible - improvement: PDF generation now returns a clearer error message when media fails to download</li> <li>b507-module-options-edit-does-not-work-correctly - fix: refresh module options in the teacher view after they are updated</li> <li>b511-publish-whole-set-questions-missing-from-list - fix: ensure the list of unchanged questions in information messages displays all relevant questions</li> <li>b512-add-eslint-rules-for-imports - improvement: adjusted import order in code for better readability and logical structure</li> <li>b513-open-link-choices-for-tab-columns-with-links - enhancement: added \u201copen link\u201d options to table columns containing hyperlinks, improving navigation across tables</li> <li>b514-pdf-generation-pandoc-exited-with-code-43-fontconfig-error-no-writable-cache-directories - adjustment: modified PDF generation to redirect Fontconfig logs to writable directories within the Lambda function</li> </ul>"},{"location":"releases/#release-20241028","title":"Release 2024/10/28","text":"<ul> <li>473-middle-click-cmdclick-on-links-in-mui-tables - added \u201copen link\u201d options to table columns with hyperlinks</li> <li>488-links-on-module-home-page - enhanced cards on the teacher module home page to allow clicks that navigate to relevant tabs</li> <li>499-switching-to-canvas-resets-to-part-a - prevented reset to part A when opening or closing the canvas</li> </ul>"},{"location":"releases/#release-20241023","title":"Release 2024/10/23","text":"<ul> <li>b498-feedback-does-not-handle-double-dollars - feedback string display offers basic support for double-dollars. (REVERTED)</li> <li>b505-enable-weekly-recap-by-default-for-new-users - email notifications tweaks for new users</li> </ul>"},{"location":"releases/#release-20241018","title":"Release 2024/10/18","text":"<ul> <li>b493-one-student-progress-csv-format-change - adjustment to the CSV format of an individual student\u2019s progress</li> <li>b495-augment-feedback-colour - updated augment feedback functionality</li> <li>b481-cannot-switch-page-in-the-error-panel-in-the-admin-dashboard - fix: allowing multiple tables on one page with paging functionality</li> </ul>"},{"location":"releases/#release-20241017","title":"Release 2024/10/17","text":"<ul> <li>b497-milkdown-response-area - milkdown response area type added</li> </ul>"},{"location":"releases/#release-20241016","title":"Release 2024/10/16","text":"<ul> <li>b482-question-scrolls-up-when-clicking-check-or-mark-as-done-on-mobile-and-tablet - fix: resolved issue where marking student submissions as done caused the page to scroll to the top on mobiles and tablets.</li> <li>b483-feedback-area-does-not-support-latex-rendering-again - adjustment to the feedback to support latex</li> <li>b490-dashboard-high-no-of-students - improved the dashboard\u2019s calculation of the current number of students.</li> <li>b494-augment-feedback-if-the-augment-is-true-and-the-returned-feedback-is-empty - updated the augment feedback functionality to handle cases where the augment flag is true, but the returned feedback is empty.</li> <li>b496-feedback-to-handle-html-and-latex - adjustment to the feedback to support html alongside latex</li> </ul>"},{"location":"releases/#release-20241015","title":"Release 2024/10/15","text":"<ul> <li>b461-bulk-rollover-follow-up-ii - updates to bulk rollover feature</li> <li>b479-scan-in-mobile-tablet-doesnt-activate-camera-or-file-selector - fix: camera did not activate in mobile or tablet when using scan option</li> </ul>"},{"location":"releases/#release-20241009","title":"Release 2024/10/09","text":"<ul> <li>b486-cloning-parts-in-wrong-order - corrected the part order in cloned module instances and updated code to ensure future clones maintain correct part order.</li> </ul>"},{"location":"releases/#release-20241007","title":"Release 2024/10/07","text":"<ul> <li>b220-download-one-student-progress - download an individual student\u2019s progress in CSV format.</li> <li>b471-admin-teacher-enrolment-should-accept-comma-separated-lists - admin enrolment of teachers accepts comma-separated lists, allowing multiple teachers to be added at once.</li> <li>b478-question-not-visible-in-tablet-mode-when-canvas-closed - fix: question was not visible in tablet mode when the canvas was closed.</li> <li>b480-draw-mode-proceed-button-almost-invisible-update-to-match-the-design-on-scan-mode - EXPRESSION response area: added the \u201cProceed\u201d button in draw mode to match the design on scan mode.</li> </ul>"},{"location":"releases/#release-20241004","title":"Release 2024/10/04","text":"<ul> <li>b218-teacher-view-modules-students-explore-do-not-limit-next - teacher view of student progress: \u201cNext\u201d button progresses to all students, not just those listed on the current table page.</li> <li>b448-question-jumps-to-top-on-mark-as-done - UI fix: resolved issue where marking student submissions as done caused the page to jump to the top.</li> <li>b466-limit-number-of-student-submissions - eval functions receive the number of student submissions per response area, and can optionally limit submissions e.g. to external services.</li> <li>b467-correct-separatefeedbacks-to-separatefeedback - schema correction: <code>separatefeedbacks</code> to <code>separatefeedback</code> (relates to the 'Augment feedback based on correctness' functionality.)</li> <li>b475-set-all-augment-booleans-to-false-in-a-specific-module - Set all \"Augment feedback based on correctness\" booleans to <code>false</code> for a selected math module.</li> </ul>"},{"location":"releases/relases_2023_24/","title":"2023-24","text":""},{"location":"releases/relases_2023_24/#release-20240927","title":"Release 2024/09/27","text":"<ul> <li>b457-option-to-omit-iscorrect-feedback - added the option for teachers to customise the feedback prefix and colour</li> <li>b459-image-on-the-same-line-as-preceding-text - corrected PDF generation to prevent images from appearing on the same line as the preceding text</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240923","title":"Release 2024/09/23","text":"<ul> <li>b449-improve-mobile-navigation - improve mobile UI</li> <li>b462-final-summer-tweaks - ui improvements and fixes</li> <li>b146-follow-up - email notifications tweaks and fixes</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240918","title":"Release 2024/09/18","text":"<ul> <li>b444-bulk-rollover-follow-up - updates to bulk rollover feature</li> <li>b452-ui-tweaks-ii - additional ui enhacements</li> <li>b453-config-panel-disabled-feedback-tab-is-still-accessible-by-next-and-previous-buttons - fixed an issue where disabled tabs in the response area configuration panel could still be accessed using navigation buttons</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240914","title":"Release 2024/09/14","text":"<ul> <li>b134-ipados-ios-safari-like-flag-problem-help-off-the-page - fixed scrolling of problem sets when using iPadOS or iOS Safari</li> <li>b439-disable-download-when-no-pdf-available - improved behavior of the \u201cDownload set\u201d drop-down button</li> <li>b407-all-questions-published-when-publishing-one - corrected version history page to display accurate data when viewing past question versions</li> <li>b434-ui-redesign-tweaks - additional tweaks to the user interface</li> <li>b442-bring-back-sets-download-options - restored \u201cSet Download\u201d drop-down button that was removed during the UI redesign</li> <li>b447-aws-db-backups-stopped - fixed the database backup issue</li> <li>b451-display-modules-even-if-sets-are-hidden - updated the system so students can view their modules, even if no sets are available</li> <li>b146-email-recap - add weekly email recap</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240910","title":"Release 2024/09/10","text":"<ul> <li>b413-authentication-failures - code cleanup for updated authentication to avoid logging out (see b413 in 2024/08/23)</li> <li>b438-enable-canvas-for-all-users - enabled canvas for all users (see b343 in 2024/06/28)</li> <li>b441-custom-milkdown-theme-bug - adjusted the new Milkdown theme to improve compatibility with math mode</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240904","title":"Release 2024/09/04","text":"<ul> <li>b441-revert-to-nord-theme - undo milkdown theme switch until math block is fixed</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240903","title":"Release 2024/09/03","text":"<ul> <li>b386-provide-drop-down-list-for-ra-default-lambda-function</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240902","title":"Release 2024/09/02","text":"<ul> <li>b398-upgrade-student-module-list - add new card view to module list student page</li> <li>b430-fix-redesign-bugs - add proper error handling to teacher pages when not a teacher</li> <li>b435-remove-env-from-git - remove .env file from git as it shouldn't be checked into vcs</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240828","title":"Release 2024/08/28","text":"<ul> <li>b383-check-for-existing-module-name - Check if module name already exists when creating new module</li> <li>b383_do_not_query_without_module_id - do not run query from header without module id </li> <li>b393-canvas-stored - Store canvas in database</li> <li>b425-studentmodule-and-studentmodules-api-retrieve-hidden-sets - do not retrieve hidden sets for students</li> <li>b427_display_pdf_errors_after_set_publish - Display PDF error message when publishing whole set</li> <li>b428-bulk-rollover-qa-comments - updates to bulk rollover feature</li> <li>b431-add-sentry - Add Sentry error monitoring</li> <li>b433-not-possible-to-enrol-students - fix to student enrollment</li> <li>b436-fix-instance-swapper - fix instance swapper</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240823-2","title":"Release 2024/08/23 - 2","text":"<ul> <li>b380-fix-tables - Table paging fix</li> <li>b379-disappearing-r-at-beginning-of-line-milkdown - fix disappearing 'R' at beginning of line milkdown</li> <li>b413-force-renew - authentication update to avoid logging out</li> <li>b429_deleting_cases_and_tests_qa - ensure feedback case order is maintained</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240823","title":"Release 2024/08/23","text":"<ul> <li>b413-grounded - updates to the authentication flow</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240822-2","title":"Release 2024/08/22 - 2","text":"<ul> <li>b402-create-table-view - table view added to the teacher module view</li> <li>b406-prepare-ui-update - changes in the user interface</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240822","title":"Release 2024/08/22","text":"<ul> <li>b356-delete-all-questions-in-set-not-handled-gracefully - create blank question when all others are deleted</li> <li>b409-module-bulk-rollover - bulk creation of new module instances (for admins)</li> <li>b417-set-part-to-a-when-switching-question - in teacher mode, when switching a question, always display part (a)</li> <li>b419-deleting-case-is-causing-feedbacks-to-shift-incorrectly - display correct remaining cases after deleting one</li> <li>b423-pdf-generation-displays-different-error-first-time - first PDF generation message consistent with follow up messages</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240807","title":"Release 2024/08/07","text":"<ul> <li>b375-correct-tex-and-pdf-for-solutions - PDF generation improvements</li> <li>b412-export-import-wrong-order-of-parts - ensure correct part and response-area order when importing</li> <li>b413-authentication-failures - part 2 - prevent application logouts</li> <li>b416-download-set-as-json-downloads-wrong-set - ensure correct set when exporting JSON</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240802","title":"Release 2024/08/02","text":"<ul> <li>b391-milkdown-fault-if-latex-on-last-line - fixed milkdown bug - if text ends with latex, it rendered raw but now renders properly.</li> <li>b415-guidance-time-suggestion-feature-by-colin - guidance time suggestion (using Machine Learning based on our database of content)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240726","title":"Release 2024/07/26","text":"<ul> <li>b369-guidance-text-on-enrollment - add a guidance text when adding students or teachers</li> <li>b381-not-possible-to-add-multiple-teachers - allow to add multiple teachers</li> <li>b410-open-doc-links-in-new-tab - open external links in a new tab</li> <li>b413-authentication-failures - fixed problems with application logouts</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240724","title":"Release 2024/07/24","text":"<ul> <li>b378-admin-module-instance-breadcrumbs-incorrect - corrected the application header (for Admin module instance page)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240723","title":"Release 2024/07/23","text":"<ul> <li>b132 - security update for milkdown</li> <li>b366-non-imperial-users-to-be-able-to-log-in - non-Imperial logins enabled</li> <li>b374-admin-analytics-evaluation-functions - analytics for evaluation function errors</li> <li>b387-import-question-does-not-import-worked-solutions - corrected question export with solutions</li> <li>b388-tutorials-and-solutions-steps-messed-up - corrected sorting of solution branches</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240718","title":"Release 2024/07/18","text":"<ul> <li>b392-essay-code-ras - add two new response area types: code and essay</li> <li>b396-better-feedback-box - improve the feedback box UI and accessibility</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240716","title":"Release 2024/07/16","text":"<ul> <li>b395-vertical-text-align - remove excess margin from top part of question</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240628","title":"Release 2024/06/28","text":"<ul> <li>b258-performance-analyse-db - faster response from DB queries</li> <li>b343-canvas - student canvas in beta mode (hidden by default)</li> <li>b351-teacher-module-page-ui-upgrades - Teacher module home page UI upgrades (tabs added)</li> <li>b357-upgrade-aws-sdk-v2-to-v3 - software library updates</li> <li>b359-generate-tex-file - upload/download whole Sets as LaTeX files</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240614","title":"Release 2024/06/14","text":"<ul> <li>b305-export-whole-set - import/export whole sets</li> <li>b334-upgrade-node-next-nest - library updates</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240529","title":"Release 2024/05/29","text":"<ul> <li>b352-support-to-eval-function-20-get-all-routes-by-getroutes - evaluation function deployment (check existing routes)</li> <li>b353-question-import-filter-out-unicode-characters - remove character (U-2006) on json import</li> <li>b354-double-confirm-on-confirmation-pop-ups - fix confirm button on modals in teacher mode</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240524","title":"Release 2024/05/24","text":"<ul> <li>b97-remove-all-references-to-mongodb - remove code linking to legacy databases</li> <li>b243-add-question-id-to-the-url - add question identifier to the url</li> <li>b285-move-module-instance-drop-down-to-left-to-replace-the-instance-label - more user-friendly module instance selection</li> <li>b304-milkdown-element-in-admin-that-will-display-on-home-page - add an administrator page to configure a home page banner</li> <li>b330-modal-update - question version switch: allow teacher either to save or discard existing draft</li> <li>b349-support-to-eval-function-20-ensure-deployments - evaluation functions: production and non-production versions</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240515","title":"Release 2024/05/15","text":"<ul> <li>b250-expression-ra-scan-mode-copy-and-paste - allow copy and paste and other improvements in the scan mode functionality</li> <li>b314-remove-experimental-from-ra-panel - \"experimental\" from the photo upload and handwritting labels removed</li> <li>b338-create-a-new-set-when-creating-a-module - a default set automatically creating when a new module is created</li> <li>b342-do-not-generate-pdf-when-creating-new-question - prevent PDF generation when a new question is added</li> <li>b345-after-pdf-generation-extraction-cleanup - a technical improvements into the PDF generation</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240426","title":"Release 2024/04/26","text":"<ul> <li>b230-pdf-generation-in-a-separate-lambda-function - PDF generation is faster, more secure, and frees up bandwidth on the main server.</li> <li>b328-improve-expression-ui-in-tests - fix expression response area preview in tests tab</li> <li>b331-work-solutions-empty-content-not-handled-as-no-work-solutions - work solutions and structured tutorials buttons not to be displayed if the content is empty</li> <li>b339-accessible-response-area-feedback - displaying error returned by evaluation functions in a user-friendly format</li> <li>b340-remove-input-type-changed-warning-on-new-ra - do not display warning about response area type change for new response areas</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240419","title":"Release 2024/04/19","text":"<ul> <li>337-individual-tests-always-fail - fix individual reponse area test runs</li> <li>327-consolidate-response-area-components - improvements for better response area consistency</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240321","title":"Release 2024/03/21","text":"<ul> <li>b332-table-smart-resizing - resize table width based on the screen size</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240319","title":"Release 2024/03/19","text":"<ul> <li>b286-ra-analytics-when-config-is-changed - Fix aggregates in stats</li> <li>b311-expression-area-layout-issues - Improve expression RA layout</li> <li>b325-populate-new-tests-with-the-answer - Populate new tests with the answer</li> <li>b322-enable-live-preview-in-teacher-mode - Attempt to enable live preview in teacher mode</li> <li>b35/b329-simplify-response-components - Simplificaton of Response Type components code</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240313","title":"Release 2024/03/13","text":"<ul> <li>b84-legacy-content-db-tables - DB updates. No change to UX</li> <li>b315-include-answer-when-importing-case - ensure the answer value is included when importing a case from stats</li> <li>b320-response-type-allowlist - improves modular response areas</li> <li>b325-populate-new-tests-with-the-answer - prepopulate the answer by the correct answer when creating a new test</li> <li>b326-question-alignments - imroves alignments on the edit question page</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240308","title":"Release 2024/03/08","text":"<ul> <li>b283-table-with-1-column-layout - improves table layout</li> <li>b324-show-required-error-on-number-input-wizard - improves number input validations</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240305","title":"Release 2024/03/05","text":"<ul> <li>b301-redesign-part-response-areas-and-text-between-them - teachers can drag response areas while surrounding text stays in place, and merges where necessary.</li> <li>b35/b310-modular-response-areas-phase-6-cleanup - completes modular response areas. Code improvements and removing legacy tables.</li> <li>b323-delete-empty-answer-in-ra-panel - ensure delete works in answer box in response area panel</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240304","title":"Release 2024/03/04","text":"<ul> <li>b319-survey-promotion-banner-on-home-page - add a banner onto the landing page advertising a survey with a link</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240301","title":"Release 2024/03/01","text":"<ul> <li>b287-limit-access-to-sets-published-outside-of-current-date - ensure access to Sets follows release rules, including via URL</li> <li>b303-redirect-help-to-userdocs - redirect lambdafeedback.com/help to user documentation and lambdafeedback.com/[module slug] to the module page</li> <li>b318-url-for-survey - redirect lambdafeedback.com/survey</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240229","title":"Release 2024/02/29","text":"<ul> <li>b35/b308-modular-response-areas-phase-4-custom-response-types - allow admin to dynamically create and manage new response types</li> <li>b35/b309-modular-response-areas-phase-5-migration - migrate all existing response types to the new modular type</li> <li>b313-always-display-post-ra-text-in-pdf-but-not-in-stats-mode - include all text in PDF (including after first response area)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240226","title":"Release 2024/02/26","text":"<ul> <li>b35-number-input-nan - fix handling of non-number input in the number answer wizard</li> <li>b317-no-header-refetch-on-mount - avoid unwanted refetch when resizing browser window on a set page</li> <li>b35/b307-modular-response-areas-phase-3-all-writes - start writting new and edited Response Area's Response to the new modular table</li> <li>b274-when-deleting-a-question-display-loading-message - display \"loading\" message when deleting a question</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240220","title":"Release 2024/02/20","text":"<ul> <li>b290-the-final-answer-button-is-displayed-even-if-there-is-no-final-answer - fix: only display 'final answer' button when there is content to show</li> <li>b297-give-error-if-creating-module-with-same-name-as-deleted-module - improved formatting of error messages</li> <li>b299-legacy-content-db-tables-ra-contents - DB updates. No change to UX.</li> <li>b302-modal-warning-before-disabling-branching - warning modal when disabling branching in worked solutions and structured tutorials</li> <li>b35/b306-modular-response-areas-phase-2-new-modular-type - backend updates for modular response areas.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240215","title":"Release 2024/02/15","text":"<ul> <li>b35/b295-modular-response-areas-phase-1-switchless-frontend - a technical improvement to the response area building blocks in the code, so that it is easier, more intuitive and more straight forward to add new response areas</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240213","title":"Release 2024/02/13","text":"<ul> <li>b271-unify-modals - unified modals to use same style</li> <li>b294-check-imports-from-material-ui - prevent importing whole library when importing an icon</li> <li>b300-delete-ra-add-warning-into-the-modal-that-the-text-below-the-ra-will-be-deleted-as-well - when deleting a response area (RA), warning modal that text below RA will also be deleted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240125","title":"Release 2024/01/25","text":"<ul> <li>b240-structured-tutorial-component-upgrade - converted structured tutorial to use the same structure and logic as worked solutions</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240124","title":"Release 2024/01/24","text":"<ul> <li>b273-limit-access-to-unpublished-sets - ensure no student access to hidden sets via a url</li> <li>b277-milkdown-first-non-markdown-update-is-ignored - milkdown fix to for edge cases that were not saved (single character; deleting selection).</li> <li>b279-table-with-1-column - wider columns for table response areas with one column</li> <li>b280-change-response-colour-to-white - specifically for 'riskAssessment' evaluation function: display feedback for incorrect answer in white colour</li> <li>b281-tweaks-to-ra-analytics - tweaks to response area analytics</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240116","title":"Release 2024/01/16","text":"<ul> <li>b272-legacy-db-tables-tutorial-sections - refactoring the database. No change to UX.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20240110","title":"Release 2024/01/10","text":"<ul> <li>b264-untangle-changes - a technical improvement to make the milkdown wrapper code clearer.</li> <li>b247-re-generate-pdf-after-deleting-a-question - an improvement so that the PDF is automatically re-generated when a published question is deleted</li> <li>b158-change-prod-bucket-to-prod-not-staging - a technical change so that imported images and generated PDF files are saved in the correct AWS bucket dependently on the environment (production, staging or development)</li> <li>b232-ra-analytics-visual-alignment - a change to display response area analytics correctly aligned with labels</li> <li>b77-published-question-change-of-input-type - an improvement to allow changing of the input type on the response area that was already published.</li> <li>b262-legacy-content-db-tables-part-contents - refactoring the database. No change to UX.</li> <li>b245-question-numbering-is-sometimes-wrong-on-the-student-module-home-page - a correction so that question numbers are reconciled after a question is deleted</li> <li>b141-update-link-in-modal - a correction of the link from the modal (which appears when deleting a response area) to the user documentation</li> <li>b211-response-area-preview-remove-border - a change in the question preview in the teacher mode so that it is displayed in the same way as in the student mode</li> <li>b103-milkdown-slow-rendering - a technical change to speed up testing in local development environments</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231215","title":"Release 2023/12/15","text":"<ul> <li>b103-milkdown-slow-rendering - developers can set a flag in local environment to speed up rendering pages with milkdown</li> <li>b235-content-with-hash-copied-across - prevent milkdown copying content with hash from one question to another</li> <li>b244-fix-notes-saving-in-the-student-mode - ensure student notes are visible including when switching from teacher to student mode</li> <li>b248-remove-unwanted-content-from-pdf - removed legacy response area pre-text and post-text from PDFs</li> <li>b251-post-a-reply-in-one-click - post a reply to a comment with one click</li> <li>b256-include-frequency-data-when-downloading-csv - correction to csv file generation for question stats, to include question numbers and frequency</li> <li>b260-number-and-unit-ra-do-not-align-with-pre-text-in-student-mode - align pre-text in the response area with number and units in student mode</li> <li>b261-master-content-sometimes-not-saved - ensure master content entered by the user is saved after publishing a question (not copied from the published version)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231208","title":"Release 2023/12/08","text":"<ul> <li>b246-rendering-of-list-of-sets-in-teacher-mode-takes-long-time - an improvement to render list of sets in teacher mode quicker</li> <li>b255-recover-lost-marked-parts - further corrections to DB. Some question parts were not marked correctly as DONE for questions imported from JSON between 13/10/23 and 5/12/24.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231205","title":"Release 2023/12/05","text":"<ul> <li>b242-mark-as-done-copied-across-questions - correction to DB submissions for questions imported from JSON between 13/10/23 and 5/12/24, which were linked together incorrectly.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231204","title":"Release 2023/12/04","text":"<ul> <li>b224-add-guidance-to-help - guidance on a question, already visible to users in a widget on top-right, is now also visible with the support material below the question   </li> <li>b228-legacy-content-db-tables-master-content - refactoring the database. No change to UX.</li> <li>b109-expression-input-tweaks - tweaks to the few improvements in the expression response area (555 in 2023/05/26): icons, placeholder, upload size limit.</li> <li>b249-selected-question-index-lost - editor UX, improve the robustness of: when a question is added or published, ensure that question remains in focus to the user.</li> <li>b241-link-from-feed-needs-updating - corrected a URL linking from the teacher feed to a question.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231113","title":"Release 2023/11/13","text":"<ul> <li>b227-correct-set-estimates - time format improvement for displaying time estimate for each set in the list of set</li> <li>b233-publish-set-pdf-generation - an adjustment to the Publish whole set functionality to generate PDF after the confirmation button is clicked   </li> </ul>"},{"location":"releases/relases_2023_24/#release-20231109","title":"Release 2023/11/09","text":"<ul> <li>b186-add-time-estimates-for-each-set-in-teacher-mode - added set estimates which is calculated as summary of estimates of all questions</li> <li>b204-input-symbols-empty-row-should-not-be-validated - an improvement to prevent validation of input symbols when a new row to enter input symbols is added</li> <li>b206-input-symbols-with-spaces - an improvement to remove potential spaces entered into the input symbol alternatives (the values must be seaparated by comma without spaces to make sure they work correctly)</li> <li>b226-update-question-split-prisma-transaction - extended Prisma timeout when a question is being saved or publish</li> <li>b225-bug-in-timed-release-for-pm-times - a change to display hours in 24 hour format when displaying time</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231103","title":"Release 2023/11/03","text":"<ul> <li>b214-admin-dashboard-carry-on - admin dashboard improvements:</li> <li>A drop down list to select the time period for the user access events graph</li> <li>The last part of the graph lines are dotted to make clear that last values are subject to change</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231101","title":"Release 2023/11/01","text":"<ul> <li>b207-pressing-enter-in-the-flag-textbox - an improvement so that when a user is using an expression response area and he attempts to submit a comment (or flag a problem) at the same time by clicking the enter, then only the comment (or the problem message) is submitted (and not the answer in the response area)</li> <li>b213-question-export-import-to-handle-mp3 - an improvement to allow to export and import questions containing an audio (or more audios)</li> <li>b217-remove-header-text-on-module-page-for-students - removed the header on the student module page as it is not needed</li> <li>b215-do-not-update-or-delete-notes-in-teacher-preview - an improvement to prevent submitting student solutions in the teacher preview mode</li> <li>b208-unposted-comments - an imrovement to handle the scenario when a user enters a comment and then, withouth submitting it, selects different question (the comment was copied to the newly selected question which is not a desired feature)</li> <li>b209-zero-comments-invite-comments - an improvement to open comments when there are no comments to invite users to comment</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231020","title":"Release 2023/10/20","text":"<ul> <li>b202-ensure-eval-function-defaults-for-new-response-areas - an improvement so that evaluation function parameters are set to default values when creating a new response area</li> <li>b71-analytics-tweaks-teacher-view - the students list, view and contact pages were merged into a single page: - Filters by email and/or by access are available to filter the single list of students - A click on a student email opens a view which displays the same analytics the student can see   </li> <li>b162-analytics-tweaks-stats-modal - improvements in the analytics view: - Colour is indicating the answer's case colour, if any, or the correct/incorrect default colour - Checkmark is indicating that the answer was correct - More options added to allow the user to agregate student answers   </li> <li> <p>b67-simplify-stats-interaction - few changes to response area statistics in the teacher mode:</p> </li> <li> <p>The case is imported straight into the relevant response area</p> </li> <li> <p>The response area menu has a new button EXPLORE so that the teacher can see the statistics per response area     </p> </li> <li> <p>b192-reaction-count-one-hour-challenge - users can see the individual count of each type of reaction</p> </li> </ul> <p></p> <ul> <li>b183-activity-feed-make-clear-there-are-more-flags-than-5 - make clear to the user how many flags and comments there are in total as there might be more than 5 displayed on the teacher dashboard</li> </ul> <p></p> <ul> <li>b110-import-multiple-jsons-from-a-single-zip - allows to import more questions from one zip file. This includes questions with attached pictures. Import of questions with attached audio files is yet to come.</li> <li>b205-admin-analytics-initial-work - first version of the admin dashboard is now provided. It includes information about number of current users, questions and user access events</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231012","title":"Release 2023/10/12","text":"<ul> <li>b180-prod-freezing-and-restarting - increasing allocated memory to accommodate multiple users triggering heavy processes (PDF compilation)</li> <li>b193-implement-auto-scaling-on-infrastructure - Infrastructure upgrades for larger scale usage.</li> <li>b124-question-export-with-pictures-fails-sometimes-on-cors-error - forcing Chrome to refresh media retriaval from S3 bucket to make sure correct headers are attached to the response</li> <li>b199-migration-script-for-physics-expression-ra - DB migration for legacy content.</li> <li>b194-create-set-and-first-question-improvement - new set automatically has a blank question ready.</li> <li>b197-not-possible-to-delete-a-question - increase timeout when deleting a question</li> <li>b143-more-info-in-modal-when-publish-whole-set - displaying list of questions that will be published in a modal before publishing whole set.</li> <li>b144-modal-to-check-before-removing-branches - a warning message is displayed before a branch from worked solutions is deleted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231005","title":"Release 2023/10/05","text":"<ul> <li>b136-change-to-breadcrumbs - an improvement to remove module instances from teachers and students breadcrumbs as they do not link to any pages</li> <li>b188-add-information-when-rendering-a-new-question - adding information that a question is being created when adding a new question</li> <li>b189-failed-fetching-your-problem-set-message-appearing-when-it-should-not - a warning message 'Failed fetching your problem set' is to be displayed only if there is an error</li> </ul>"},{"location":"releases/relases_2023_24/#release-20231003","title":"Release 2023/10/03","text":"<ul> <li>b191-expression-response-area-defaults - an improvement so that when creating a new response area of type EXPRESSION, the default values are set to:</li> <li>TRUE for Live preview</li> <li>FALSE for Display input symbols</li> <li>FALSE for Include in PDF</li> <li>TRUE for Enable handwriting input</li> <li>TRUE for Enable photo upload</li> <li>b187-support-materials-access-enhancements - enhancements to the support materals student access configuration:</li> <li>A new button event was added to record whether students proceeded or cancelled after a warning message appeared when a student tried to open a support material</li> <li>Labels were renamed to make their meaning clearer (e.g. 'Open' was changed to 'Available' and 'Hidden' to 'Unavailable')</li> <li>When a question part is marked as done, then no warning is displayed to a student when opening a support material (even if marked as Open with warnings)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230929","title":"Release 2023/09/29","text":"<ul> <li>b148-problem-adding-new-question-after-changing-name-of-current-question - an improvement so that a user cannot start changing newly added question (e.g. changing name) until all processes are finished and therefore preventing these changes to be wiped out.</li> <li>b161-renaming-question-straight-after-making-it - this is the same problem as b148</li> <li>b151-quote-marks-can-break-flags - an improvement so that double-quote marks, if used in a text, are displayed correctly in the generated csv file</li> <li>b164-grade-param-type-changed-reverts-to-string-when-value-is-empty - an improvment to identify a number as a number in the grade parameters, so that the type is displayed number and not as string</li> <li>b166-no-template-questions-in-the-list - an improvement to display all existing template questions in the list (when adding a new question from a template)</li> <li>b190-draw-area-width-keeps-changing - an improvement to stop the drawing area changing its width when a warning message is displayed that the writting cannot be interpreted</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230927","title":"Release 2023/09/27","text":"<ul> <li>b157-new-eval-function-reset-parameters - improvement in the response area panel, when the evaluation function is changed, then the default evaluation function parameters are re-set.</li> <li>b167-teachers-are-sent-to-the-most-recent-instance-on-the-module-homepage-even-when-they-dont-have-access-they-should-be-sent-to-the-most-recent-one-that-they-have-access-to</li> <li>b163-failed-fetching-your-problem-set-displayed-on-every-page-load - an improvement so that the warning message only appears when the fetch returns an error.</li> <li>b149-restrict-access-to-worked-solutions - restrict student access to support materials on set level and on question level.   </li> <li>b165-preview-not-the-same-as-student-view - an improvement to displaye pre-text, value and post-text aligned horizontally in the response area student view</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230908","title":"Release 2023/09/08","text":"<ul> <li>128-feedback-area-does-not-support-latex-rendering - Feedbacks returned by the evaluation function are displayed using latex editor.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230907","title":"Release 2023/09/07","text":"<ul> <li>b155-aws-ending-support-for-nodejs-14-in-aws-lambda - A clear-up of an outdated library.</li> <li>b153-pressing-enter-in-a-number-response-adds-new-line-to-the-response - Handle Enter in the response area as a submission of the answer.</li> <li>b139-archive-feature-enhancements - Enancements of module as module instance archiving.</li> <li>b68-cleaning-up-the-editor - many ui enhancements in the question editing page</li> <li>b115-case-color-under-feedback-tab-for-response-area-is-not-functional - The custom colour for feedbacks is now displayed correctly.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230830","title":"Release 2023/08/30","text":"<ul> <li>b33-audio-clips - in the content editor, drag-and-drop an audio file, and it will add a sound (e.g. narration) to the content.</li> <li>b145-xetex-pdf - PDFs are now compiled with xelatex, not PDFlatex.</li> <li>b150-extracting-code-from-listener-into-callback-fn - stats for typed expressions now record full submissions only (not keystrokes)</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230822","title":"Release 2023/08/22","text":"<ul> <li>b147-time-guidance-is-currently-very-small - An adjustment after upgrading one of the libraries which caused the time guidance to shrink.</li> <li>b142-module-clone-enhancements - An enhancemnt to include links to already generated PDF files for all sets in the cloned module instance.</li> <li>b138-503-error - An enhancement to navigate to the teacher module / module instance after clicking Cancel button in the Set Metadata page.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230818","title":"Release 2023/08/18","text":"<ul> <li>b114-matrix-input-centering-in-teacher-mode-but-not-in-student-mode - The Check button for matrix questions in the response area panel is now vertically centred in the student view.</li> <li>b140-response-area-pre-text-doubled - The legacy response area pre-text was removed from the student view.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230816","title":"Release 2023/08/16","text":"<ul> <li>b127-cloned-instances-are-missing-tutorials-and-worked-solutions - An enhancement of the module cloning functionality to include worked solutions and tutorials.</li> <li>b125-when-publishing-question-update-the-student-view - An enhacement so that when a teacher publishes a question then, this question is visible in the student view without having to refresh the browser or log out and back in again.</li> <li>b83-revisit-set-archiving - This is a technical improvement of the existing functionality to archive sets so that it is done in the same way as archiving of other entities. It has no visible any impacts to a user.</li> <li>b111-archive-module-instance-option - A new feature to allow to archive a module instance. This feature is only available to an administrator.</li> <li>b126-archive-module-option - A new feature to allow to archive a module. This feature is only available to an administrator.</li> <li>b108-error-when-clicking-add-question-button-while-inside-part-content-box - Technical improvement. Upgrade of some libraries (Material UI) to prevent errors caused by issues in the older library version.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230721","title":"Release 2023/07/21","text":"<ul> <li>b101-tests-run-from-the-configure-panel-have-the-islatex-parameter-set-to-true - A correction to the settings on the new Expression input (see 555 in 2023/05/26). When calling an evaluation function, the <code>is_latex</code> parameter dependends on the type of input (type/draw/scan).</li> <li>b120-PDF-skill-time-info - PDFs now include information on skill level, time estimates, and guidance below the question title and above the question content.</li> <li>b122-multi-year-carry-on - extended UI features referring to module instances (see b82 below).</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230719","title":"Release 2023/07/19","text":"<ul> <li>b82-multi-year-duplicate-module-instance-and-link-entities - new feature to clone module instances   </li> <li>b118-multi-year-tidy-up - multi module feature enhancements such as sorting and filtering module instances on the admin Module page</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230714","title":"Release 2023/07/14","text":"<ul> <li>b72-multi-year-module-instances-introduction - All Modules now exist as an 'Instance' of a Module, in preparation for allowing multiple Instances. The UI navigation is updated to handle Module Instances.</li> <li>b81-show-preview-of-ra-in-input-type-select - Selecting an Input Type for a Response Area: a searchable preview of Input Types improves the UX: users see the preview while selecting.   </li> <li>b91-prevent-multiple-blank-questions - When a question is added, the 'add quesiton' button is temporarily disabled while the application updates.</li> <li>b112-bug-the-tab-navigation-bar-at-the-top-disappears - Editor tabs are pesistent including during keyboard navigation</li> <li>b116-pdf-display-between-ras - PDF generation: for multiple Response Areas in a Part, the order is now always correct</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230622","title":"Release 2023/06/22","text":"<ul> <li>b39-new-editor-menus - question editor area menus have been converted into tabs. Other improvements have also been made to the editor layout inlcuding switching between teacher and student mode and staying on the same question. </li> <li>b62-add-tabs-to-reponse-area-panel - the Response Area panel is grouped into tabs that aid navigation and encourage a workflow that matches the way teachers think. Other layout improvements were also made within the tabs. </li> <li>b79-input-type-on-published-ra-should-not-be-editable - input type cannot be changed after publishing (see 598 here 2023/06/05).</li> <li>b85-incorrect-required-error-message - enhanced validation for number 0 in numeric response area.</li> <li>588-question-import-export-handle-images - import export includes images; a zip file is used to combine the JSON and the images.</li> <li>601-parameter-defaults-for-an-eval-function-cpq - improved the appearance of boolean evaluation function parameters.</li> <li>603-user-docs-updates - user documentation repo renamed from \"documentation\" to \"user_documentation\".</li> <li>608-link-word-sign-in-to-sign-in-on-homepage - on the home page 'sign in' text is now a link to sign in.</li> <li>619-mcq-check-button-should-be-vertically-central - the Check button for multi-choice questions in the response area panel is now vertically centred.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230605","title":"Release 2023/06/05","text":"<ul> <li>598-published-questions-change-of-approach - questions are now fully editable after publishing. All data from student responses persists through these changes. One exception is that the input type of a response area cannot be changed after publication, because this would change the format of the data that is recorded (you can, however, delete the response area and create a new one instead). Other new features: duplicate a Response Area; reorder Response Areas using drag and drop (in a similar way as reordering Parts).</li> <li>613-enable-publish-whole-set - see 606 below (2023/05/26). The 'Publish Whole Set' button is now enabled.</li> <li>614-error-with-stats-on-dev - ensures statistics still work with the new handwriting input (see 555 below).</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230526","title":"Release 2023/05/26","text":"<ul> <li>555-handwriting-response-area-upgrades - A new version of the Expression input type is in use. Input by handwriting onscreen or with scanned images is an option for teachers to make available to students (default: off). Also, regardless of the input mode (type/draw/scan) the live preview now gives 'pre-submission' feedback on whether the response can be interpreted, and the Check button is only available if interpretation is successful.</li> <li>606-publish-whole-set-causing-stats-to-disappear - The 'Publish Whole Set' button in Teacher Edit mode has been disabled because it was causing data to become unlikned in the DB, giving the effect of data like number of completed parts 'disappearing'. Existing data has now been relinked and is all visible to users. The feature that caused the problem has been disabled while we prepare a replacement to be pushed shortly.</li> <li>612-whole-part-marked-as-done-with-more-response-areas - Student functionality. If a question part has multiple Response Areas, the logic is now that only if all Response Areas are correctly answered will the 'Mark as done' feature be automatically checked. Previously only one correct answer was required to trigger this effect.</li> <li>585-question-simple-import-and-export - Teacher functionality. The import/export functionality has been enhanced so that it Response Area parameters, cases, and tests are now all included.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230321","title":"Release 2023/03/21","text":"<ul> <li>571-simple-teacher-comment-feed - Teacher functionality. New 'Activity feed' (formerly 'Flagged Questions') contains flagged questions and comments. The teacher can filter the table to see e.g. only flags or only comments. The teacher can also sort the table e.g. to see the new activities first.</li> <li>569-numeric-input-strips-out-strings-that-may-have-meaning - Technical dept. For the Response Area input type 'Number', additional validation added; if the input contains a non-numeric value then a relevant error message is displayed to the user (this is linked to the 573-response-area-validation-specific-errors below).</li> <li>573-response-area-validation-specific-errors - Teacher and student functionality. More specific error messages are displayed when the user inserts a value in an incorrect format (e.g. a non-numeric value into the input that expects a number).</li> <li>582-empty-structured-tutorial-shouldnt-display - Technical debt. When a tutorial is deleted, it is not displayed at all to students (as opposed to being blank).</li> <li>585-question-simple-import-and-export - Teacher functionality. Export a question to a file in JSON format. Import a question from a file in JSON format. Images are not imported/exported - these need to be handled manually until a new feature is ready. This feature opens the door to file imports if content can be converted into the correct format.</li> <li>586-question-import-add-schema-validation - Teacher functionality. When importing a question from a file, the data structure and format is validated. If the validation fails then relevant error messages displayed to the user.</li> </ul>"},{"location":"releases/relases_2023_24/#release-20230306","title":"Release 2023/03/06","text":"<ul> <li>566-pdf-error-identification - Teacher functionality. When a PDF fails to compile, the location of the error source is given in more detail, e.g. 'Q2(c)'.</li> <li>576-orderedsetids-throws-error-in-main - Technical. When loading sets in a module on the teacher side, an error no longer appears in the console.</li> <li>522-adding-teacher-when-creating-module-inadequate-error-message - Admin functionality. When adding a new module, a teacher can be added simultaneously. If the proposed teacher is not already registered as a teacher, then they are now automatically created as a teacher and a confirmation message is displayed.</li> <li>524-remove-teacher-from-list - Admin functionality. Remove a teacher from the list of teachers. If the teacher is still a teacher on a module, then display a modal confirming which modules the teacher will be removed from. If the user confirms, the teacher is removed from all the modules and then they are deleted from the list of teachers.</li> <li>572-comment-upvote-tweeks - Teacher and Student functionality. Right margin on the comments tweaked so that the sorting feature and 'post' button are not too far away from each other.</li> <li>483-show-all-button - Teacher functionality. The Show All feature is now enabled in the question preview mode.</li> <li> <p>520-default-to-an-eval-function-after-selecting-the-response-area - Teacher functionality. In the Response area edit panel, automatically select a default eval function as follows (it can be edited by the teacher if necessary). The default selections are:</p> Response area Default evaluation function MCQ arrayEqual NUMERIC isSimilar Expression and Text symbolicEqual Table and Matrix arraySymbolicEqual NUMERIC_UNITS comparePhysicalQuantities </li> </ul>"},{"location":"releases/relases_2023_24/#release-20230210","title":"Release 2023/02/10","text":"<ul> <li>560-comment-feature-tweaks - UI improvements to the comments feature.</li> <li>550-comment-upvotes - users can upvote comments by clicking on the heart. Sorting by upvotes is default</li> <li>546-always-test-a-response-area-when-saving - [For teachers only] this is an invisible feature that automatically tests a response area, when closing the editing panel, to check that the correct answer is accepted as correct. A failure to pass this test will show an error and not save the response area until fixed. The reason for this feature is to catch things like empty cells in the teacher answer which, e.g. for a matrix, would make marking student answers impossible. If such errors were allowed to pass, then students would experience errors when using the response area - hence it cannot be allowed. The auto-test feature is not enabled for all response areas as some are not compatible - the option can be enabled/disabled for each response area by admins.</li> <li>568-numeric-input-expects-string - upgrade to the numeric input type when dealing with string inputs.</li> </ul>"},{"location":"releases/status/","title":"Status","text":"<p>Lambda Feedback is a cloud-native application that is available with full service 24/7.</p> <p>This page contains information about any known incidents where service was interrupted. The page begain in November 2024 following a significant incident. The purpose is to be informative, transparent, and ensure lessons are always learned so that service improves over time.</p>"},{"location":"releases/status/#2025-march-28th-access-blocked-within-a-particular-organisations-wifi","title":"2025 March 28th: access blocked within a particular organisation's WiFi","text":"<p>The URL lambdafeedback.com is served by a content delivery network (CDN), that was blocked by a particular organisation's WiFi. During this period, users on that WiFi couldn't access the site.  </p>"},{"location":"releases/status/#timeline","title":"Timeline","text":"<p>2025/03/27 09:17 GMT We received a report that some users can't load the website at all. We announced this on the home page.</p> <p>2025/03/27 10:37 GMT The issues were identified as isolated to Imperial WiFi. Update on home page including advice to use a different WiFi (e.g. hotspot, or other location), or a different DNS. Ticket with ICT and number shared with users. There was a response within minutes requesting more information, but no further response until the next morning.</p> <p>2025/03/28 09:22 GMT Imperial ICT acknowledged that their security software had blocked the whole CDN. Lambda Feedback was specifically unblocked and full service was resumed. We have asked for a broader unblocking.</p> <p>2025/03/28 09:32 GMT Authentication services were down (no logins) but resumed within a few minutes and service remained good. We'll investigate and report.</p> <p>2025/03/28 13:47 GMT Imperial ICT confirmed a wider unblocking.</p> <p>2025/03/28 14:45 GMT Incident closed. </p>"},{"location":"releases/status/#lessons-learned","title":"Lessons learned:","text":"<p>Networks that provide internet access can block or incorrectly redirect users when trying to access Lambda Feedback. The block can be specific to one site or, as in this case, it can block a whole content delivery network (CDN) that serves many sites. </p> <p>Affected users will never reach the site, and we will have no way to know that they are failing to access. </p>"},{"location":"releases/status/#recommended-actions","title":"Recommended Actions","text":"<ul> <li> <p>Alert the ICT departments of key user groups to this problem, and ensure in advance that the relevent CDNs are not blocked.</p> </li> <li> <p>Create a backup plan for if the URL or the CDN are not correctly routed</p> </li> <li> <p>Monitor traffic to identify drops in usage that may indicate an issue (we already do this, but the drop was not significant enough in this case to be evident)</p> </li> <li> <p>Monitor the Lambda Feedback email address (this was effective in this case and we were in touch with users)</p> </li> <li> <p>Create a live chat with 'power users' for better communication during these incidents. A chat was started during this incident and will continue to be used</p> </li> <li> <p>Consider local WiFi/networks as a possible cause for blocking site access, and test for this cause when troubleshooting access issues</p> </li> <li> <p>Investigate the cause of the short unavailability of logins at 09:33 GMT on 28th March 2025.</p> </li> </ul>"},{"location":"releases/status/#2025-february-13th-incident-related-to-new-teacher-roles-feature","title":"2025 February 13th: Incident related to new teacher roles feature","text":"<p>During this period teachers were not able to access teacher pages.</p>"},{"location":"releases/status/#timeline_1","title":"Timeline","text":"<p>7:20am deployed a set of new features, including teacher roles</p> <p>9:47am issue reported - users with the TEACHER role were unable to access teacher pages.</p> <p>10:27am Issue reproduced on staging</p> <p>10:39am Issue fixed on staging; release to production initiated</p> <p>10:49am Confirmation that the fix worked in production</p>"},{"location":"releases/status/#lessons-learned_1","title":"Lessons learned:","text":"<ul> <li>Features that behave differently for users with the TEACHER role (compared to users with the ADMIN role) must be tested by a user with the TEACHER role who is not a super-admin. This is because super-admins automatically revert from TEACHER back to ADMIN. The same applies to features that behave differently for users with the STUDENT role.</li> </ul>"},{"location":"releases/status/#2024-mid-december-to-2025-january-2nd-imperial-college-security-measures-affected-logins","title":"2024 mid-December to 2025 January 2nd: Imperial College security measures affected logins","text":"<p>During this period the application was 100% available and operational. We were alerted on 2nd January that some users were not given permission by Imperial College London Microsoft 365 to login to third party applications. This was a a severe incident as it affected access to the application for some users.</p>"},{"location":"releases/status/#timeline_2","title":"Timeline","text":"<p>11:19 GMT We discovered the issue, escalated to Imperial College ICT and put a notice on our home page.</p> <p>16:51 GMT The application was approved by Imperial College ICT, which resolved the problem.</p> <p>Monitoring immediately after and the following morning showed that two known users who had issues no longer have issues. Other users continued to login before, during, and after the incident. No further reports of issues received. Case closed.</p>"},{"location":"releases/status/#analysis","title":"Analysis","text":"<ul> <li> <p>Lambda Feedback has been using Imperial College Microsoft 365 logins (Entra / Azure Active directory) since July 2021 without issues until this incident.</p> </li> <li> <p>ICT reported on 2nd January: \"due to heightened security arrangements put in place in mid December, ICT prevented unauthorised App registrations as these can pose security threats through unwarranted access to user information and the access to other systems. We have now granted access for the Lambda feedback app.\"</p> </li> <li> <p>Although the app was 'registered' on Entra/AAD in July 2021 within the Imperial College tenant, the permissions required for authentication (read the profile of the user) were granted by the user the first time they logged on. The changes imposed by ICT withdrew the privilege from users to grant such permissions, hence the inability to login. Permissions can be bulk granted by admins in the tenant, but this had not been done. On 2nd January, those permissions were given by admin and the problem was resolved.</p> </li> <li> <p>Due to the holiday season, this problem only surfaced on 2nd January, despite the cause being in 'mid-December'.</p> </li> </ul>"},{"location":"releases/status/#lessons-learned_2","title":"Lessons learned:","text":"<ul> <li>When an organisation uses single sign-on (SSO), ensure that the Lambda Feedback application permissions are granted by the organisation admin, even if the service initially works without those permissions being granted by admins. This action will protect against possible future issues, especially like the incident reported here.</li> </ul>"},{"location":"releases/status/#2024-november-4-8th-incident-related-to-new-logins","title":"2024 November 4-8th: Incident related to new logins","text":"<p>Deployment of a new authentication process caused service interruptions. Login was not possible at certain times, affecting all users. Effects were between Monday 4th and Friday 8th November, all related to release b506.</p>"},{"location":"releases/status/#timeline_3","title":"Timeline","text":"<p>Monday 4th November:</p> <p>7:30am deployed new login system. Initially appeared OK but concerns over server loading.</p> <p>8:15am began reverting to the old system to avoid any unnecessary risks. Revert took longer than rehearsed.</p> <p>8:55am system live and operational in its reverted form. No issues hereafter.</p> <p>The issue was identified and fixed ready to push with the new login system early the next day.</p> <p>Tuesday 5th November</p> <p>6:30am deployed new login system with update to avoid errors found on previous push. Systems fine.</p> <p>9:27am first report of login issues with ipads.</p> <p>9:37am issue with Safari and iOS identified. Message update on the app.</p> <p>10:45am revert complete after efforts to implement a fix failed.</p> <p>The issue was related to blocked thrid-party cookies being on some browsers. A fix was developed to push the next day.</p> <p>Wednesday 6th November</p> <p>7:08am deployed new login system with updated auth URLs. Testing on all device types OK.</p> <p>12:00pm Two users reported issues via email. Investigations continued including emails with users.</p> <p>Thursday 7th November</p> <p>Throughout the day the problem was analysed and we found that 12 users had logged in during a brief configuration error, and as a result were not stored correctly in the new auth DB, and this caused problems. All users access was restored and they were contacted with information and an apology.</p> <p>Friday 8th November</p> <p>10:31am The app became intermittent. This was traced to an updated in the logging. Resource was increased which partially solved the problem. A deployment was pushed to solve the problem.</p> <p>11:00am (approx) the issues were fully solved.</p>"},{"location":"releases/status/#lessons-learned_3","title":"Lessons learned:","text":"<ul> <li>When practicing a revert before a deployment, make sure the exact same revert process is used in production (e.g. don't go via CircleCI if rehearsals were via GitHub). Lesson from Monday morning following a delayed revert process.</li> <li>Test changes on all device types. Lesson from Tuesday following issues with Apple devices.   If systems are used temporarily in production, check for adverse affects on any users who used both systems. Lesson from Wednesday/Thursday following issues for a small number of users.</li> <li>Ensure errors due to trivial issues, like changes to logs or failed schema in the DB, do not cause the whole app to go down. Lesson from Monday and Friday.</li> <li>Ensure adequate logging systems are in place (these have been improved already), and that a clear process is in place for users to contact the team (use lambdafeedback@imperial.ac.uk, or support@lambdafeedback.com which will redirect there)</li> <li>Ensure important messages to users can be seen, e.g. even if they can't log in.   Keep teachers informed promptly and with transparent info. Generally this protocol was followed during this incident.</li> </ul> <p>Conclusion:</p> <ul> <li>The new login system offers significant benefits, including allowing logins with a range of systems such as Google or personally created accounts. The app is now available to essentially any users.</li> <li>The deployment involved errors which caused access issues for over an hour on more than one occasion in the same week.</li> <li>Most of the outages were preventable with improved testing</li> <li>The mitigation attempts were successful in reducing the severity of the incident</li> <li>Mitigation could have been better. New logs, lower risk tolerance, and better reversion are needed in future</li> <li>Overall the level of outage is not considered acceptable and in future should be avoided</li> </ul>"},{"location":"student/","title":"Student/User Documentation","text":""},{"location":"student/#question-structure","title":"Question structure","text":"<p>The image above shows an example question, with numbers to indicate:</p> <ol> <li>Breadcrumbs showing location</li> <li>Name of the Problem Set</li> <li>PDF version (link)</li> <li>Names of the questions in the Set, indicating which question is open</li> <li>Question number and name</li> <li>Guidance (expands on hover)</li> <li>Master content (always visible to student)</li> <li>Part selection (tabs)</li> <li>Part content (only visible when relevant part is open - (a),(b), etc.)</li> <li>Response area, where student responses are entered and feedback is given</li> <li>Feedback to the teacher (currently in flux regarding the design - 31/8/22)</li> <li>Access to content 'below the line' providing extra support.</li> </ol>"},{"location":"student/#below-the-line","title":"Below the line","text":"<ul> <li>My solutions - create your own content. Drag and drop images or type content. Use standard markdown.</li> <li>Structured tutorial - teachers use this in different ways. It is generally a way to provide scaffolding if you're struggling.</li> <li>Final Answer - warning, don't ever look at the answer before you make your own genuine attempt at answering the question.</li> <li>Worked solutions - warning, don't ever look at the solutions before you make your own attempt. If necessary, look at the first line and reveal a step at a time.</li> </ul>"},{"location":"student/answering_questions/","title":"Answering Questions","text":""},{"location":"student/answering_questions/#overview","title":"Overview","text":"<p>The main view of a question is divided into two parts. The top half contains content that is relevant to the whole question, and the bottom half contains content for each individual part. Additionally, the part content may include one or more  response areas. When you think you have answered the question, enter your answer into the response area, if it exists, and press the \"Check\" button to check your work. If you are correct, the question will be marked as \"done\". If there is no response area (e.g. for a \"show that...\" question), you can manually mark the question as done using the box at the bottom right.</p>"},{"location":"student/answering_questions/#answers-and-worked-solutions","title":"Answers and Worked Solutions","text":"<p>If you are stuck, you can view worked solutions using the \"Worked Solutions\" option on the bottom ribbon. The steps in the solution are revealed step-by-step, so you should avoid the temptation to look at the whole solution at once, and try to complete as much as possible independently. </p> <p></p> <p>You can also view the answer to each question using the \"Final Answer\" option on the bottom ribbon. This contains the  answer only, with no intermediate results or working.</p> <p>It is important that you always make your own genuine attempt to solve each problem before resorting to the final answers or the worked solutions. To help encourage this, a warning will appear if you try to access help before a question-specific time limit has elapsed.</p>"},{"location":"student/faq/","title":"Frequently Asked Questions","text":""},{"location":"student/faq/#why-i-cannot-find-the-module-i-am-looking-for","title":"Why I cannot find the module I am looking for","text":"<p>Access to each module is provided by the teacher owning the module.</p> <p>If you cannot find the module you are looking for, please contact your teacher.</p>"},{"location":"student/getting_started_student/","title":"Get started as a student using Lambda Feedback","text":""},{"location":"student/getting_started_student/#accessing-content","title":"Accessing content","text":""},{"location":"student/getting_started_student/#log-in","title":"Log in","text":"<p>Use your Imperial Microsoft account to sign in and access your modules. Once you sign in, you should see a list of the modules you are enrolled in:</p> <p>You can see the current progress of each module in this view.</p> <p></p>"},{"location":"student/getting_started_student/#select-a-module","title":"Select a module","text":"<p>Click on the module name to select it. You should now see a list of available problem sets. If none are available, your teacher may not have assigned any yet.</p> <p>You can see the current progress of each problem set in this view.</p> <p></p>"},{"location":"student/getting_started_student/#select-a-problem-set","title":"Select a problem set","text":"<p>Select the problem set you wish to work on, and you should see a list of questions on the left-hand side, with the selected question on the right. If a question has sub-parts, you can select them on the right.</p> <p></p>"},{"location":"student/getting_started_student/#accessing-the-pdf-version-of-a-problem-set","title":"Accessing the PDF version of a problem set","text":"<p>If you prefer to work on a PDF version of the problem set, you can generate a PDF by clicking the 'pdf' button underneath the problem set title.</p> <p></p>"},{"location":"student/getting_started_student/#answering-questions","title":"Answering questions","text":"<p>You can make progress on the problem by entering correct answers or clicking the 'Mark as done' button on the bottom right of each question page. This can be useful to track progress if working on the PDF version, or for questions which do not have a response box, e.g., show that questions.</p> <p>See the Answering Questions page for more help with answering questions.</p> <p></p>"},{"location":"student/getting_started_student/#using-the-workspace","title":"Using the Workspace","text":"<p>The Workspace provides you with various functionalities to assist you during your learning process: 1. #### Canvas:  A pane where you can write down your thought process and notes for the previewed question (handwriting, sticky notes &amp; text).</p> <p></p> <p></p> <p>Your edits and progress in the Workspace are saved per each Question you preview. So, you will be able to view your old edits for the Question you are currently on.</p>"},{"location":"student/getting_started_student/#chat","title":"Chat:","text":"A chat interface connecting you with helpful AI Chatbots to discuss any questions you have on the current topic you are working on."},{"location":"student/milkdown_student/","title":"Text Editing","text":"<p>The Milkdown editor is widely used in Lambda Feedback wherever rich text input is required. On the student interface, it is used to add personal solution notes, and to write comments.</p> <p>It accepts:</p> <ul> <li>Standard Markdown</li> <li>\\(\\LaTeX\\)</li> <li>Images (paste or drag and drop)</li> <li>Videos (paste a URL)</li> </ul>"},{"location":"student/milkdown_student/#latex","title":"LaTeX","text":"<p>LaTeX is a typesetting system widely used in academia to produce well-formatted documents. It is mostly used in Lambda Feedback for its capability to render complex mathematical expressions clearly and accurately. </p> <p>As an example, the following LaTeX code:  <pre><code>\\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S\n</code></pre> Produces the following output:</p> \\[ \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S \\] <p>In the Milkdown editor, anything surrounded by dollar signs (like <code>$ x^2 $</code>) will be interpreted as LaTeX.  Only the subset supported by KaTeX, which includes most common LaTeX functions, can be used.</p>"},{"location":"student/milkdown_student/#latex-equations-in-5-minutes","title":"LaTeX equations in 5 minutes","text":""},{"location":"student/milkdown_student/#numbers-and-letters","title":"Numbers and letters","text":"<p>Numbers and Latin letters can be entered as you would expect: <pre><code>1, 2, 3, 3.14159, -2.5, x, y, z\n</code></pre></p> \\[ 1, 2, 3, 3.14159, -2.5, x, y, z \\] <p>Subscripts can be written with <code>_</code> and superscripts can be written with <code>^</code>, for example in <code>x^2</code> (\\(x^2\\)) or <code>x_2</code> (\\(x_2\\)). Only the first character or command after a <code>_</code> or <code>^</code> will be taken. To subscript or superscript multiple characters, they can be  grouped in curly braces, like this: <code>V_{ab}</code> (\\(V_{ab}\\)).</p>"},{"location":"student/milkdown_student/#basic-functions","title":"Basic functions","text":"<p>Functions in LaTeX start with a backslash <code>\\</code>.</p> <p>Some common functions include Greek letters:</p> <ul> <li><code>\\pi</code> (\\(\\pi\\))</li> <li><code>\\delta</code> (\\(\\delta\\))</li> <li><code>\\Delta</code> (\\(\\Delta\\)) </li> <li>etc.</li> </ul> <p>Equalities: </p> <ul> <li><code>\\approx</code> (\\(\\approx\\))</li> <li><code>\\ne</code> (\\(\\ne\\))</li> <li><code>\\gt</code> (\\(\\gt\\))</li> <li>etc.</li> </ul> <p>Symbols/operators: - <code>\\int</code> (\\(\\int\\)) - <code>\\sum</code> (\\(\\sum\\)) - <code>\\sin</code> (\\(\\sin\\)) - <code>\\ln</code> (\\(\\ln\\)) - etc.</p>"},{"location":"student/milkdown_student/#functions-with-arguments","title":"Functions with arguments","text":"<p>Some functions take arguments. Arguments are given between curly braces <code>{}</code>. </p> <p>Some commonly functions with arguments are:</p> <ul> <li><code>\\sqrt{x}</code> (\\(\\sqrt{x}\\)). This places a square root sign around the argument.</li> <li> <p><code>\\frac{x}{y}</code> (\\(\\frac{x}{y}\\)). This command takes two arguments, and produces a fraction with the first argument in the numerator and the second on the denominator.</p> </li> <li> <p>Diacritics:</p> <ul> <li><code>\\vec{x}</code> (\\(\\vec{x}\\)).</li> <li><code>\\dot{x}</code> (\\(\\dot{x}\\)). </li> <li><code>\\hat{x}</code> (\\(\\hat{x}\\)).</li> <li>etc.</li> </ul> </li> <li> <p><code>\\mathrm{x}</code> (\\(\\mathrm{x}\\)). This formats the argument as regular, upright text, rather than italics. Should be used for units and operators (e.g. \\(\\frac{\\mathrm{d}y}{\\mathrm{d}x}\\)).</p> </li> </ul>"},{"location":"student/milkdown_student/#nesting","title":"Nesting","text":"<p>Functions can be nested arbitrarily. For example, a square root may contain a fraction, which may contain another square root: <pre><code>\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\n</code></pre></p> \\[\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\\]"},{"location":"student/milkdown_student/#going-further","title":"Going further","text":"<p>If you are unsure of the correct function to use to produce the desired result, there is a list of all supported KaTeX functions here.</p>"},{"location":"student/response_areas/","title":"Response Areas","text":""},{"location":"student/response_areas/#numerical-answers","title":"Numerical answers","text":"<p>This type of response area expects a numerical answer. Usually, a tolerance is allowed, so you will still be marked as correct if your response differs from the answer by a small amount. </p> <p></p> <p>Some questions will also require you to enter the units of the answer. In this case, any form of the same unit, with any SI prefix, should be accepted. For example, if the answer to a question is <code>10 MPa</code>, <code>0.01 GPa</code> and <code>10 MNm^-2</code> should both be marked correct. </p>"},{"location":"student/response_areas/#entering-mathematical-expressions","title":"Entering mathematical expressions","text":"<p>Entering mathematical expressions on Lambda is very similar to if you were doing it in Matlab, for example. </p>"},{"location":"student/response_areas/#examples","title":"Examples","text":"Expression Lambda Feedback input \\(x^2 - x - 2\\) <code>x^2 - x - 2</code> \\(\\sqrt{\\sin(x) + \\frac{\\pi}{2}}\\) <code>sqrt(sin(x) + pi/2)</code> $e^{\\frac{\\pi x}{2} + 1} <code>exp((pi*x)/2 + 1)</code>"},{"location":"student/response_areas/#reference","title":"Reference","text":"Operator Symbol Lambda Feedback input Addition \\(a + b\\) a+b Subtraction \\(a - b\\) a-b Multiplication \\(a \\times b\\) a*b Division \\(\\frac{a}{b}\\) a/b Exponentiation \\(a^b\\) a^b Square root \\(\\sqrt{a}\\) sqrt(a) <p>Common elementary functions such as \\(\\sin\\), \\(\\cos\\), \\(\\arcsin\\), \\(\\ln\\) etc. are also supported.</p>"},{"location":"teacher/","title":"Teacher and Content Author Documentation","text":"<p>In the 2022/23 academic year Lambda Feedback was in its alpha version. We are now in our Beta version. This means that some features are incomplete, and that the documentation is not ready yet. All teacher-users should be in direct contact with someone on the development team so that you can ask for support.</p> <p>Please provide feedback on the documentation as it develops.</p> <p>We will populate this area more soon.</p> <p>Try Getting Started ...</p>"},{"location":"teacher/guides/analytics/","title":"Analytics Guide","text":"<p>Analytics begin when a question is published. After publishing a question for the first time it becomes available to students and their usage is logged and fed back to the student and the teacher. </p>"},{"location":"teacher/guides/analytics/#analytics-history","title":"Analytics History","text":"<p>Analytics are linked to response areas. Each question can have more response areas and they can be added or removed. When a response area is removed, then it is removed only from the \"current version\" of the question (the version that the teacher is editing) and it persists on the previous version(s) of the question. It means that all submissions and analytics remain, but they are now linked to the response area which only exists on a previous version(s) of the question.</p> <p>Currently it is possible to see only analytics against the published version of the question.</p> <p>We are now working on the improvement so that it is possible to see analytics against all reponse areas (including those that exist only on previous versions of the question).</p>"},{"location":"teacher/guides/analytics/#tracking-students-response","title":"Tracking students' response","text":"<p>To improve the feedback that students receive and to better understand which areas they need help with, it is possible to check the different student responses and the frequency of each response for each response area. </p> <p>To see these statistics:</p> <ol> <li>Click on the Stats tab in teacher mode </li> <li>Then click on Explore in the top right corner of each response area.</li> </ol> <p>You can even export these statistics as csv file!</p>"},{"location":"teacher/guides/content-sets-questions/","title":"Editing questions","text":"<p>In this guide, we will walk through how to create sets and questions.</p> <ol> <li>Click on a Set in order to edit or add questions.</li> </ol> <p></p> <ol> <li>A guide to the editor:</li> </ol> <p></p> Label Name Description 1 Question Name Here we can edit the name given to the question 2 Master content Always visible. Uses a milkdown component. 3 Current part Referenes which question part we are editing 4 Part Content Here we can edit the specific content of the part (the sub question). 5 Response Area This is the means by which the student answers a given question. It is optional to include a response area. 6 Question Help Options Here we may add a Structured Tutorial, a Final Answer (\"Show Answer\"), or Worked Solutions. These buttons are also how the student will see them, hence the name of \"Show Answer\". 7 Teacher-Student View Toggle Here we may toggle between the teacher view (\"EDIT\"), and the student view (\"PREVIEW\"). This way we can edit the question, but also see a preview of how it would look to the student. 8 Edit Guidance Here we can add further details for guidance, estimated working time and skill level 9 Part option Here we can add, duplicate or (if there are more than one) delete a question part 10 Add question Here we can add create a new blank question, duplicate a question or upload a zip file containing JSON file(s) for Lambda Feedback. 11 File, Preview and Stats Travel to pages to a) File: version management/ download as JSON and delete question b) Preview: view the question as a student would see it c) Stats: View stats of student responses of the question set."},{"location":"teacher/guides/faq/","title":"Frequently Asked Questions","text":""},{"location":"teacher/guides/faq/#how-can-i-enroll-students-on-my-new-module","title":"How can I enroll students on my new Module","text":"<p>Students and users are given access to a module using their college email address (from microsoft).</p> <ol> <li>Login and navigate to your Teacher dashboard</li> <li>Select the module on which you want to enroll students</li> <li>When on the module page, click the View Students button</li> <li>Enter the enrolment page by clicking the Entroll Students  button</li> <li>Enroll students by supplying one or more student email addresses</li> </ol>"},{"location":"teacher/guides/faq/#how-can-i-move-questions-between-problem-sets","title":"How can I move questions between problem sets?","text":"<p>When creating a new question the teacher can choose to \"clone\" from an existing question. The teacher can then delete the original version.</p> <ol> <li>In the problem set you wish to move the question to, select the v symbol to the right of the Add Question button</li> <li>From the dropdown menu, select Clone From Question</li> <li>Select the title of the question you wish clone from the list that appear</li> <li>If you wish, go back and delete the question from its original location</li> </ol>"},{"location":"teacher/guides/faq/#how-can-i-share-a-link-to-a-problem-set","title":"How can I share a link to a Problem Set?","text":"<p>To share a link with students, open the Problem Set in STUDENT mode (light blue top bar), and copy the URL from the browser.</p> <p>To share a link with teachers who will access the content editor and analytics, share a link from TEACHER mode (orange top bar); students won't be able to access this link.</p>"},{"location":"teacher/guides/faq/#how-can-i-set-parameters-for-evaluation-functions","title":"How can I set parameters for evaluation functions?","text":"<p>The most common parameters will be visible uder the EVALUATE tab in the configure panel.</p> <p>If there is a parameter that is not already visible it can be set using the Advanced - raw parameters (also under the EVALUATE tab) by doing the following:</p> <p></p> <ol> <li>Hover over the list of parameters in the Advanced - raw parameters area. Click the green plus-symbol that appears.</li> </ol> <p></p> <ol> <li>Type the name of the parameter (without quotation marks).</li> </ol> <p></p> <ol> <li>Hover over the box that says <code>NULL</code> next to the newly added parameter. Click the green pen symbol that appears to the right of it.</li> </ol> <p></p> <ol> <li>Type in the desired value in box that appears. By default it will be assumed that the parameter value is a string. The webclient will infer other possible types based on the written input. If the setting should be a string, click the green checkmark to the top right, and if you want the inferred type click the green checkmark at the bottom right.</li> </ol> <p></p> <ol> <li>The parameter is now set.</li> </ol> <p></p>"},{"location":"teacher/guides/faq/#how-do-i-reorder-questions","title":"How do I reorder questions?","text":"<p>It is only possible to reorder published questions in a set. This prevents inadvertently inserting new questions between two published ones. This ensures consistency to the student when viewing a published set as existing questions will remain in an unchanged order, with new questions being added to the bottom (unless manually changed by the teacher). You can tell a question is unpublished as it will take the '1.X' numbering format</p> <p>To reorder questions:</p> <ol> <li>Publish the questions you wish to reorder using FILE &gt; SAVE AND PUBLISH (alternatively click on the PUBLISH WHOLE SET button)</li> <li>Refresh the page</li> <li>Drag and drop the questions into the new order</li> <li>Ensure the green box pops up saying: 'questions reordered successfully' - there is no need to republish the set</li> </ol>"},{"location":"teacher/guides/faq/#what-to-do-when-space-is-not-showing-in-the-pdf-generated-by-lambda-feedback","title":"What to do when <code>\\space</code> is not showing in the pdf generated by lambda feedback?","text":"<p>The Pandoc library that lambdafeedback use to generate a pdf does not support <code>\\space</code>. Alternatives that could be use to generate a space in math block is to use the tilde symbol <code>~</code> or <code>\\,</code> for thinner spacing.</p>"},{"location":"teacher/guides/faq/#what-to-do-if-the-pdf-is-not-compiling-my-inline-math-equation","title":"What to do if the pdf is not compiling my inline math equation?","text":"<p>Please check if there is an additional space at the start or a the end of the equation. This is usually the cause for inline math blocks not compiling.</p> <p>Sometimes if you are copy-pasting text into equations you may end up with certain characters that look normal but actually have different ASCII codes than what you intended. This may also cause a PDF not to compile.</p>"},{"location":"teacher/guides/faq/#how-can-i-have-the-same-font-for-the-unit-and-for-the-number-in-the-math-block","title":"How can I have the same font for the unit and for the number in the math block?","text":"<p>You can use the code <code>\\mathrm{}</code> or <code>{\\rm}</code>. Both code will give you your units in serifed Times New Roman, which is the same font as the number in the math block when compiled.</p>"},{"location":"teacher/guides/faq/#complex-numbers-notation","title":"Complex Numbers Notation","text":"<p>If you want to use <code>I</code> for the imaginary constant, add the parameter <code>complexNumbers</code> to \"advanced - raw parameters\" by clicking the green (+). Type in <code>complexNumbers</code> and press enter. Click the green edit button, type in \"True\" and a pop-up <code>bool - true</code> will appear. Click the green tick.</p> <p></p> <p>You can denote <code>i</code> and <code>j</code> as <code>I</code> by using the input symbols below. </p> <p></p> <p>Furthermore, the system can equate <code>exp(Ix)</code> to <code>cos(x)+Isin(x)</code>.</p>"},{"location":"teacher/guides/gettingstarted/","title":"Get started as a teacher using Lambda Feedback","text":""},{"location":"teacher/guides/gettingstarted/#access-a-module","title":"Access a module","text":"<p>Use your Imperial Microsoft account to sign in and access your modules. By default you are logged in as a student and the interface will be blue. If you have teacher priviliges then you will see a teacher button at the top.</p> <p></p> <p>To enter teacher mode, click on the Teacher button, and the colour of the interface will change to orange. This is where you are able to access all your modules, as well as upload and edit problem sets.</p> <p>As of 07/2023, new modules can only be added to Lambda Feedback by administrators. Please speak to an admin if you wish for your module to be added to the website.</p> <p></p> <p>To find the module you want, you can sort ASCENDING as per the image below: Image: quick sort (left) or filtering (right)</p> <p>As of 31/8/22 the filtering/sorting only works on the content visible on the current page (other pages are ignored). We aim to fix this by sorting at the backend.</p> <p>Select the module you wish to edit. </p>"},{"location":"teacher/guides/gettingstarted/#create-a-new-problem-set","title":"Create a new problem set","text":"<p>Click on your module and then click on \"content\" (upper left-hand corner). </p> <p>Create a new set by pressing the button seen below and this will automatically appear with a default name which you can edit by clicking 'edit set metadata': </p> <p>To edit the content, click on the set name. This will open the Set in a 'WYSIWYG' editor. The first question is automatically created with a default name.</p> <p>The question structure is described here.</p>"},{"location":"teacher/guides/gettingstarted/#below-the-line","title":"Below the line","text":"<p>Below the main question content you can provide high quality support material for students.</p> <p></p> <p>A student guide is here and teachers use the content as follows:</p> <ul> <li>Structured tutorial is a canvas to provide scaffolding to students struggling with the question.</li> <li>Final answer is self explanatory.</li> <li>Worked solutions provides detailed, step-by-step solutions.</li> </ul> <p>All content below the line uses milkdown functionality. Worked solutions can be branched. Future developments will add branching and response areas to structured tutorials.</p> <p>It is not necessary to include all three methods of help, if only one of the tabs is filled then only that one button will be included in the published student version.</p> <p>For general terminology, see here.</p> <p>To see further details on how to edit your questions, see here.</p>"},{"location":"teacher/guides/gettingstarted/#enrolling-students","title":"Enrolling students","text":"<p>In TEACHER mode, open your module home page and click 'VIEW STUDENTS' then 'ENROL STUDENTS'</p> <p> </p> <p>Enter a comma-separated list of email addresses. Press 'Enter' to confirm the addresses, and then 'SUBMIT' to enrol the students</p>"},{"location":"teacher/guides/gettingstarted/#imperial-college-london-email-addresses","title":"Imperial College London email addresses","text":"<p>You must use the long form email address:</p>"},{"location":"teacher/guides/gettingstarted/#valid","title":"Valid:","text":"<p>first.nameYY@imperial.ac.uk (student) j.doe@imperial.ac.uk (staff) first.name@imperial.ac.uk (staff)</p>"},{"location":"teacher/guides/gettingstarted/#invalid","title":"Invalid","text":"<p>abc123@ic.ac.uk abc123@imperial.ac.uk user@ic.ac.uk user@imperial.ac.uk first.nameYY@ic.ac.uk</p> <p>The reason for the above is because we use Azure Active Directory - i.e. Microsoft - to authorise users.</p>"},{"location":"teacher/guides/good-practice/","title":"Good practice","text":""},{"location":"teacher/guides/good-practice/#romanised-operators","title":"Romanised operators","text":"<p>Use romanised operators such as \\(\\sin\\), \\(\\frac{\\mathrm{d}}{\\mathrm{d}x}\\) instead of \\(sin\\), \\(\\frac{d}{dx}\\), etc.</p>"},{"location":"teacher/guides/good-practice/#use-empty-lines","title":"Use empty lines","text":"<p>Using empty lines can improve the readability and neatness of your content. Empty lines are often useful before and after an equation, and between paragraphs of text. An empty line in markdown requires two spaces on the line, otherwise the line is ignored.</p>"},{"location":"teacher/guides/good-practice/#space-between-numbers-and-units","title":"Space between numbers and units","text":"<p>Put appropriate space between a number and its unit, such as <code>5 m</code> or <code>3 kg</code>, according to the SI conventions.</p>"},{"location":"teacher/guides/good-practice/#romanise-units-and-check-their-case","title":"Romanise units and check their case","text":"<p>Use romanised units such as \\(\\text{m}\\), \\(\\text{kN}\\) instead of \\(m\\), \\(kN\\). Ensure that the case of the unit is correct.</p>"},{"location":"teacher/guides/good-practice/#add-tests-to-response-areas","title":"Add tests to response areas","text":"<p>In a response area, press <code>configure</code> then <code>tests</code>.</p> <p>Tests allow you to enter potential student responses, define whether they are correct or not, then run the evaluation function on those student responses. This allows you to quickly test whether or not the evaluation function works as expected.</p>"},{"location":"teacher/guides/good-practice/#save-and-publish-as-you-go","title":"Save and publish as you go","text":"<p>Saving and publishing work regularly is recommended to prevent accidental data loss.</p>"},{"location":"teacher/guides/good-practice/#use-branching-when-relevant","title":"Use branching when relevant","text":"<p>Branching is a feature for <code>worked solutions</code>. It allows you to have different solution pathways Usage examples:</p> <ul> <li>When a question can be solved via multiple different methods, branching can be used for each method.</li> <li>When a question has multiple parts, where each part involves substitution of different values, branching can be used for each part.</li> </ul> <p></p>"},{"location":"teacher/guides/good-practice/#use-pre-response-area-text-to-be-clear-what-should-be-entered","title":"Use pre-response area text to be clear what should be entered","text":"<p>Pre-response area text is found under <code>configure</code> - <code>INPUT</code> in the evaluation function.</p> <p>You can use LaTeX in the pre-response area text.</p> <p></p>"},{"location":"teacher/guides/good-practice/#use-dfrac-for-bigger-fractions-when-needed","title":"Use <code>\\dfrac</code> for bigger fractions when needed.","text":"<p>Use <code>$\\dfrac{numerator}{denominator}$</code> for bigger fractions when you need to display them more clearly or emphasize them. For example, <code>$\\dfrac{3}{4}$</code> will produce a bigger fraction than <code>$\\frac{3}{4}$</code>. Alternatively, you can use <code>$\\displaystyle$</code> at the start of an inline equation to render everything afterwards full-size (as in display maths mode), this is especially helpful for integrals.</p>"},{"location":"teacher/guides/good-practice/#use-small-when-smaller-fonts-or-fractions-are-needed","title":"Use <code>\\small</code> when smaller fonts or fractions are needed","text":"<p>Use <code>$\\small{text}$</code> when you need to display smaller fonts or fractions in your LaTeX expressions. For example, <code>$\\small{\\frac{1}{2}}$</code> will produce a smaller fraction than <code>$\\frac{1}{2}$</code>.</p>"},{"location":"teacher/guides/good-practice/#use-audio-clips","title":"Use audio clips","text":"<p>Just drag + drop an audio file into the milkdown editor.</p>"},{"location":"teacher/guides/good-practice/#issue-with-input-symbols","title":"Issue with input symbols","text":"<p>For the <code>code</code> of input symbols in the response areas, the system cannot accept brackets (i.e <code>dot(x)</code> for \\(\\dot{x}\\)) and response must be formatted in different ways (i.e <code>dot_x</code>). </p> <p><code>a_b</code> will render as \\(a_b\\) without adding an input symbol, but note that <code>dot_x</code> overrides the <code>_</code> input (in this example, will render as \\(\\dot{x}\\) instead of \\(dot_x\\))</p>"},{"location":"teacher/guides/good-practice/#use-live-preview-and-permit-all-types-of-input","title":"Use live preview and permit all types of input","text":"<p>Live preview and input types are found in an evaluation function under <code>configure</code> - <code>INPUT</code>.</p> <p>Live preview instantly renders a student's input. This is very useful for long/complicated equations, as it allows students to ensure their input is correct.</p> <p></p>"},{"location":"teacher/guides/good-practice/#latex-help","title":"Latex help","text":"<ol> <li>Use \\begin{array} to generate compact table i.e  <pre><code>\\begin{array}{|c|c|}\n\\hline\n\\theta_{2,0} &amp; \\theta_{1,L}\\\\\n\\hline\n-6700 &amp; 130.5641\\\\\n\\hline\n-6600 &amp; 161.6086\\\\\n\\hline\n\\end{array}\n</code></pre></li> </ol> <ol> <li>Use <code>\\begin{aligned}</code> to keep your working formatted nicely <pre><code>\\begin{array}{ll}\nM_{d e f} &amp;=\\dfrac{1}{2}(M+M^T)\\\\\n&amp; =\\dfrac{1}{2} \\begin{pmatrix} 4 &amp; 14\\\\ -6 &amp; -11 \\end{pmatrix}+\\begin{pmatrix} 4 &amp; -6\\\\ 14 &amp; -11 \\end{pmatrix}\\\\\n&amp; =\\begin{pmatrix} 4 &amp; 4\\\\ 4 &amp; -11 \\end{pmatrix}\n\\end{array}\n</code></pre></li> </ol> <ol> <li>Use <code>\\left</code> and <code>\\right</code> for equations with multiple brackets</li> </ol> <pre><code>f(x)=\\left (\\frac{(\\cos (x) -x) + i(\\sin (x) - x)}{wi} \\right)\n</code></pre> <p>This also works for <code>[ ]</code> and <code>\\{ \\}</code></p> <ol> <li>Use <code>\\sin</code>, <code>\\cos</code> etc... if you are too lazy to write out <code>\\text{sin }</code> everytime in equation mode.</li> </ol>"},{"location":"teacher/guides/milkdown/","title":"The milkdown editor","text":"<p>The milkdown editor is widely used in Lambda Feedback. It accepts:</p> <ul> <li>standard markdown</li> <li>\\(\\LaTeX\\) (delimited by $ and limited to KaTeX functionality)</li> <li>images (paste or drag and drop)</li> <li>videos (paste a URL)</li> </ul>"},{"location":"teacher/guides/milkdown/#common-needs-in-milkdown","title":"Common needs in milkdown","text":"<p>Here's a walkthrough to create some basic content:</p>"},{"location":"teacher/guides/milkdown/#empty-lines","title":"Empty lines","text":"<p>Two blank spaces in a line will ensure it persists (as in standard markdown).</p>"},{"location":"teacher/guides/milkdown/#inline-maths","title":"Inline maths","text":"<p>Use the <code>$</code> sign to delimited inline maths. For example type the following:</p> <p><code>This is inline maths, $\\alpha&lt;0$, and it is useful</code></p> <p></p>"},{"location":"teacher/guides/milkdown/#equation-mode","title":"Equation mode","text":"<p>Start a blank line with <code>$$</code> then press the space bar. This will introduce an equation editor. Type raw \\(\\LaTeX\\) into the shaded part and see the live preview in the lower part. *Press <code>ctrl+enter</code> (Mac: <code>cmd+enter</code>) to exit the equation editing box.</p> <p>For example, type the following after typing <code>$$ [space]</code> into a fresh line:</p> <p><code>f(x) = \\int_{-\\infty}^\\infty \\hat{f}(\\xi)\\,e^{2 \\pi i \\xi x} \\,\\mathrm{d}\\xi</code></p> <p></p>"},{"location":"teacher/guides/milkdown/#steps-in-worked-solutions","title":"Steps in worked solutions","text":"<p>If you begin a fresh line with <code>---</code> (three dashes) then a horizontal rule appears. Alternatively click the button on the toolbar to insert a horizontal rule.</p> <p></p> <p>If you are editing a worked solution, then Lambda Feedback will split the worked solution into steps according to the location of horizontal rules. You can delete and add the rules and the solution steps will update.</p> <p>For example:</p> <p><code>This is the first step of the solution - which is a good hint towards solving</code> <code>---</code> <code>This is a second step, which makes it more obvious</code> <code>---</code> <code>Finally we reach the solution</code></p> <p>When viewing the worked solutions, this is how it looks:</p> <p></p> <p>This is the process to create the solution steps:</p> <p></p>"},{"location":"teacher/guides/milkdown/#images","title":"Images","text":"<p>You can add images with drag-and-drop or copy-and-paste. Currently (July '24) there is no way to resize images other than lowering the resolution - see below. </p> <p></p>"},{"location":"teacher/guides/question-export-import/","title":"Question export and import","text":""},{"location":"teacher/guides/question-export-import/#export-a-question","title":"Export a question","text":"<p>Under the File menu, select the Export as JSON option:</p> <p></p> <p>The question and images (if any) will be downloaded into your download folder.</p>"},{"location":"teacher/guides/question-export-import/#import-a-question","title":"Import a question","text":"<p>Click Add question menu and select Import questions from file option:</p> <p></p> <p>The file explorer opens. Select the zip file containing the question and click open. The question will be added as the last question.</p> <p>The zip file must contain question data in a valid JSON format. The best way to obtain a valid JSON format is to export a question, unzip the download file and open the JSON file. If the question contains media, they must be in the media folder inside of the zip file.</p>"},{"location":"teacher/guides/question-export-import/#import-more-than-1-question","title":"Import more than 1 question","text":"<p>The zip file can contain more than one question. Each of the questions must be in the JSON file and in the correct format. All media must be in the media folder.</p> <p>It is possible to e.g. export 2 questions, then unzip the exported zip files and then zip both questions and their medias into one zip file and then import the one zip file. Here is an example of a folder containing 2 questions and their medias:</p> <p></p> <p>The name of the folder and names of json files are not important. However, the name of media files must correspond with the names used in the json files when referring the media.</p>"},{"location":"teacher/guides/guidance/guidance-time-suggestion/","title":"Obtaining Guidance Time","text":"<p>In this guide, we will walk through how to use the guidance time suggestion feature.</p> <ol> <li> <p>Fill in all the question's attributes as much as possible. i.e, the question's text, the worked solution, skill level etc., The more information is filled in, the more accurate the suggested guidance time will be.</p> </li> <li> <p>Click on the \"Suggest\" button in the guidance configuration tab after you have filled in all the question's attributes.</p> </li> </ol> <p></p>"},{"location":"teacher/reference/access_control/","title":"Access control","text":""},{"location":"teacher/reference/access_control/#modules","title":"Modules","text":"<p>Module access for students is controlled by enrolling student users. More details to be added here.</p>"},{"location":"teacher/reference/access_control/#sets","title":"Sets","text":"<p>Set access is granted to all users enrolled on a module, but the Set can be hidden by the teacher. Two methods can be used to hide a Set:</p> <ol> <li>Start and end dates (both optional) can be created in the Set Metadata</li> <li>The Set can me manually hidden, which overrides the above settings.</li> </ol>"},{"location":"teacher/reference/access_control/#support-material-within-questions","title":"Support material within questions","text":"<p>The following types of support materials are available to students in the <code>help</code> section:</p> <ul> <li>Sructured tutorial</li> <li>Final answer</li> <li>Worked solutions</li> </ul> <p>Two methods can be used to hide support material:</p>"},{"location":"teacher/reference/access_control/#configuring-student-access-at-the-set-level","title":"Configuring student access at the set level","text":"<p>Open the Edit Set Metadata page by clicking on the Edit Set Metadata button in the list of sets:</p> <p></p> <p>The page contains the Student access to support material section:</p> <p></p> <p>Access to each support material type can be set to one of the following options:</p>"},{"location":"teacher/reference/access_control/#available","title":"Available","text":"<p>Students can open this support material type without any restrictions.</p> <p>This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).</p>"},{"location":"teacher/reference/access_control/#available-with-warnings","title":"Available with warnings","text":"<p>A warning window appears if the studen opens the content before the recommended time.</p> <p>The recommended time is the Minimum time estimate (mins) which can be set on the question Guidance page:</p> <p></p> <p>However, the option will be changed to Available, if any of the following is true:</p> <ul> <li>The student has downloaded the PDF</li> <li>The part is marked as done</li> <li>There is no minimum time estimate set for the question</li> <li>The time now minus the time the student first accessed the question is more than the minimum time estimate</li> </ul> <p>This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).</p>"},{"location":"teacher/reference/access_control/#unavailable","title":"Unavailable","text":"<p>Students cannot open any support material for any question in the set.</p> <p>This is valid for all questions in the set, even those for which the support material access is set to Available at the question level (see below).</p>"},{"location":"teacher/reference/access_control/#configuring-student-access-at-the-question-level","title":"Configuring student access at the question level","text":"<p>The support material access configuration at the question level is located on the File tab:</p> <p></p> <p>All support material is available by default, it can be changed:</p> <ul> <li>If the switch is off, then the support material is available</li> <li>If the switch is on, then the support material is unavailable</li> </ul>"},{"location":"teacher/reference/access_control/#summary-overview","title":"Summary overview","text":"Set level setting Question level setting Result (using Final answer as an example) Description Comment Unavailable N/A The Final answer is disabled The setting at the question level is ignored Available Unavailable The Final answer is disabled Available with warnings Unavailable The Final answer is disabled The same result as above Available with warnings Available When the Final answer is clicked, a warning message appears Additional conditions must be met:  <li>PDF not downloaded</li> <li>Part not marked as done</li><li>The minimum time estimate is set for the question</li> <li>The time now minus the time the student first accessed the question is more than the minimum time estimate</li>If any of them is not met, then the support material will be available with no warnings."},{"location":"teacher/reference/latex_functionality/","title":"Latex functionality","text":"<p>Lambda feedback uses one content source to serve two outputs: web and PDF. Each output has different requirements, and content must meet both requirements in order to serve both outputs.</p>"},{"location":"teacher/reference/latex_functionality/#content-formatting","title":"Content formatting","text":"<p>All content is formatted in markdown. Headings, font style, lists, tables, images, \\(\\LaTeX\\), can all be created using standard markdown.</p> <p>Special attention is required when formatting \\(\\LaTeX\\) which, although it is formatted using standard markdown (i.e. delimited by the <code>$</code> for 'inline formulas', and <code>$$</code> for an equation environment), must use a subset of \\(\\LaTeX\\) in order to compile for both outputs. This sometimes requires a compromise by the author.</p>"},{"location":"teacher/reference/latex_functionality/#the-milkdown-editor","title":"The milkdown editor","text":"<p>All content in Lambda Feedback is stored as markdown (ASCII content), however it is always input/edited using the milkdown editor. This editor has the advantage of providing live interactive previews of content, including \\(\\LaTeX\\) via katex.</p>"},{"location":"teacher/reference/latex_functionality/#web-requirements-katex","title":"Web requirements: katex","text":"<p>\\(\\LaTeX\\) content is rendered in the web browser using the katex Javascript library. The katex home page has an intereactive input where content is rendered and can be checked for validity. The documentation lists which functions are supported.</p> <p>Katex is a subset of \\(\\LaTeX\\). Therefore some functions that work in \\(\\LaTeX\\) do not work in katex and won't render on the web. For example, the <code>tikz</code> package - which is a complex graphics package - is not supported by katex. Unsupported packages have knock-on effects, for example while the <code>\\cancel{}</code> function is supported, <code>\\cancelto{}{}</code> is not because it relies on <code>tikz</code>.</p> <p>Ensuring that content renders correctly on the web is straightforward because the editor gives a live preview - and will indicate when errors occur.</p>"},{"location":"teacher/reference/latex_functionality/#pdf-requirements-pandoc-and-pdflatexxetex","title":"PDF requirements: pandoc and PDFlatex/XeTeX","text":"<p>When a Problem Set is saved in the editor, the PDF is automatically compiled by sending the markdown content to Pandoc, which internally uses \\(\\LaTeX\\) (either PDFlatex of xelatex - depending on the settings within the Lambda Feedback stack) to render a PDF.</p> <p>Problems can occur with PDF compilation even if the web rendering is successful, because it uses different libraries to the web browser. If the PDF fails to compile, a red error bar will appear and provide the location of the error within the Problem Set (identifying which question), and the error that Pandoc returned.</p> <p>One key limitation of the Lambda Feedback system is that it uses markdown content, so cannot produce all the richness of a full \\(\\LaTeX\\) document (and the AMS math library). All equation environments in markdown are signalled by the <code>$$</code> delimiter which is equivalent to <code>\\begin{equation*}</code>. This rules out using alternative primary environments, such as <code>align</code>, <code>gather</code>, <code>multiline</code>, <code>alignat</code>, <code>falign</code>.</p> <p>You can use subordinate environments within an equation environment, for example the following is valid:</p> <pre><code>$$\n\\begin{aligned}\na &amp; b\\\\\nc &amp; d\n\\end{aligned}\n$$\n</code></pre> <p>Here the suffix <code>-ed</code> in <code>aligned</code> implies a subordinate environment; likewise <code>gathered</code>, <code>alignedat</code> etc. are all valid within an equation environment.</p>"},{"location":"teacher/reference/latex_functionality/#warning-no-blank-lines-allowed-in-aligned-subordinate-environment","title":"Warning: No blank lines allowed in <code>aligned</code> subordinate environment","text":"<p>If a blank line is present within a subordinate environment <code>\\begin{aligned}</code> then Pandoc will fail to compile the PDF. For example:</p> <p>```Faulty code example: $$ \\begin{aligned} a &amp; b\\ c &amp; d</p> <p>\\end{aligned} $$ ```</p> <p>The above will fail to compile the PDF. But removing the blank line will solve the problem.</p> <p>For further reading search for the AMS math package and related literature.</p>"},{"location":"teacher/reference/latex_functionality/#numbering-equations","title":"Numbering equations","text":"<p>Equation numbering is problematic and our advice is to use manual numbering. Automatic numbering is possible, for example using <code>\\begin{equation}</code> within a <code>$$</code> environment. Note that sometimes the equation numbers continue counting from one environment to the next, while at other times they don't. You cannot use <code>\\ref</code> to refer to automatic equation numbers.</p> <p>An alternative approach is to use an unnumbered aligned environment, and to add an extra column with the equation number in (e.g. <code>&amp;(2)</code>).</p> <p>For some ad hoc good practice tips, see good practice.</p>"},{"location":"teacher/reference/pdf_generation/","title":"PDF Generation","text":"<p>Lambda Feedback uses a single source to render content both in the browser and by PDF. The browser view uses katex to render LaTeX, which limits the scope of LaTeX that can be used. katex doesn't use traditional LaTeX packages, but emulates many of the popular packages: Package Emulation</p> <p>When a question is published by a teacher, a PDF copy is also generated. The PDF compilation process uses <code>xelatex</code> and the Latex Template is public. The template installs relevant packages, and a list of packages is compiled below. This list will be updated as we receive more requirements from users.</p>"},{"location":"teacher/reference/pdf_generation/#supported-packages-when-generating-a-pdf","title":"Supported packages when generating a PDF","text":"Package Description amsmath Provides essential mathematical features like aligned equations, matrices, and advanced math functions; fundamental for most LaTeX math. amssymb Adds extra mathematical symbols beyond the base LaTeX set. babel Enables multilingual support with proper hyphenation and typographical conventions; allows switching languages within a document. biblatex Manages bibliographies with advanced features and customization; more flexible than traditional packages like <code>natbib</code>. bidi Enables bidirectional text support for mixing LTR and RTL scripts like Arabic and Hebrew. booktabs Provides commands for professional-looking tables with proper spacing and design; discourages vertical rules. bracket Offers commands for properly sizing and aligning brackets in math environments. cancel Draws slashes through math expressions to indicate cancellation; useful for derivations. eurosym Adds the Euro (\u20ac) symbol with options for appearance to integrate with fonts. fixltx2e Fixes bugs and improves LaTeX2e; useful for older distributions (deprecated in recent versions). fancyvrb Enhances verbatim text with customization; useful for code listings with line numbers and styling. fontenc Specifies font encoding; allows use of comprehensive encodings like T1 for accented characters. fontspec Allows use of system fonts (OpenType, TrueType) with XeLaTeX/LuaLaTeX; offers advanced font selection. geometry Simplifies setting page dimensions, margins, and layout parameters. graphicx Includes and manipulates images in documents. grffile Allows inclusion of graphics with filenames containing multiple dots or spaces. hyperref Creates hyperlinks in documents; makes references, citations, and TOC entries clickable. ifxetex Checks if document is compiled with XeTeX; useful for engine-specific configurations. ifluatex Checks if document is compiled with LuaTeX; enables engine-specific configurations. inputenc Specifies input encoding (e.g., UTF-8) for source files; allows direct typing of accented characters. listings Includes and formats source code with syntax highlighting and customization. longtable Creates tables that span multiple pages with automatic breaks and repeated headers. lmodern Provides the Latin Modern font family; improved version of default fonts with more characters. mathspec Uses OpenType fonts for math in XeLaTeX/LuaLaTeX; matches math fonts with text fonts. microtype Improves typography with microtypographic extensions like character protrusion and font expansion. natbib Advanced citation management; supports various styles and integrates with BibTeX. parskip Adds vertical space between paragraphs and removes indentation. pifont Provides access to Dingbat fonts for symbols like checkmarks and crosses. polyglossia Multilingual support for XeLaTeX/LuaLaTeX; provides language-specific typographical rules. setspace Adjusts line spacing (single, one-and-a-half, double) in documents. ulem Adds advanced underlining and strikethrough styles; allows underlines to break at line ends. upquote Ensures straight quotes in code listings; prevents conversion to curly quotes. xcolor Adds color to text and math expressions. xeCJK Typesets Chinese, Japanese, and Korean (CJK) characters in XeLaTeX; handles fonts, spacing, and line breaking."},{"location":"teacher/reference/evaluation_functions/","title":"Evaluation Functions","text":"<p>Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containerized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. </p>"},{"location":"teacher/reference/response_area_components/","title":"Response Area Components","text":"<p>These are what the user interacts with on the front-end. They check an input given by the student, and provide feedback. As React components, they admit a certain number of parameters which are described in this section.</p> <p></p>"},{"location":"teacher/reference/response_area_components/#response-area-creation","title":"Response Area creation","text":"<p>Response areas are added to the question field, and are configured for each question in the set.</p>"},{"location":"teacher/reference/response_area_components/#add-response-area","title":"Add Response Area","text":"<p>The user can add any number of response areas to a question part. These may be separated by text fields. If desired, adding a double space in the field will space out response areas.</p>"},{"location":"teacher/reference/response_area_components/#duplicate","title":"Duplicate","text":"<p>The user can duplicate the response area within the same part using the duplicate icon. When a response area is duplicated, data entered in the teacher edit mode is copied (such as input type, evaluation function, input symbols, pre and post text, answer, tests, cases, ...). Data entered in the teacher preview mode (such as comments) and student mode (comments, flags, likes, student solutions, ...) is not copied.</p>"},{"location":"teacher/reference/response_area_components/#reorder","title":"Reorder","text":"<p>It is possible to reorder response areas within the part using the \"drag and drop\" feature. It works in the same way a reordering of parts:</p> <p>Move your mouse cursor over the response are you want to move. Then, press and hold down the left mouse button, move the object to the location you desire, and release the mouse button to set it down.</p>"},{"location":"teacher/reference/response_area_components/#delete-response-area","title":"Delete Response Area","text":"<p>The user can delete a response area without any restrictions. A popup message appears to confirm the deletion.</p> <p>However, it is important to remember that there might be submissions against the response area you are trying to delete. If this is the case, the popup message will contain the relevant information.</p> <p>See more information about analytics against deleted response areas in Analytics</p>"},{"location":"teacher/reference/response_area_components/#response-area-configuration","title":"Response Area configuration","text":"<p>The user can configure a response area using the configure button:  which opens the 'Response Area Panel'. The panel follows a workflow designed around the way a teacher thinks:</p> <ul> <li>Input</li> <li>Evaluation</li> <li>Feedback</li> <li>Tests</li> </ul> <p>Each stage is in a separate tab. Teachers are recommended to be mindful of this process when creating a response area.</p>"},{"location":"teacher/reference/response_area_components/#input","title":"Input","text":"<ul> <li>Select an input style for the student by scrolling or filtering. These consist of the following:</li> <li>Matrix</li> <li>Number</li> <li>Boolean (True/false)</li> <li>Text (Suitable for short text answers)</li> <li>Table</li> <li>Multiple-choice</li> <li>Expression (Gives a preview for the typed symbolic expression)</li> <li>Numeric units (Two fields, allowing for units to be assessed)</li> <li>Code</li> <li>Essay (Suitable for long text answers)</li> <li>Each field has suitable evaluation functions. For example, a simple numerical answer is best suited to number, as this supports isSimilar, while assessing an equation is best suited to the expression foeld, as this supports compareExpressions.</li> <li>Each input field may be configured with a series of options:</li> <li>Enable live preview. Default TRUE for the EXPRESSION input type, to validate student input before submitting for feedback.</li> <li>Display input symbols. Default FALSE. When TRUE, the symbols and associated shortcut codes that may be required for a problem are displayed beneath the input field. These are configured in the 'Evaluate' tab. </li> <li>Include in PDF. Default FALSE except for MCQ. Only affects the PDF version. Includes pre/post response text in the PDF, with a blank space between.</li> <li>Pre/post response text (optional). To clarify to students what to input in the response area. Accepts plain text, including single-dollar-delimited latex. E.g. <code>Estimate $f(x)=$</code> is acceptable. When using fractions in this field, use <code>$\\dfrac{}{}$</code> as this is more legible.</li> <li>Answer. Enter a reference answer. This will typically be the absolute solution to a problem. When requesting a symbolic answer, it must be given in terms of the chosen symbol shortcuts.  Configure the answer where relevant (e.g. number of rows and columns).</li> <li>Response Area Preview: for teachers to verify the configuration</li> <li> <p>EVALUATE: configure how student expressions are evaluated. This is a 'no code' parametric configuration. Settings will be upgraded as the system improves.</p> </li> <li> <p>Evaluation Function - select an evaluation function from the list. For example:</p> <ul> <li>isSimilar will perform a basic numerical comparison between the reference answer and         student input, with a configurable level of absolute and relative uncertainty.</li> <li>compareExpressions is typically used where a symbolic answer is requested.</li> </ul> </li> <li>Parameters - configure as provided, and add new parameters as required. Details depend on the Evaluation Function.</li> <li> <p>Input symbols - define a dictionary of symbols and their equivalent in code form. This essentially associates a LaTeX-rendered symbol with a machine-readable variable label, with the LaTeX render returned to the student through the preview. These symbols may also be hiddent to students. All inputs are plain text. For example, the symbol <code>$f(x)$</code> may have code <code>fx</code> and alternatives <code>f_x</code>, <code>f(x)</code>, <code>f</code>. This dictionary will be provided to the evaluation function, even if the teacher has not displayed it to the student. This allow teachers to accept several alternative symbols, such as those with different cases or conventional expressions. The configuration of input symbols is a very important part of providing high quality feedback. Note that the 'visibility' Boolean applies if input symbols are displayed to students, otherwise it is irrelevant. It allows Teachers to communicate some symbols to students, while keeping others hidden to the student but visible to the evaluation function.</p> </li> <li> <p>FEEDBACK: add 'cases'. Each case is an alternative reference answer, with customised parameters, so that multiple cases can be dealt with independently. Cases can be used to capture multiple correct approaches that are not equivalent. Cases can also be used to identify common incorrect approaches and to provide customised feedback. The FEEDBACK tab is typically populated after students start using the system, and when data is available to point to common expressions. Configuring a case works similarly to setting up the default answer in the INPUT tab, with added options for changing the colour of the given feedback, give custom feedback and toggling whether the case answer should be considered correct or not. Adding custom feedback will overwrite the feedback produced by the evaluation function. When a response is submitted, it is evaluated for all cases and feedback for the first case that matches will be displayed. When determining which matched case is first, the default answer described in the INPUT tab will take precedence over all other cases, otherwise the feedback for the matched case with the lowest number will be displayed (i.e. the answer given in the INPUT tab can be considered to be Case 0).  Feedback fields also support LaTeX equations in both <code>$f(x)$</code> and <code>$$f(x)$$</code> formats, and Markdown inputs such as line breaks. Make sure to follow good typesetting practice in this field.</p> </li> </ul> <p></p> <ul> <li>TESTS: tests provide a systematic way to log what behaviour the teacher expects. It provides a useful record and an efficient way to retest the bevhaiour of a response area over time (e.g. as evaluation functions evolve, or as the subject matter itself changes).</li> </ul>"},{"location":"teacher/reference/response_area_components/#restrictions-on-changes-the-input-type","title":"Restrictions on changes: the input type","text":"<p>It is possible to change the input type (e.g. from Text to Number) without any restrictions until the response area is saved (with or without publishing) to students.</p> <p>After the response area is saved, it is still possible to change the input type, but it will result into replacing the response area by a new one. The previous response area will still exist, but only on the previous version of the question. When replacing the response area, all response area content data (those entered by teachers including tutorials, final answer and worked solutions) are copied, but any existing response area event data (student answers, click events and statistics) will remain linked only to the previous response area.</p> <p>Student answers, click events and statisticsthose data are never lost by deleting a response area, they are always preserved. Once a question is saved (with or without publishing), then any new changes are saved into a new (draft) version of the question. So, if e.g. a response area is deleted after a question was published, then it is deleted from the draft version only. And if this draft version is later published, then the previously published version is preserved (and with it the \"deleted\" response area and linked submissions).</p> <p>The reason why the input type change is restricted is to preserve high quality data analytics as explained in the examples below.</p>"},{"location":"teacher/reference/response_area_components/#an-example-1-changing-input-type-on-published-response-area","title":"An Example 1 - changing input type on PUBLISHED response area","text":""},{"location":"teacher/reference/response_area_components/#first-part","title":"FIRST Part","text":"<ul> <li>The teacher creates a new question with Response Areas RA1 and RA2. -&gt; The version of the question is QV1 with status DRAFT.</li> <li>The teacher is making changes (including the change of the input type if needed). -&gt; The changes are being saved into QV1 with no restrictions</li> <li>The teacher performs PUBLISH action (let's assume RA1 and RA2 input types are both Number for this example). -&gt; The QV1 version status is changed to PUBLISHED</li> <li>Students are submitting their answers -&gt; submissions are recorder against Response Area RA1 and RA2 (in the single Number format for both response areas)</li> <li>The teacher clicks on a question to edit it -&gt; with the first click a new question version QV2 is created with status DRAFT</li> <li>The teacher makes following changes (which are being recorded against QV2):<ul> <li>In RA1: adds a new Input Symbol -&gt; the change is recorded against RA1</li> <li>In RA2: the teacher unlocks and changes the input type -&gt; RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example)</li> </ul> </li> <li>The teacher performs PUBLISH action -&gt; the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table)</li> <li>Students are submitting their answers -&gt; submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format)</li> </ul> <p>=&gt; No submissions are lost. The original submissions (in the Number format) are linked to the RA2, which is preserved on the question version QV1. New submissions (in the table format) are linked to the RA3 which is recorded on the question version QV2.</p> <p>Please note: All statistics and submissions are currently displayed against the published question version only. So, though the submissions against RA2 are preserved, it is not currently possible to see them. We are working on the improvement to make this possible.</p>"},{"location":"teacher/reference/response_area_components/#second-part-this-is-an-extension-of-the-first-part","title":"SECOND Part - this is an extension of the FIRST Part","text":"<ul> <li>The teacher decides to REVERT (the question created in the FIRST Part) to the question version QV1 -&gt; a new question version QV3 is created and the content of the QV1 is copied to QV3 -&gt; QV3 is DRAFT version which contains RA1 (input type Number) and RA2 (input type Number)</li> <li>The teacher performs PUBLISH action -&gt; the QV3 is published and the teacher can now see submissions against RA1 and RA2, but he cannot see anymore submissions against RA3 (these are preserved against the QV2 version)</li> </ul>"},{"location":"teacher/reference/response_area_components/#an-example-2-changing-input-type-on-saved-response-area","title":"An Example 2 - changing input type on SAVED response area","text":"<ul> <li>The teacher creates a new question with Response Areas RA1 and RA2. -&gt; The version of the question is QV1 with status DRAFT.</li> <li>The teacher is making changes (including the change of the input type if needed). -&gt; The changes are being saved into QV1 with no restrictions</li> <li>The teacher performs SAVE action (let's assume RA1 and RA2 input types are both Number for this example). -&gt; The QV1 version status is changed to SAVED</li> <li>The teacher clicks on a question to edit it -&gt; with the first click a new question version QV2 is created with status DRAFT</li> <li>The teacher makes following changes (which are being recorded against QV2):<ul> <li>In RA1: adds a new Input Symbol -&gt; the change is recorded against RA1</li> <li>In RA2: the teacher unlocks and changes the input type -&gt; RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example)</li> </ul> </li> <li>The teacher performs PUBLISH action -&gt; the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table)</li> <li>Students are submitting their answers -&gt; submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format)</li> <li>The teacher decides to REVERT to the question version QV1 -&gt; a new question version QV3 is created and the content of the QV1 is copied to QV3 -&gt; QV3 is DRAFT version which contains RA1 (input type Number) and RA2 (input type Number)</li> <li>The teacher performs PUBLISH action -&gt; the QV3 is published and the teacher can now see submissions against RA1. There are no submissions against RA2 as it has not been (until now) published. Submissions against RA3 are not visible, but they are preserved against the question version QV2.</li> </ul>"},{"location":"teacher/reference/response_area_components/#an-example-3-adding-new-response-area-to-a-published-question","title":"An Example 3 - adding new response area to a published question","text":"<ul> <li>The teacher creates a new question with Response Areas RA1 and RA2. -&gt; The version of the question is QV1 with status DRAFT.</li> <li>The teacher is making changes (including the change of the input type if needed). -&gt; The changes are being saved into QV1 with no restrictions</li> <li>The teacher performs SAVE (or PUBLISH) action (let's assume RA1 and RA2 input types are both Number for this example). -&gt; The QV1 version status is changed to SAVE (or PUBLISHED)</li> <li>The teacher clicks on a question to edit it -&gt; with the first click a new question version QV2 is created with status DRAFT</li> <li>The teacher adds a new response area RA3</li> </ul> <p>=&gt; At this point the teacher is making changes in the question version QV2 (DRAFT) in which:</p> <pre><code>- The input types of RA1 and RA2 are locked, because they exist on a saved version QV1. It does not matter if QV1 is (only) saved or published or if there are  or there are not existing submissions. The reason why it is locked is that the teacher can revert into this version later after submissions are created. By locking it we are making sure that the \"unlock\" process will be triggered which will preserve the original response area and it will create a new response area and it will make sure that the submissions are linked to response area with compatible input type.\n- The input type of RA3 is not locked at this point, because RA3 does not (yet) exist on any saved question version.\n</code></pre>"},{"location":"teacher/reference/response_area_components/Expression/","title":"Expression","text":"<p>This response area is very similar to Text, differing in that it can display how the user's response was interpreted back to them through the 'live preview' feature. This works using the grading function, providing a <code>feedback.response_latex</code> field, which gets rendered.</p>"},{"location":"teacher/reference/response_area_components/Expression/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Expression/#issimilar","title":"<code>isSimilar</code>","text":"<p>Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol).</p>"},{"location":"teacher/reference/response_area_components/Expression/#symbolicequal","title":"<code>symbolicEqual</code>","text":"<p>Compares two symbolic expressions for mathematical equivalence, using SymPy. See SymPy for further information.</p>"},{"location":"teacher/reference/response_area_components/Expression/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Expression/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#allow-handwrite-experimental","title":"Allow Handwrite (Experimental)","text":"<p>Enables a handwriting canvas in the browser, which allows a student can use to draw their expression, rather than type using Sympy's syntax.</p>"},{"location":"teacher/reference/response_area_components/Expression/#photo-experimental","title":"Photo (Experimental)","text":"<p>Allows a student to upload their expression as an image, as an alternative to handwriting if the student isn't using a phone or tablet.</p>"},{"location":"teacher/reference/response_area_components/Expression/#setting-the-answer","title":"Setting The Answer","text":"<p>Type the correct answer into the 'Response Area Answer' using standard syntax. As the student enters the answer, this will be rendered using the 'live preview' feature, to ensure the correct expression has been entered.</p> <p>Use the 'Response Area Preview' to check the answer has been set correctly.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Expression/#example-student-response-area","title":"Example Student Response area","text":"<p>Correct response given </p> <p>Incorrect response given </p>"},{"location":"teacher/reference/response_area_components/Matrix/","title":"Matrix","text":"<p>Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Matrix/#arrayequal","title":"ArrayEqual","text":"<p>Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#arraysymbolicequal","title":"ArraySymbolicEqual","text":"<p>Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Matrix/#rows-and-cols-required","title":"<code>rows and cols</code> (required)","text":"<p>Required paramter, describes the shape of the Matrix to be displayed. </p> <p>In the 'Response area answer' section, the number of rows and columns can either be typed directly into the corresponding boxes, or adjusted using the up and down arrows, which appear once the mouse hovers over the input box.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Matrix/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Matrix/#setting-the-answer","title":"Setting The Answer","text":"<p>Once the required number of rows and cols has been selected, Each element of the matrix can be entered by clicking the individual input boxes and typing in the correct numbers.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Matrix/#example-student-response","title":"Example Student Response","text":"<p>Correct response given  Incorrect response given </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/","title":"MultipleChoice","text":"<p>General multiple choice response area. Features multiple options for single answer and randomising the order.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/MultipleChoice/#arrayequal","title":"ArrayEqual","text":"<p>Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#parameters","title":"Parameters","text":""},{"location":"teacher/reference/response_area_components/MultipleChoice/#options-required","title":"<code>options</code> (required)","text":"<p>This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the <code>parseEquations</code> function, meaning they can support markdown styling and LaTeX. </p> <p>Example</p> <pre><code>\"options\": [\n  \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\",\n  \"\\\\( \\\\pi = 3 \\\\)\",\n  \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\",\n  \"\\\\( 3 = \\\\pi \\\\)\",\n]\n</code></pre>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#randomise-optional","title":"<code>Randomise</code> (optional)","text":"<p>This is an optional boolean which will shuffle the options array on each render of this component. </p> <p>switch to 'randomise' </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-optional","title":"<code>singleAnswer</code> (optional)","text":"<p>By default, each item options is rendered using the html <code>checkbox</code> input type. Setting the <code>singleAnswer</code> boolean flag will turn those into <code>radio</code> buttons, allowing the student to select only one option at a time.</p> <p>Note Changing this flag will alter the shape of the Response Structure, and potentially require changing the grading function type and settings.</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>This structure is different depending on if the <code>singleAnswer</code> option was used:</p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-false-or-undefined","title":"<code>singleAnswer</code> == False (or undefined)","text":"<p>In this case, the user data is saved as an array with the same length as <code>options</code>, where each item is either a <code>1</code> or a <code>0</code> depending on if the corresponding option was selected.</p> <p>Example</p> <p>If for an instance where there are 4 options, and the first and third options were selected, the response field would be:</p> <pre><code>\"response\": [1, 0, 1, 0]\n</code></pre> <p>Example Screenshot: SingleAnswer = False  </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-true","title":"<code>singleAnswer</code> == True","text":"<p>In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. </p> <p>Example</p> <p>If for an instance where there are 4 options, and the third option was selected, the response field would be:</p> <pre><code>\"response\": 2\n</code></pre> <p>Example Screenshot: SingleAnswer = True  </p>"},{"location":"teacher/reference/response_area_components/MultipleChoice/#example-student-response","title":"Example Student Response","text":"<p> This shows a response where <code>singleAnswer</code> was set to False, since each option is displayed as a <code>checkbox</code></p>"},{"location":"teacher/reference/response_area_components/Number/","title":"Number","text":"<p>Very similar to the Text response area, except the user response is parsed as a float.</p>"},{"location":"teacher/reference/response_area_components/Number/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Number/#issimilar","title":"<code>isSimilar</code>","text":"<p>Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol). </p>"},{"location":"teacher/reference/response_area_components/Number/#isexactequal","title":"<code>isExactEqual</code>","text":"<p>A strict comparison in Python using '=='. This function is a basic utility but often not the function you really want to use because it is quite brittle.</p>"},{"location":"teacher/reference/response_area_components/Number/#setting-the-answer","title":"Setting The Answer","text":"<p>In the 'Response Area Answer' section, enter the required float into the input field. To test this, try typing correct and incorrect answers into the 'Response Area Preview' section.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Number/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Number/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Number/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Number/#grading_parameters-optional","title":"<code>grading_parameters</code> (optional)","text":"<ul> <li><code>atol</code>: Absolute tolerance parameter</li> <li><code>rtol</code>: Relative tolerance parameter</li> </ul> <p>Valid params include atol and rtol, which can be used in combination, or alone. As the comparison made is the following:</p> <p>is_correct = abs(res - ans) &lt;= (atol + rtol*abs(ans))</p>"},{"location":"teacher/reference/response_area_components/Number/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>The response is simply sent as a float, parsed from the input field using the JavaScript <code>parseFloat</code> function.</p> <p>Example</p> <pre><code>\"response\": 15.8\n</code></pre>"},{"location":"teacher/reference/response_area_components/Number/#example-student-response","title":"Example Student Response","text":"<p>Correct response: </p> <p>Incorrect response: </p>"},{"location":"teacher/reference/response_area_components/NumericUnits/","title":"NumericUnits","text":"<p>Provides two input fields with <code>Number</code> and <code>Units</code> placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). </p> <p>Note this area will display how the user's response was interpred using the <code>interp_string</code> field provided in the feedback object returned by that function (if it exists).</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/NumericUnits/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/NumericUnits/#response-structure","title":"Response Structure","text":"<p>This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. </p> <p>In this case, the response is a single string which features the user's response to both fields separated by a space.</p> <p>Example</p> <pre><code>\"response\": \"150 g\"\n</code></pre>"},{"location":"teacher/reference/response_area_components/NumericUnits/#example-screenshot","title":"Example Screenshot","text":""},{"location":"teacher/reference/response_area_components/Table/","title":"Table","text":"<p>Table response area. Will populate the component with a grid of text input fields, in order to facilitate inputing elements of a table. The number of rows and columns can be specified, along with their corresponding names.</p>"},{"location":"teacher/reference/response_area_components/Table/#evaluation-function-options","title":"Evaluation Function Options","text":""},{"location":"teacher/reference/response_area_components/Table/#arrayequal","title":"ArrayEqual","text":"<p>Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.</p>"},{"location":"teacher/reference/response_area_components/Table/#arraysymbolicequal","title":"ArraySymbolicEqual","text":"<p>Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information.</p>"},{"location":"teacher/reference/response_area_components/Table/#component-parameters","title":"Component Parameters","text":""},{"location":"teacher/reference/response_area_components/Table/#rows","title":"<code>rows</code>","text":"<p>The number of rows required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.</p>"},{"location":"teacher/reference/response_area_components/Table/#cols","title":"<code>cols</code>","text":"<p>The number of columns required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Table/#post_response_text-optional","title":"<code>post_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Table/#pre_response_text-optional","title":"<code>pre_response_text</code> (optional)","text":"<p>Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.</p>"},{"location":"teacher/reference/response_area_components/Table/#setting-the-answer","title":"Setting The Answer","text":"<p>Once the required number of rows and cols has been inputted, The names of each row and column should be changed depending on their corresponding variables. The value of each grid element can then be entered into individual input fields. </p> <p>If the row or column names are not changed, these will appear blank in the student response area.</p> <p></p>"},{"location":"teacher/reference/response_area_components/Table/#example-student-response","title":"Example Student Response","text":"<p>Correct response: </p> <p>Incorrect response: </p>"},{"location":"dev_eval_function_docs/arrayEqual/","title":"ArrayEqual","text":"<p>Edit on GitHub  View Code </p> <p>Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.</p>"},{"location":"dev_eval_function_docs/arrayEqual/#inputs","title":"Inputs","text":"<p>Valid params include <code>atol</code> and <code>rtol</code>, which can be used in combination, or alone. (just like the <code>IsSimilar</code> grading function)</p> <pre><code>{\n  \"response\": \"&lt;array&gt;\",\n  \"answer\": \"&lt;array&gt;\",\n  \"params\": {\n    \"atol\": \"&lt;number&gt;\",\n    \"rtol\": \"&lt;number&gt;\"\n  }\n}\n</code></pre> <p>Note: <code>response</code> and <code>answer</code> arrays are parsed using <code>np.array(dtype=np.float32)</code>, any errors this causes are returned and the comparison fails.</p>"},{"location":"dev_eval_function_docs/arrayEqual/#atol","title":"<code>atol</code>","text":"<p>Absolute tolerance parameter</p>"},{"location":"dev_eval_function_docs/arrayEqual/#rtol","title":"<code>rtol</code>","text":"<p>Relative tolerance parameter</p>"},{"location":"dev_eval_function_docs/arrayEqual/#outputs","title":"Outputs","text":""},{"location":"dev_eval_function_docs/arrayEqual/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/arrayEqual/","title":"ArrayEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>MATRIX</code></li> <li><code>TABLE</code></li> <li><code>MULTIPLE_CHOICE</code></li> </ul> <p>This function is used to compare two number arrays/vectors/matrices, provided absolute and/or relative tolerance parameters <code>rtol</code> and <code>atol</code>. This is carried out using the numpy.allclose function.</p> <p>If the answer is not an array of numbers an exception is raised. If the response is not an array of numbers, a feedback message that informs the user that only numbers are accepte will be generated.</p>"},{"location":"user_eval_function_docs/arrayEqual/#optional-parameters","title":"Optional parameters","text":"<p>There is one optional parameter: <code>feedback_for_incorrect_response</code>.</p>"},{"location":"user_eval_function_docs/arrayEqual/#feedback_for_incorrect_response","title":"<code>feedback_for_incorrect_response</code>","text":"<p>All feedback for all incorrect responses will be replaced with the string that this parameter is set to.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/","title":"ComparePhysicalQuantities","text":"<p>Edit on GitHub  View Code </p> <p>Evaluation function which proveds some basic some dimensional analysis functionality.</p> <ul> <li>DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></li> <li>Substitutions of symbols before comparison of expressions is done</li> <li>Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem</li> </ul> <p>Note: When the <code>quantities</code> grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example <code>Nm/s</code> or <code>Newton*metre/SECOND</code> will not be handled correctly, but <code>newton*metre/second</code> will.</p> <p>Note: Prefixes have lower precedence than exponentiation, e.g. <code>10*cm**2</code> will be interpreted as \\(10 \\cdot 10^{-2} \\mathrm{metre}^2\\) rather than \\(10 (10^(-2)\\mathrm{metre})^2\\).</p> <p>Note: This function allows omitting <code>*</code> and using <code>^</code> instead of <code>**</code> if the grading parameter <code>strict_syntax</code> is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities).</p> <p>Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g.  e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. <code>m</code>) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Longer short form symbols take precedence over shorter short forms, e.g. <code>sr</code> will be interpreted as <code>steradian</code> instead of <code>second radian</code>.</p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p> <p>Note: When running the unit test some tests are expected to take much longer than the other. These tests can be skipped by adding <code>skip_resource_intensive_tests</code> as a command line argument to improve iteration times.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#changing-default-feedback-messages","title":"Changing default feedback messages","text":"<p>The feedback messages can be set on a per-task basis (see description of the <code>custom_feedback</code> input parameter).</p> <p>The default feedback messages are defined in <code>feedback_responses_list</code> defined near the top of <code>evaulation.py</code>, which contains a list of dictionaries of feedback responses that are used througout the code. All feedback messages visible to learners are defined in these dictionaries. The entries in the dictionaries are either be string of functions that return strings.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#inputs","title":"Inputs","text":"<p>All input parameters need to be supplied via the Grading parameters panel.</p> <p>There are seven optional parameters that can be set: <code>elementary_functions</code>, <code>substitutions</code>, <code>quantities</code>, <code>strict_syntax</code>, <code>rtol</code>, <code>atol</code> and <code>comparison</code>.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#custom_feedback","title":"<code>custom_feedback</code>","text":"<p>Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback.</p> <p>The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","title":"Feedback tags for all comparisons","text":"<ul> <li><code>PARSE_ERROR_WARNING</code> Response cannot be parsed as an expression or physical quantity.</li> <li><code>PER_FOR_DIVISION</code> Warns about risk of ambiguity when using <code>per</code> instead <code>/</code> for division.</li> <li><code>STRICT_SYNTAX_EXPONENTIATION</code> Warns that <code>^</code> cannot be used for exponentiation when <code>strict_syntax</code> is set to <code>true</code>.</li> <li><code>QUANTITIES_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of quantities could not be parsed.</li> <li><code>SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of substitutions could not be parsed.</li> </ul>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","title":"Feedback tags for <code>buckinghamPi</code> comparison","text":"<ul> <li><code>VALID_CANDIDATE_SET</code> Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text.</li> <li><code>NOT_DIMENSIONLESS</code> Message displayed when at least one groups is not dimensionless.</li> <li><code>MORE_GROUPS_THAN_REFERENCE_SET</code> Message displayed when the response contains more groups than necessary.</li> <li><code>CANDIDATE_GROUPS_NOT_INDEPENDENT</code> Message displayed when the groups in the response are not independent.</li> <li><code>TOO_FEW_INDEPENDENT_GROUPS</code> Message displayed when the response contains fewer groups than necessary.</li> <li><code>UNKNOWN_SYMBOL</code> Message displayed when the response contains some undefined symbol.</li> <li><code>SUM_WITH_INDEPENDENT_TERMS</code>  Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.</li> </ul>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#substitutions","title":"<code>substitutions</code>","text":"<p>String that lists all substitutions that should be done to the answer and response inputs before processing.</p> <p>Each substitution should be written in the form <code>('original string','substitution string')</code> and all pairs concatenated into a single string. Substitutions can be grouped by adding <code>|</code> between two substitutions. Then all substitutions before <code>|</code> will be performed before the substitutions after <code>|</code>.</p> <p>The input can contain an arbitrary number of substitutions and <code>|</code> symbols.</p> <p>Note that using substitutions will replace all default definitions of quantities and dimensions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#quantities","title":"<code>quantities</code>","text":"<p>String that lists all quantities that can be used in the answer and response.</p> <p>Each quantity should be written in the form <code>('quantity name','(units)')</code> and all pairs concatenated into a single string. See tables below for available default units.</p> <p>Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities.</p> <p>NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question.</p> <p>If the <code>comparison</code> parameter is set to <code>dimensions</code>, it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions.</p> <p>If the <code>comparison</code> parameter is set to <code>buckinghamPi</code>, then <code>quantities</code> should be set in a different way. See the detailed description of <code>buckinghamPi</code> further down.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html</p> SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that degrees Celsius is omitted.</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html</p> <p>Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*kilo*metre/second**2</code> is accepted but <code>10 kilometre/second^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#rtol","title":"<code>rtol</code>","text":"<p>Maximum relative error allowed when comparing expressions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#atol","title":"<code>atol</code>","text":"<p>Maximum absolute error allowed when comparing expressions.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#comparison","title":"<code>comparison</code>","text":"<p>Parameter that determines what kind of comparison is done. There are four possible options:</p> <ul> <li><code>expression</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the <code>atol</code> and <code>rtol</code> parameters).</li> <li><code>expressionExact</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible.</li> <li><code>dimensions</code> Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities.</li> <li><code>buckinghamPi</code> Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</li> </ul> <p>For more details on each options see the description below and the corresponding examples.</p> <p>If <code>comparison</code> is not specified it defaults to <code>expression</code>.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expression","title":"<code>expression</code>","text":"<p>Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close.</p> <p>How big the difference is between the value of the answer and the value of the response is decided by the <code>rtol</code> and <code>atol</code> parameters. If neither <code>atol</code> nor <code>rtol</code> is specified the function will allow a relative error of \\(10^{-12}\\). If <code>atol</code> is specified its value will be interpreted as the maximum allowed absolute error. If <code>rtol</code> is specified its value will be interpreted as the maximum allowed relative error. If both <code>atol</code> and <code>rtol</code> the function will check both the absolute and relative error.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expressionexact","title":"<code>expressionExact</code>","text":"<p>Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#dimensions","title":"<code>dimensions</code>","text":"<p>Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities.</p> <p>With this option the quantities (specified by the <code>quantities</code> parameter) can be given either dimension only, or units.</p>"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#buckinghampi","title":"<code>buckinghamPi</code>","text":"<p>Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</p> <p>There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and set answer to <code>-</code>. The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response.</p> <p>Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/","title":"ComparePhysicalQuantities","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>NUMERIC_UNITS</code></li> </ul> <p>Evaluation function which proveds some basic some dimensional analysis functionality.</p> <ul> <li>DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></li> <li>Substitutions of symbols before comparison of expressions is done</li> <li>Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem</li> </ul> <p>Note: When the <code>quantities</code> grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example <code>Nm/s</code> or <code>Newton*metre/SECOND</code> will not be handled correctly, but <code>newton*metre/second</code> will.</p> <p>Note: Prefixes have lower precedence than exponentiation, e.g. <code>10*cm**2</code> will be interpreted as \\(10 \\cdot 10^{-2}~\\mathrm{metre}^2\\) rather than \\(10 \\cdot (10^{-2}~\\mathrm{metre})^2\\).</p> <p>Note: This function allows omitting <code>*</code> and using <code>^</code> instead of <code>**</code> if the grading parameter <code>strict_syntax</code> is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities).</p> <p>Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g.  e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. <code>mug</code> will be interpreted as <code>micro*gram</code> instead <code>metre*astronomicalunit*gram</code>. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. <code>m</code>) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. <code>mN</code> will be interpreted as <code>milli newton</code>, <code>Nm</code> as <code>newton metre</code>, <code>mmN</code> as <code>milli metre newton</code>, <code>mNm</code> as <code>milli newton metre</code> and <code>Nmm</code> as <code>newton milli metre</code>. - Longer short form symbols take precedence over shorter short forms, e.g. <code>sr</code> will be interpreted as <code>steradian</code> instead of <code>second radian</code>.</p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#inputs","title":"Inputs","text":"<p>All input parameters need to be supplied via the Grading parameters panel.</p> <p>There are seven optional parameters that can be set: <code>elementary_functions</code>, <code>substitutions</code>, <code>quantities</code>, <code>strict_syntax</code>, <code>rtol</code>, <code>atol</code> and <code>comparison</code>.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#custom_feedback","title":"<code>custom_feedback</code>","text":"<p>Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback.</p> <p>The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","title":"Feedback tags for all comparisons","text":"<ul> <li><code>PARSE_ERROR_WARNING</code> Response cannot be parsed as an expression or physical quantity.</li> <li><code>PER_FOR_DIVISION</code> Warns about risk of ambiguity when using <code>per</code> instead <code>/</code> for division.</li> <li><code>STRICT_SYNTAX_EXPONENTIATION</code> Warns that <code>^</code> cannot be used for exponentiation when <code>strict_syntax</code> is set to <code>true</code>.</li> <li><code>QUANTITIES_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of quantities could not be parsed.</li> <li><code>SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY</code> Text in error message that appears if list of substitutions could not be parsed.</li> </ul>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","title":"Feedback tags for <code>buckinghamPi</code> comparison","text":"<ul> <li><code>VALID_CANDIDATE_SET</code> Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text.</li> <li><code>NOT_DIMENSIONLESS</code> Message displayed when at least one groups is not dimensionless.</li> <li><code>MORE_GROUPS_THAN_REFERENCE_SET</code> Message displayed when the response contains more groups than necessary.</li> <li><code>CANDIDATE_GROUPS_NOT_INDEPENDENT</code> Message displayed when the groups in the response are not independent.</li> <li><code>TOO_FEW_INDEPENDENT_GROUPS</code> Message displayed when the response contains fewer groups than necessary.</li> <li><code>UNKNOWN_SYMBOL</code> Message displayed when the response contains some undefined symbol.</li> <li><code>SUM_WITH_INDEPENDENT_TERMS</code>  Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.</li> </ul>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p> <p>Note: setting <code>elementary_functions</code> to true will disable using short forms symbols for units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#substitutions","title":"<code>substitutions</code>","text":"<p>String that lists all substitutions that should be done to the answer and response inputs before processing.</p> <p>Each substitution should be written in the form <code>('original string','substitution string')</code> and all pairs concatenated into a single string. Substitutions can be grouped by adding <code>|</code> between two substitutions. Then all substitutions before <code>|</code> will be performed before the substitutions after <code>|</code>.</p> <p>The input can contain an arbitrary number of substitutions and <code>|</code> symbols.</p> <p>Note that using substitutions will replace all default definitions of quantities and dimensions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#quantities","title":"<code>quantities</code>","text":"<p>String that lists all quantities that can be used in the answer and response.</p> <p>Each quantity should be written in the form <code>('quantity name','(units)')</code> and all pairs concatenated into a single string. See tables below for available default units.</p> <p>Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities.</p> <p>NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question.</p> <p>If the <code>comparison</code> parameter is set to <code>dimensions</code>, it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions.</p> <p>If the <code>comparison</code> parameter is set to <code>buckinghamPi</code>, then <code>quantities</code> should be set in a different way. See the detailed description of <code>buckinghamPi</code> further down.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html</p> SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html</p> <p>Note that degrees Celsius is omitted.</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html</p> <p>Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*kilo*metre/second**2</code> is accepted but <code>10 kilometre/second^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#rtol","title":"<code>rtol</code>","text":"<p>Maximum relative error allowed when comparing expressions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#atol","title":"<code>atol</code>","text":"<p>Maximum absolute error allowed when comparing expressions.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#comparison","title":"<code>comparison</code>","text":"<p>Parameter that determines what kind of comparison is done. There are four possible options:</p> <ul> <li><code>expression</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the <code>atol</code> and <code>rtol</code> parameters).</li> <li><code>expressionExact</code> Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible.</li> <li><code>dimensions</code> Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities.</li> <li><code>buckinghamPi</code> Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</li> </ul> <p>For more details on each options see the description below and the corresponding examples.</p> <p>If <code>comparison</code> is not specified it defaults to <code>expression</code>.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expression","title":"<code>expression</code>","text":"<p>Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close.</p> <p>How big the difference is between the value of the answer and the value of the response is decided by the <code>rtol</code> and <code>atol</code> parameters. If neither <code>atol</code> nor <code>rtol</code> is specified the function will allow a relative error of \\(10^{-12}\\). If <code>atol</code> is specified its value will be interpreted as the maximum allowed absolute error. If <code>rtol</code> is specified its value will be interpreted as the maximum allowed relative error. If both <code>atol</code> and <code>rtol</code> the function will check both the absolute and relative error.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expressionexact","title":"<code>expressionExact</code>","text":"<p>Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#dimensions","title":"<code>dimensions</code>","text":"<p>Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities.</p> <p>With this option the quantities (specified by the <code>quantities</code> parameter) can be given either dimension only, or units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#buckinghampi","title":"<code>buckinghamPi</code>","text":"<p>Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem.</p> <p>There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and set answer to <code>-</code>. The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the <code>quantities</code> parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response.</p> <p>Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#1-checking-the-dimensions-of-an-expression-or-physical-quantity","title":"1 Checking the dimensions of an expression or physical quantity","text":"<p>DEPRECATED</p> <p>RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></p> <p>This example will check if the response has dimensions \\(\\frac{\\mathrm{length}^2}{\\mathrm{time}^2}\\).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a","title":"a)","text":"<p>To check an expression there needs to be some predefined quantities that can be used in the expression. Since only dimensions will be checked units are not necessary (but could be used as well).</p> <p>Here a response area with input type <code>TEXT</code> and two grading parameters, <code>quantities</code> and <code>comparison</code>, will be used.</p> <p><code>quantities</code> is defined as follows: <pre><code>('d','(length)') ('t','(time)') ('v','(length/time)')\n</code></pre></p> <p><code>comparison</code> is set to <code>dimensions</code>.</p> <p>The answer is set two some expression with the right dimensions, e.g. <code>v**2</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>v**2</code> <code>v^2</code> <code>5*v**2</code> <code>5v^2</code> <code>(d/t)**2+v**2</code> <code>(d/t)^2+v^2</code> <code>d**2/t**2</code> <code>d^2/t^2</code> <code>d**2*t**(-2)</code> <code>d^2 t^(-2)</code> <code>d/t*v</code> <code>vd/t</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b","title":"b)","text":"<p>Checking the dimensions of a quantity directly, i.e. the dimensions of an expression of the form <code>number*units</code>, no predefined quantities are necessary.</p> <p>Here a response area with input type <code>TEXT</code> and one grading parameter,<code>comparison</code>, will be used.</p> <p><code>comparison</code> is set to <code>dimensions</code>.</p> <p>The answer is set two some expression with the right dimensions, e.g. <code>length**2/time**2</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected in the answer we do not need to set any input symbols.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>metre**2/second**2</code> <code>metre^2/second^2</code> <code>m^2/s^2</code> <code>(centi*metre)**2/hour**2</code> <code>(centimetre)^2/h^2</code> <code>(cm)^2/h^2</code> <code>246*ohm/(kilo*gram)*coulomb**2/second</code> <code>246 ohm/(kilogram) coulomb^2/second</code> <code>246 O/(kg) c^2/s</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#2-checking-the-value-of-an-expression-or-a-physical-quantity","title":"2 Checking the value of an expression or a physical quantity","text":"<p>DEPRECATED</p> <p>RECOMMENDED ALTERNATIVE: CompareExpressions with the <code>physical_quantity</code> parameter set to <code>true</code></p> <p>This examples checks if your expression is equal to \\(2~\\frac{\\mathrm{kilometre}}{\\mathrm{hour}}\\).</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_1","title":"a)","text":"<p>Here an expression with predefined quantities is checked as exactly as possible. This is done with a TEXT response area with the following parameters: <code>quantities</code> is set to: <pre><code>('d','(length)') ('t','(time)') ('v','(length/time)')\n</code></pre> Note that short form symbols cannot be used when defining quantities.</p> <p><code>comparison</code> is set to <code>expressionExact</code>.</p> <p>The response area answer is set to <code>2*v</code> but there are many other expressions that would work just as well. Note that we cannot write <code>2*kilo*metre/second</code> as response or answer since the predefined quantity <code>t</code> will substitute the <code>t</code> in <code>metre</code> which results in unparseable input.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units and single character symbols are expected in the answer we will not set the grading parameter <code>symbols</code>.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>2*v</code> <code>2v</code> <code>2000/3600*d/t</code> <code>2000/3600 d/t</code> <code>1/1.8*d/t</code> <code>d/(1.8t)</code> <code>v+1/3.6*d/t</code> <code>v+d/(3.6t)</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_1","title":"b)","text":"<p>Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed absolute tolerance of \\(0.05 \\frac{metre}{second}\\) can be done with a TEXT response area with <code>atol</code> set to <code>0.05</code> and the answer set to <code>2*kilo*metre/hour</code>.</p> <p>Note: <code>atol</code> is always assumed to be given in the base SI units version of the expression. This is likely to change in future versions of the function.</p> <p>The <code>comparison</code> parameter could also be set to <code>expression</code> but since this is the default it is not necessary.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected in the answer no input symbols are necessary.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>0.556*metre/second</code> <code>0.556 metre/second</code> <code>0.556 m/s</code> <code>0.560*metre/second</code> <code>0.560 metre/second</code> <code>0.560 m/s</code> <code>0.6*metre/second</code> <code>0.6 metre/second</code> <code>0.6 m/s</code> <code>2*kilo*metre/hour</code> <code>2 kilometre/hour</code> <code>2 km/h</code> <code>1.9*kilo*metre/hour</code> <code>1.9 kilometre/hour</code> <code>1.9 km/h</code> <code>2.1*kilo*metre/hour</code> <code>2.1 kilometre/hour</code> <code>2.1 km/h</code> <p>In the example given in the example problem set, the following responses are tested and evaluated as incorrect:</p> Strict syntax Relaxed syntax Using symbols <code>0.61*metre/second</code> <code>0.61 metre/second</code> <code>0.61 m/s</code> <code>2.2*kilo*metre/hour</code> <code>2.2 kilometre/hour</code> <code>2.2 km/h</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c","title":"c)","text":"<p>Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed relative tolerance of \\(0.05\\) can be done with a TEXT response area with <code>rtol</code> set to <code>0.05</code> and the answer set to <code>2*kilo*metre/hour</code>.</p> <p>The <code>comparison</code> parameter could also be set to <code>expression</code> but since this is the default it is not necessary.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax Using symbols <code>0.533*metre/second</code> <code>0.533 metre/second</code> <code>0.533 m/s</code> <code>2.08*kilo*metre/hour</code> <code>2.08 kilometre/hour</code> <code>2.08 km/h</code> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since only default SI units are expected it is not necessary to set any input symbols.</p> <p>In the example given in the example problem set, the following responses are tested and evaluated as incorrect:</p> Strict syntax Relaxed syntax Using symbols <code>0.522*metre/second</code> <code>0.522 metre/second</code> <code>0.522 m/s</code> <code>2.11*kilo*metre/hour</code> <code>2.11 kilometre/hour</code> <code>2.11 km/h</code>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#3-checking-if-a-set-of-quantities-match-the-buckingham-pi-theorem","title":"3 Checking if a set of quantities match the Buckingham pi theorem","text":""},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_2","title":"a)","text":"<p>In this example the task is: Given \\(U\\), \\(L\\) and \\(\\nu\\), suggest a dimensionless group.</p> <p>For this problem we do not need to predefine any quantities and give exact dimensions. The algorithm assumes that all symbols in the answer (that are not numbers or predefined constants such as \\(\\pi\\)) are quantities and that there are no other quantities that should appear in the answer.</p> <p>Note: This means that the algorithm does not in any way check that the stated answer is dimensionless, ensuring that that is left to the problem author.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code> and answer set to <code>['U*L/nu']</code>. It is not necessary to use this specific answer, any example of a correct dimensionless group should work.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_2","title":"b)","text":"<p>In this example the task is: Given \\(U\\), \\(L\\), \\(\\nu\\) and \\(f\\), determine the necessary number of dimensionless groups and give one example of possible expressions for them.</p> <p>This task is similar to example a) with two significant differences. First, adding \\(f\\) means that there are now two groups required, and second the problem will constructed by defining the quantities and let the function compute the rest on its own instead of supplying a reference example.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code>, <code>quantities</code> set to <code>('U','(length/time)') ('L','(length)') ('nu','(length**2/time)') ('f','(1/time)')</code> and <code>answer</code> set to <code>-</code>.</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c_1","title":"c)","text":"<p>In this example the task is: Suppose we are studying water waves that move under the influence of gravity. We suppose that the variables of interest are the acceleration in free fall \\(g\\), the velocity of the wave \\(v\\), the height of the wave \\(h\\) and the wave length \\(\\ell\\). We also suppose that they are related by a dimensionally consistent equation \\(f(g,v,h,l) = 0\\). Determine the minimum number of dimensionless \\(\\pi\\)-variables needed to describe this problem according to the Buckingham pi-theorem and give one example of possible expressions for the dimensionless quantities.</p> <p>For this problem two dimensionless groups are needed, see the worked solution for a terse solution that gives the general form of the dimensionless quantities.</p> <p>For this example a TEXT response area is used with <code>comparison</code> set to <code>buckinghamPi</code> and then give a list of correct group expressions formatted as the code for a python list. For this example the answer <code>['g**(-2)*v**4*h*l**3', 'g**(-2)*v**4*h**2*l**4']</code> was used (this corresponds to \\(p_1 = 1\\), \\(p_2 = 2\\), \\(q_1 = 3\\), \\(q_2 = 4\\) in the worked solution). The feedback was costumized by setting the <code>custom_feedback</code> parameter too: <code>\"custom_feedback\": { \"VALID_CANDIDATE_SET\": \"Your list of power products satisfies the Buckingham Pi theorem.\", \"NOT_DIMENSIONLESS\": \"At least one power product is not dimensionless.\", \"MORE_GROUPS_THAN_REFERENCE_SET\": \"Response has more power products than necessary.\", \"CANDIDATE_GROUPS_NOT_INDEPENDENT\": \"Power products in response are not independent.\", \"TOO_FEW_INDEPENDENT_GROUPS\": \"Candidate set contains too few independent groups.\", \"UNKNOWN_SYMBOL\": \"One of the prower products contains an unkown symbol.\", \"SUM_WITH_INDEPENDENT_TERMS\": \"The candidate set contains an expression which contains more independent terms that there are groups in total. The candidate set should ideally only contain expressions written as power products.\" }</code></p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. To remove this requirement the grading parameter <code>strict_syntax</code> is set to false. Since <code>nu</code> is a multicharacter symbol it needs to be added as an input symbol.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#4-defining-costum-sets-of-units","title":"4 Defining costum sets of units","text":"<p>In this problem it is demonstrated how to use <code>substitutions</code> to define costum units.</p>"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_3","title":"a)","text":"<p>In this problem currencies will be us as units, and thus the quantities will no longer be physical.</p> <p>Here the <code>substitutions</code> parameter will be set so that the evaluation function can be used to compare. Note that using <code>substitutions</code> this way means that the default SI units can no longer be used.</p> <p>The following exchange rates (from Bank of England 1 August 2022) will be used:</p> Currency Exchange rate \\(1\\) EUR \\(1.1957\\) GBP \\(1\\) USD \\(1.2283\\) GBP \\(1\\) CNY \\(8.3104\\) GBP \\(1\\) INR \\(96.943\\) GBP <p>To compare prices written in different currencies a reference currency needs to be chosen. In this case GBP will be used. To substitute other currencies for their corresponding value in GBP the following grading parameter can be used: <pre><code>\"substitutions\":\"('EUR','(1/1.1957)*GBP') ('USD','(1/1.2283)*GBP') ('CNY','(1/8.3104)*GBP') ('INR','(1/96.9430)*GBP')\"\n</code></pre> Since these conversion are not exact and for practical purposes prices are often not gives with more than two decimals of precision we also want to set the absolute tolerance, <code>atol</code>, to \\(0.05\\).</p> <p>With default settings it is required to put <code>*</code> (or <code>/</code>) between each part of the response and answer. By setting the grading parameter <code>strict_syntax</code> to false the <code>*</code> can be omitted and <code>^</code> can be used instead of <code>**</code>. To ensure that this works correctly it is necessary to list the multicharacter symbols that are expected to appear in the answer and response as input symbols. For this example this means setting <code>EUR</code>, <code>USD</code>, <code>CNY</code> and <code>INR</code> as codes for inut symbols.</p> <p>In the example given in the example problem set, the answer set to <code>10*GBP</code> and the following responses are tested and evaluated as correct:</p> Strict syntax Relaxed syntax <code>11.96*EUR</code> <code>11.96 EUR</code> <code>12.28*USD</code> <code>12.28 USD</code> <code>83.10*CNY</code> <code>83.10 CNY</code> <code>969.43*INR</code> <code>969.43 INR</code>"},{"location":"dev_eval_function_docs/compareBoolean/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Brief description of what this evaluation function does, from the developer perspective</p>"},{"location":"dev_eval_function_docs/compareBoolean/#inputs","title":"Inputs","text":"<p>Specific input parameters which can be supplied when the <code>eval</code> command is supplied to this function.</p>"},{"location":"dev_eval_function_docs/compareBoolean/#outputs","title":"Outputs","text":"<p>Output schema/values for this function</p>"},{"location":"dev_eval_function_docs/compareBoolean/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/compareBoolean/#simple-evaluation","title":"Simple Evaluation","text":"<pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre> <pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/compareBoolean/","title":"compareBoolean","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>CODE</code></li> </ul> <p>This function uses SymPy to test two Boolean expressions for equivalence. Expressions are considered equal if they result in the same truth table.</p> <p>When answering questions on Booleans, it is easy for students to come up with equivalent expressions but in a different form (e.g.  if using a Karnaugh map versus by inspection). compareBoolean aims to alleviate some of the frustration that may arise by accepting any response that is equivalent to the correct answer.</p>"},{"location":"user_eval_function_docs/compareBoolean/#syntax","title":"Syntax","text":"<p>The current syntax expected by this function is based on the bitwise Boolean syntax used in C, Matlab and many other programming languages.</p> Operator Meaning LaTeX <code>A | B</code> <code>A OR B</code> \\(A + B\\) <code>A &amp; B</code> <code>A AND B</code> \\(A \\cdot B\\) <code>A ^ B</code> <code>A XOR B</code> \\(A \\oplus B\\) <code>~A</code> <code>NOT A</code> \\(\\overline{A}\\) <p>The order of precedence is as follows:</p> <ol> <li>NOT</li> <li>AND</li> <li>OR/XOR</li> </ol> <p>Brackets can be used to group terms and specify the order of evaluation. For example, <code>A &amp; B | C &amp; D</code> is interpreted as <code>(A &amp; B) | (C &amp; D)</code>.</p>"},{"location":"user_eval_function_docs/compareBoolean/#examples","title":"Examples","text":"<p>The function can understand a wide variety of complex boolean expressions. Here are some examples to illustrate its capabilities. Each pair of expressions is equivalent, and would be marked as \"correct\" by compareBoolean.</p> Response Answer Comments <code>x &amp; y</code> <code>y &amp; x</code> A trivial example, but probably the most common way student responses will differ from the answer <code>(x &amp; ~y) | (y &amp; ~z)</code> <code>x ^ y</code> Both expressions are equivalent to a logical exclusive or. <code>~(~x &amp; ~y)</code> <code>x | y</code> In this example de Morgan's laws have been used to find an equivalent representation of the OR operator."},{"location":"user_eval_function_docs/compareBoolean/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/compareBoolean/#optional-parameters","title":"Optional parameters","text":"<p>There are two optional parameters that can be set: <code>enforce_expression_equality</code> and <code>disallowed</code>.</p>"},{"location":"user_eval_function_docs/compareBoolean/#enforce_expression_equality","title":"<code>enforce_expression_equality</code>","text":"<p>If this Boolean parameter is true, the response and the answer must be strictly equal, i.e in the same form.</p>"},{"location":"user_eval_function_docs/compareBoolean/#disallowed","title":"<code>disallowed</code>","text":"<p>This parameter is a list of strings (<code>\"and\"</code>, <code>\"or\"</code>, <code>\"not\"</code> or <code>\"xor\"</code>). If one of these strings is present in the list, that operation will be disallowed. For example, responding <code>A | B</code> to an answer of <code>~(~A &amp; ~ B)</code> would normally be considered correct, but if a  <code>\"disallowed\": [\"or\"]</code> parameter were added, it would be considered incorrect. This could be useful for questions on De Morgan's laws, such as  expressing a function using only NAND gates.</p>"},{"location":"user_eval_function_docs/compareBoolean/#examples-from-integration-tests","title":"Examples from Integration Tests","text":""},{"location":"user_eval_function_docs/compareBoolean/#trivial-comparisons","title":"Trivial comparisons","text":"<p>The response and answer are exactly the same, so the response should be considered correct.</p> Response Answer Correct? <code>A &amp; B</code> <code>A &amp; B</code> \u2713 <p>Multi-character variable names are also supported</p> Response Answer Correct? <code>A &amp; Test</code> <code>A &amp; Test</code> \u2713"},{"location":"user_eval_function_docs/compareBoolean/#trivial-comparisons-but-not-identical","title":"Trivial comparisons, but not identical","text":"<p>Variables can appear in any order.</p> Response Answer Correct? <code>B &amp; A</code> <code>A &amp; B</code> \u2713 <p>The wrong operator is used, so this is incorrect as the two expressions have different truth tables.</p> Response Answer Correct? <code>A | B</code> <code>A &amp; B</code> \u2717"},{"location":"user_eval_function_docs/compareBoolean/#more-complex-comparisons","title":"More complex comparisons","text":"<p>XOR can be implemented using NAND or NOR</p> <p>Using NAND:</p> Response Answer Correct? <code>~(~(A &amp; ~(A &amp; B)) &amp; ~(B &amp; ~(A &amp; B)))</code> <code>A ^ B</code> \u2713 <p>Using NOR:</p> Response Answer Correct? <code>~(~(~A | ~B) | ~(A | B))</code> <code>A ^ B</code> \u2713 <p>A few examples using de Morgan's laws:</p> Response Answer Correct? <code>~(~A &amp; ~(B &amp; C))</code> <code>A | (B &amp; C)</code> \u2713 <code>A | ~(~B | ~C)</code> <code>A | (B &amp; C)</code> \u2713"},{"location":"dev_eval_function_docs/arraySymbolicEqual/","title":"ArraySymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>This evaluation function can take any level of nesting for \"response\" and \"answer\" fields, as comparison is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymbolicEqual function, called using the experimental EvaluationFunctionClient from the evaluation-function-utils library.</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#inputs","title":"Inputs","text":"<p>This compares cells using the <code>symbolicEqual</code> function. Please consult that function's documentation for details on it's allowable parameters, as the ones provided to this function are fed through as they are.</p> <pre><code>{\n  \"response\": \"&lt;array (of arrays) of strings&gt;\",\n  \"answer\": \"&lt;array (of arrays) of strings&gt;\",\n  \"params\": {\n            \"Any params accepted by symbolicEqual\"\n    }\n}\n</code></pre> <p>Note: <code>response</code> and <code>answer</code> arrays should ultimately have string elements, even though they can have any level of nesting.</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>grade</code> command look like the following:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"detailed_feedback\": [\n      {\n        \"is_correct\": \"&lt;bool&gt;\",\n        \"level\": \"&lt;sympy correctness level&gt;\"\n      },\n      {\n        \"...\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Note: The <code>detailed_feedback</code> result field is of the same shape as the answer, giving specific information for the correctness of each cell in the evaluated array</p>"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#examples","title":"Examples","text":""},{"location":"dev_eval_function_docs/arraySymbolicEqual/#simple-arrays","title":"Simple Arrays","text":"<p>Correct behaviour Input <pre><code>{\n  \"response\": [\"a\", \"b + c\"],\n  \"answer\": [\"a\", \"c + b\"]\n}\n</code></pre></p> <p>Output <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": true,\n        \"detailed_feedback\": [\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            },\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            }\n        ]\n    }\n}\n</code></pre></p> <p>Incorrect behaviour Input <pre><code>{\n  \"response\": [\"a\", \"b + 2*c\"],\n  \"answer\": [\"a\", \"c + b\"]\n}\n</code></pre></p> <p>Output <pre><code>{\n    \"command\": \"eval\",\n    \"result\": {\n        \"is_correct\": false,\n        \"detailed_feedback\": [\n            {\n                \"is_correct\": true,\n                \"level\": \"1\"\n            },\n            {\n                \"is_correct\": false\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"user_eval_function_docs/arraySymbolicEqual/","title":"ArraySymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>MATRIX</code></li> <li><code>NUMBER</code></li> <li><code>TABLE</code></li> </ul> <p>This function compares two symbolic expression arrays, with any level of nesting (2D, 3D, irregular shape, ...). Each cell is compared using the <code>SymbolicEqual</code> evaluation function.</p>"},{"location":"user_eval_function_docs/arraySymbolicEqual/#inputs","title":"Inputs","text":"<p>This function support all input parameters that <code>SymbolicEqual</code> uses.</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/","title":"WolframAlphaEqual","text":"<p>Edit on GitHub  View Code </p> <p>This simple evaluation function uses the WolframAlpha API to compare two strings. It performs two requests in parallel:</p> <ol> <li>One to check how the user's <code>response</code> is interpreted by WolframAlpha</li> <li>One to compare the <code>response</code> to the <code>answer</code>, by submitting the following input to the api: <code>res == ans</code></li> </ol> <p>NOTE: To work, this grading script requires a valid WolframAlpha AppID! This should be stored in the <code>WOLFRAM_APPID</code> env variable.</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#inputs","title":"Inputs","text":"<p>This function doesn't need any parameters, simply two response and answer fields <pre><code>{\n  \"response\": \"&lt;string&gt;\",\n  \"answer\": \"&lt;string&gt;\"\n}\n</code></pre></p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#outputs","title":"Outputs","text":"<pre><code>{\n  \"is_correct\": \"&lt;bool or null&gt;\",\n  \"interp_string\": \"&lt;string&gt;\",\n  \"raw_comp\": \"&lt;dict&gt;\",\n  \"raw_interp\": \"&lt;dict&gt;\"\n}\n</code></pre>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#is_correct","title":"<code>is_correct</code>","text":"<p>Extracted from the second WolframAlpha call. More specifically, the <code>\"id\": \"Result\"</code> pod's first <code>subpod.plaintext</code> value.</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#interp_string","title":"<code>interp_string</code>","text":"<p>Human friendly string which indicates how WolframAlpha interpreted the user response. Extracted from the first WolframAlpha call. Corresponds also to the value of <code>plaintext</code> from the first <code>subpod</code> in the <code>\"id\" : \"Input\"</code> pod.</p> <p>For example, if the user entered <code>10 kg</code>, this might look like <code>10 kg (kilograms)</code></p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#raw_comp","title":"<code>raw_comp</code>","text":"<p>For debugging, passes the raw json result obtained from the second WolframAlpha call (comparison).</p>"},{"location":"dev_eval_function_docs/wolframAlphaEqual/#raw_interp","title":"<code>raw_interp</code>","text":"<p>For debugging, passes the raw json result obtained from the first WolframAlpha call (interpretation).</p> <p>Will also return <code>error</code> detailing any issues that were encountered along the way</p>"},{"location":"user_eval_function_docs/wolframAlphaEqual/","title":"WolframAlphaEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>NUMERIC_UNITS</code></li> </ul> <p>This function uses the WolframAlpha engine to compare a student's response to the correct answer. Its power can be leveraged to realise physical-units-aware evaluation, symbolic expression comparison and much more. No validation is carried out on function inputs, values are simply sent to the API in the form <code>response == answer</code>. So for example if the student provided <code>10 kilograms</code>, and the answer was defined as <code>0.01 tonnes</code>, then <code>10 kilograms == 0.01 tonnes</code> is sent to the WolframAlpha API.</p>"},{"location":"dev_eval_function_docs/compareConstructs/","title":"compareConstructs","text":"<p>Edit on GitHub  View Code </p> <p>compareConstructs is an evaluation function that checks and verifies Python code against a given correct answer.</p>"},{"location":"dev_eval_function_docs/compareConstructs/#warning","title":"WARNING","text":"<p>Currently, there is little to no security implemented in this function. User-submitted responses are evaluated using Python's <code>eval()</code> and <code>exec()</code> functions, which provide  no isolation whatsoever. This means that user code can trivially run any command on the  server, potentially stealing secrets and taking advantage of server resources.</p> <p>It is VERY IMPORTANT that this situation is resolved before releasing this to students. More information and some potential solutions are given here.</p>"},{"location":"dev_eval_function_docs/compareConstructs/#current-state","title":"Current State","text":"<p>Currently, compareConstructs provides the following checking methods, which are each described in detail in their own documents:</p> <ul> <li>Syntax and style (<code>checks/general_check.py</code>)</li> <li>Console output (<code>checks/output_check.py</code>)</li> <li>Variable content (<code>checks/global_variable_check.py</code>)</li> <li>Functions (<code>checks/func_check.py</code>)</li> </ul>"},{"location":"dev_eval_function_docs/compareConstructs/#architecture-overview","title":"Architecture Overview","text":"<p>By convention, all Lambda Feedback evaluation functions written in Python have a function called <code>evaluation_function(answer, response, params)</code>. In this case, <code>response</code> is the code entered by the student, <code>answer</code> is the answer written by the teacher, and <code>params</code> are  parameters configured for this particular question. This is found in <code>evaluation.py</code>. The return value contains the feedback to give to the student through the web client.</p> <p>In compareConstructs, this function calls <code>run_checks()</code>, which runs the above checks sequentially. Exactly which checks are run depends on  the <code>param</code> given. If none of these checks are run, a request is made to the OpenAI API to mark the response, with all the possibility of error this implies.</p>"},{"location":"dev_eval_function_docs/compareConstructs/#future-improvements","title":"Future Improvements","text":"<p>The scope of this project is very open ended, so there is a lot of room to add features and make improvements. Some suggestions are listed in future_improvements.md.</p>"},{"location":"dev_eval_function_docs/compareConstructs/#contact","title":"Contact","text":"<p>compareConstructs was started in the summer of 2024 by Jieyu Zhao, with significant contributions from Max Hurlow, who also wrote the documentation. If you have any questions about the project, feel free to contact me (Max). I'm sure you can figure out how.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/","title":"Security","text":"<p>Edit on GitHub  View Code </p> <p>Currently, there is no isolation of response execution. This has potentially major security implications, as any code submitted by users will run directly on the server and have all the  capabilities that the host eval function does (including the use of <code>sudo</code>).</p> <p>This should probably be the first thing that any new developers on the project consider, before adding any new features.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/#solutions","title":"Solutions","text":"<p>There are many potential ways to solve this issue. Some of these considered by the authors  are given here. Note that any other solution would be acceptable, as long as it effectively prevents user code from gaining privileged access to the host system.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/#linux-process-isolation","title":"Linux process isolation","text":"<p>The Linux operating system provides ways to reduce the permissions of processes and remove access to system resources. These include - <code>chroot</code>: this changes the root directory of the filesystem, preventing access to any other   files. - User and group permissions: a process can be made to run under an unprivileged user and group,   which prevents the use of <code>sudo</code> and any privileged syscalls. - <code>seccomp</code>: This allows granular control of which syscalls the process is allowed to use.</p> <p>The author made an attempt to implement this, and while it had the potential to work well, permissions issues on AWS Lambda meant that it was not viable. It is possible that with some more research this could be made to work, so the code for this can be found in <code>checks/run_checks.py</code>, and is gated behind a command-line option.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/#containers","title":"Containers","text":"<p>Running the user's code in a container would essentially be a pre-packaged implementation of the  above. For a developer with experience using Docker or other containerisation projects, this may be an attractive option, but it is currently unknown whether the aforementioned permissions issues would allow this.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/#emulation","title":"Emulation","text":"<p>Running a Python interpreter in a fully emulated virtual machine would provide the most effective  isolation possible between the user's code and the host machine. A promising possibility for achieving this is Pyodide, which is a Python interpreter runnning on the WebAssembly (Wasm) virtual machine.</p> <p>While Wasm is primarily intended to run in web browsers, it is also gaining popularity for  server-side use. Using this for compareConstructs would involve making use of Node.js (or  any other framework that provides a Wasm interpreter, like wasmtime) to run Pyodide, which would then interpret the user's response.</p> <p>The advantage of this is complete isolation without needing any special permissions, as everything is handled without operating system intervention. The possible downsides include reduced performance and added complexity.</p>"},{"location":"dev_eval_function_docs/compareConstructs/security/#static-analysis","title":"Static analysis","text":"<p>It may be possible to analyse the code to determine whether it would do anything malicious.</p> <p>It is quite straightforward to restrict access to <code>import</code>s and certain built-in functions when using <code>eval()</code>, but as this blog post describes, there are several methods to to sidestep these restrictions and gain access to dangerous functions like <code>os.system()</code> anyway.</p> <p>However, it appears that all such methods involve poking around with Python's \"magic\" attributes, which all have the form <code>__&lt;whatever&gt;__</code>. It may be possible to prevent this by analysing  the AST to find any use of these attributes, which are unlikely to be needed in \"normal\" code. It would also be necessary to disallow the use of nested <code>eval()</code>, which would let you obfuscate your access to these methods (e.g. <code>eval(\"_\" + \"_import_\" + \"_\")</code>).</p> <p>It is unknown whether this would provide sufficient protection against malicious code. It may be  best to use this in conjunction with another isolation method. It is also highly Python-specific, so it would not be easy to abstract this to allow for the use of other languages.</p>"},{"location":"dev_eval_function_docs/compareConstructs/syntax_and_style/","title":"Syntax and Style","text":"<p>Edit on GitHub  View Code </p>"},{"location":"dev_eval_function_docs/compareConstructs/syntax_and_style/#motivation","title":"Motivation","text":"<p>Before any dynamic analysis can take place, it must be verified that the student's response is syntactically correct. This check ensures that this is the case, and supplies a suitable error message in the case of a syntax error.</p> <p>Additionally, it is important to stick to a consistent, widely-used code style when writing \"real\" code. compareConstructs aims to encourage this by providing feedback when improvements could be made. At this time, only indentation is checked, but we hope that other lints will be implemented in the future.</p>"},{"location":"dev_eval_function_docs/compareConstructs/syntax_and_style/#implementation","title":"Implementation","text":"<p>Syntax is checked by using Python's <code>ast</code> module to parse the source into an abstract syntax tree. This process cannot complete if there are syntax errors, so if this is the case a feedback message is returned and the response is marked incorrect. The resultant AST is returned by the function so it can be reused by later checks.</p> <p>The source code is also processed to ensure a consistent indentation style of either two or four spaces is used throughout the response.</p> <p>This check is implemented in <code>checks/general_check.py</code>.</p>"},{"location":"dev_eval_function_docs/compareConstructs/syntax_and_style/#examples","title":"Examples","text":"<pre><code>print(\"Hello, World! \" # This is incorrect as there is no closing bracket.\n\nfor i in range(0, 10) # There should be a colon at the end of the 'for' statement.\n  print(i)\n</code></pre>"},{"location":"dev_eval_function_docs/compareConstructs/variable_content/","title":"Variable Content","text":"<p>Edit on GitHub  View Code </p>"},{"location":"dev_eval_function_docs/compareConstructs/variable_content/#motivation","title":"Motivation","text":"<p>While the function checking approach may be preferable for testing algorithms, depending on the question it may be more natural to check values assigned to variables. compareConstructs provides a capability for doing this.</p>"},{"location":"dev_eval_function_docs/compareConstructs/variable_content/#implementation","title":"Implementation","text":"<p>The <code>global_variables_check_list</code> parameter contains a list of variables to check. Depending on preference, this can be in the form of a list of strings, each string containing a variable name, or a single comma-delimited string, with variable names separated by commas.</p> <p>The response and answer will both be evaluated, and after evaluation is complete the values in the variables in the list will be  compared. If any required variables do not exist, or if any have different values to the answer, the response will be marked incorrect.</p> <p>If the variable stores float or a complex number, a tolerance will be applied to account for floating point imprecision.</p> <p>It is possible that the student's response assigns the correct values, but does so to variables with incorrect names. The checking function can detect this, and still return a correct result. This facility is experimental however, and should not be relied upon. It is recommended that any future developers consider improving this feature, and possibly gate it behind a parameter.</p> <p>This checking function is implemented in <code>checks/global_variable_check.py</code>.</p>"},{"location":"dev_eval_function_docs/compareConstructs/variable_content/#example","title":"Example","text":"<p>The response in this case will be marked correct.</p> <p>Response: <pre><code>test1 = []\nfor i in range(10)\n    test1.append(i)\n\ntest2 = 10 + 32\n</code></pre> Answer: <pre><code>test1 = [i for i in range(10)]\ntest2 = 42\n</code></pre> Params: <pre><code>{\n    \"global_variables_check_list\": [\"test1\", \"test2\"]\n}\n</code></pre></p>"},{"location":"dev_eval_function_docs/compareConstructs/functions/","title":"Function Check","text":"<p>Edit on GitHub  View Code </p>"},{"location":"dev_eval_function_docs/compareConstructs/functions/#motivation","title":"Motivation","text":"<p>This function, implemented in <code>checks/func_check.py</code>, subjects a function written by a student to a list of  tests provided by the teacher. This ensures that the implementation of the function is correct for any valid arguments, and because the students do not know what these tests will be, they cannot \"cheat\" or write  over-specialised code.</p>"},{"location":"dev_eval_function_docs/compareConstructs/functions/#implementation","title":"Implementation","text":"<p>When this check is used (if the <code>check_func</code> field is set in the parameters), the answer  provided by the teacher is interpreted differently. It must contain at least two items: - A function with the same name as the <code>check_func</code> parameter, which provides a \"reference implementation\"   that all the tests are run against. - A variable called <code>tests</code>, which is a list of tuples. Each tuple represents an individual test case, and the   values in the tuple are the arguments that will be passed to the function for each case. If the function takes   only one argument, it is not necessary to wrap it in a tuple.</p> <p>For each test case in the <code>tests</code> variable, the student's response function and the reference function will both be evaluated with the arguments for that case. Their return values will then be compared.</p> <p>By default, this comparison is done using the <code>==</code> operator, and there is a built-in special case for NumPy arrays. If a more advanced comparison is required, the answer can include a function called <code>equals</code>. This function takes two arguments and returns a boolean indicating if its arguments were considered equal. If this function is present, it is always used instead of the built-in comparisons.</p> <p>If the output of the two functions is equal for each test case, the student's response will be marked correct.</p>"},{"location":"dev_eval_function_docs/compareConstructs/functions/#example","title":"Example","text":"<p>An example question might test a trivial function called <code>sum</code> that simply adds together its two arguments.  The answer for this question might be the following: <pre><code>def sum(a, b):\n    return a + b\n\ntests = [\n    (0, 0),\n    (1, 0),\n    (100, 165),\n    (730, 21),\n    #etc...\n]\n</code></pre> The student's response will be evaluated with arguments of (0, 0), (1, 0), (100, 165) etc., and if any value it returns differs from the value returned by the reference function, it will be marked incorrect.</p>"},{"location":"dev_eval_function_docs/compareConstructs/functions/#advanced","title":"Advanced","text":"<p>Since <code>tests</code> is a normal Python variable, it need not be declared statically, so test cases can be dynamically  generated. An example where you may want to do this would be to generate a large number of random tests with problem-specific constraints. Arbitrary code can be used to generate the tests, and any built-in Python library can be used, as well as some others like NumPy.</p> <p>The following example creates 1000 random tests to ensure that the <code>sum</code> function works as intended.</p> <p>Answer: <pre><code>from random import randint\n\ndef sum(a, b):\n    return a + b\n\ntests = [(randint(0, 10), randint(0, 10)) for _ in range(1000)]\n</code></pre></p>"},{"location":"dev_eval_function_docs/compareConstructs/parameters/","title":"Parameters","text":"<p>Edit on GitHub  View Code </p> <p>Parameters are communicated from the web application to the evaluation function in JSON format, which are passed to <code>evaluation_function()</code> as a dictionary.</p> <p>This dictionary has several fields:</p> <ul> <li><code>global_variables_check_list</code>: This is either a list of strings, or a comma-delimited string   containing a list of variable names to check. If this list is present, a   global variable check is performed, and the given variables are compared   between the response and the answer.</li> <li><code>check_func</code>: This is a string containing the name of a function. If this is present,   a function check is performed on this function</li> <li><code>check_names</code>: A boolean. If this is true, all functions and classes in the response must   match with the answer exactly.</li> <li><code>output_inexact</code>: A boolean. If this is true, a tolerance is applied to values printed to the   console. This is useful for comparing numbers, where for example <code>0</code>, <code>0.0</code> and <code>0.000000005</code>   should be considered the same due to floating point precision. Otherwise, string comparisons   are used.</li> </ul> <p>This parameter format is simple, but flawed. It is not possible to specify exactly which checks are run. For example, currently, if <code>check_func</code> is set, the specified function will be tested but the global variables will not be checked, even if <code>global_variables_check_list</code> is given. This is a clear area for improvement.</p>"},{"location":"dev_eval_function_docs/compareConstructs/future_improvements/","title":"Future Improvements","text":"<p>Edit on GitHub  View Code </p> <p>This project was started with an open-ended brief, so there are many ways that it could be expanded and built upon. Some potential avenues to explore are given below. Note that these are just suggestions, so any relevant features  would be welcome.</p> <ul> <li>Security</li> <li>This has been mentioned before, but security is currently very poor or nonexistant.</li> <li>There are many possible ways this could be improved.</li> <li>A difficult problem with lots of room for experimentation and innovation.</li> <li>Checking</li> <li>The current checks perform well in some cases, but are sometimes unreliable.<ul> <li>More unit tests are needed to prevent incorrect feedback.</li> </ul> </li> <li>Potential improvements to checking functions:<ul> <li>Check that a consistent naming convention is used for variable and function names   (camelCase, snake_case etc.).</li> <li>Currently only one function can be checked at once. Add the ability to check   an arbitrary number of functions in the response (harder than it seems because of    the current method used; see functions.md).</li> <li>Add the ability to check class methods as well as bare functions.</li> <li>Performance checking:</li> <li>Currently all algorithms that yield a correct result will be treated equally.</li> <li>compareConstructs could check whether an optimal algorithm was used to produce the     result, and give feedback if it wasn't.</li> <li>Example: a question asks to write a function that sorts a list. compareConstructs     could check how the execution time scales with the size of the list over many evaluations,     and use statistical methods to determine its time complexity.</li> <li>Useful for teaching algorithms. If the question specifically specifies a particular algorithm,     there is currently no way to ensure this is being used. For example, a student could write a      bubble sort when a quicksort was required, and still be marked correct.</li> </ul> </li> <li>Feedback</li> <li>The feedback given to students is quite bare-bones in some cases.</li> <li>It would be beneficial if more helpful advice was given for incorrect responses, or if      areas for improvement were identified for correct responses.</li> <li>Architecture</li> <li>The current architecture applies checks sequentially, which results in dependencies: if     one check fails, the others will not be run, leading to misleading feedback.</li> <li>Checks should be made more independent of each other, which could potentially allow     parallelisation.</li> <li>The current parameters system doesn't provide very good control over     exactly which checks are run, and it is unclear to users exactly what is being checked.     This should be replaced by a more robust system.</li> <li>Lambda Feedback currently doesn't provide a capability for separate response areas to share     information. If this is ever implemented, it may be useful to allow one code area to \"extend\"     another, to reduce the amount of copying and pasting for incremental questions.</li> <li>Lambda Feedback doesn't allow evaluation functions to store any data between invocations.     If this restriction is lifted, it would be possible to provide feedback depending on the number     of consecutive wrong answers, for example to provide more hints the longer a student has been     stuck.</li> </ul>"},{"location":"user_eval_function_docs/compareConstructs/","title":"compareConstructs","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p> <p>compareConstructs is an evaluation function that checks and verifies Python code against a given correct answer.</p> <p>It is currently HIGHLY EXPERIMENTAL and should not be used in any non-testing module.</p>"},{"location":"user_eval_function_docs/compareConstructs/#capabilities","title":"Capabilities","text":"<p>Currently, compareConstructs provides the following checking methods, which are each described in detail in their own documents:</p> <ul> <li>Syntax and style</li> <li>Console output</li> <li>Variable content</li> <li>Functions</li> </ul>"},{"location":"user_eval_function_docs/compareConstructs/#parameters","title":"Parameters","text":"<p>The parameters that the function accepts are described here.</p>"},{"location":"user_eval_function_docs/compareConstructs/syntax_and_style/","title":"Syntax and Style","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p>"},{"location":"user_eval_function_docs/compareConstructs/syntax_and_style/#motivation","title":"Motivation","text":"<p>Before any dynamic analysis can take place, it must be verified that the student's response is syntactically correct. This check ensures that this is the case, and supplies a suitable error message in the case of a syntax error.</p> <p>Additionally, it is important to stick to a consistent, widely-used code style when writing \"real\" code. compareConstructs aims to encourage this by providing feedback when improvements could be made. At this time, only indentation is checked, but we hope that other lints will be implemented in the future.</p>"},{"location":"user_eval_function_docs/compareConstructs/syntax_and_style/#implementation","title":"Implementation","text":"<p>Syntax is checked by using Python's <code>ast</code> module to parse the source into an abstract syntax tree. This process cannot complete if there are syntax errors, so if this is the case a feedback message is returned and the response is marked incorrect. The resultant AST is returned by the function so it can be reused by later checks.</p> <p>The source code is also processed to ensure a consistent indentation style of either two or four spaces is used throughout the response.</p> <p>This check is implemented in <code>checks/general_check.py</code>.</p>"},{"location":"user_eval_function_docs/compareConstructs/syntax_and_style/#examples","title":"Examples","text":"<pre><code>print(\"Hello, World! \" # This is incorrect as there is no closing bracket.\n\nfor i in range(0, 10) # There should be a colon at the end of the 'for' statement.\n  print(i)\n</code></pre>"},{"location":"user_eval_function_docs/compareConstructs/variable_content/","title":"Variable Content","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p>"},{"location":"user_eval_function_docs/compareConstructs/variable_content/#motivation","title":"Motivation","text":"<p>While the function checking approach may be preferable for testing algorithms, depending on the question it may be more natural to check values assigned to variables. compareConstructs provides a capability for doing this.</p>"},{"location":"user_eval_function_docs/compareConstructs/variable_content/#implementation","title":"Implementation","text":"<p>The <code>global_variables_check_list</code> parameter contains a list of variables to check. Depending on preference, this can be in the form of a list of strings, each string containing a variable name, or a single comma-delimited string, with variable names separated by commas.</p> <p>The response and answer will both be evaluated, and after evaluation is complete the values in the variables in the list will be  compared. If any required variables do not exist, or if any have different values to the answer, the response will be marked incorrect.</p> <p>If the variable stores float or a complex number, a tolerance will be applied to account for floating point imprecision.</p> <p>It is possible that the student's response assigns the correct values, but does so to variables with incorrect names. The checking function can detect this, and still return a correct result. This facility is experimental however, and should not be relied upon. It is recommended that any future developers consider improving this feature, and possibly gate it behind a parameter.</p> <p>This checking function is implemented in <code>checks/global_variable_check.py</code>.</p>"},{"location":"user_eval_function_docs/compareConstructs/variable_content/#example","title":"Example","text":"<p>The response in this case will be marked correct.</p> <p>Response: <pre><code>test1 = []\nfor i in range(10)\n    test1.append(i)\n\ntest2 = 10 + 32\n</code></pre> Answer: <pre><code>test1 = [i for i in range(10)]\ntest2 = 42\n</code></pre> Params: <pre><code>{\n    \"global_variables_check_list\": [\"test1\", \"test2\"]\n}\n</code></pre></p>"},{"location":"user_eval_function_docs/compareConstructs/functions/","title":"Function Check","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p>"},{"location":"user_eval_function_docs/compareConstructs/functions/#motivation","title":"Motivation","text":"<p>This function, implemented in <code>checks/func_check.py</code>, subjects a function written by a student to a list of  tests provided by the teacher. This ensures that the implementation of the function is correct for any valid arguments, and because the students do not know what these tests will be, they cannot \"cheat\" or write  over-specialised code.</p>"},{"location":"user_eval_function_docs/compareConstructs/functions/#implementation","title":"Implementation","text":"<p>When this check is used (if the <code>check_func</code> field is set in the parameters), the answer  provided by the teacher is interpreted differently. It must contain at least two items: - A function with the same name as the <code>check_func</code> parameter, which provides a \"reference implementation\"   that all the tests are run against. - A variable called <code>tests</code>, which is a list of tuples. Each tuple represents an individual test case, and the   values in the tuple are the arguments that will be passed to the function for each case. If the function takes   only one argument, it is not necessary to wrap it in a tuple.</p> <p>For each test case in the <code>tests</code> variable, the student's response function and the reference function will both be evaluated with the arguments for that case. Their return values will then be compared.</p> <p>By default, this comparison is done using the <code>==</code> operator, and there is a built-in special case for NumPy arrays. If a more advanced comparison is required, the answer can include a function called <code>equals</code>. This function takes two arguments and returns a boolean indicating if its arguments were considered equal. If this function is present, it is always used instead of the built-in comparisons.</p> <p>If the output of the two functions is equal for each test case, the student's response will be marked correct.</p>"},{"location":"user_eval_function_docs/compareConstructs/functions/#example","title":"Example","text":"<p>An example question might test a trivial function called <code>sum</code> that simply adds together its two arguments.  The answer for this question might be the following: <pre><code>def sum(a, b):\n    return a + b\n\ntests = [\n    (0, 0),\n    (1, 0),\n    (100, 165),\n    (730, 21),\n    #etc...\n]\n</code></pre> The student's response will be evaluated with arguments of (0, 0), (1, 0), (100, 165) etc., and if any value it returns differs from the value returned by the reference function, it will be marked incorrect.</p>"},{"location":"user_eval_function_docs/compareConstructs/functions/#advanced","title":"Advanced","text":"<p>Since <code>tests</code> is a normal Python variable, it need not be declared statically, so test cases can be dynamically  generated. An example where you may want to do this would be to generate a large number of random tests with problem-specific constraints. Arbitrary code can be used to generate the tests, and any built-in Python library can be used, as well as some others like NumPy.</p> <p>The following example creates 1000 random tests to ensure that the <code>sum</code> function works as intended.</p> <p>Answer: <pre><code>from random import randint\n\ndef sum(a, b):\n    return a + b\n\ntests = [(randint(0, 10), randint(0, 10)) for _ in range(1000)]\n</code></pre></p>"},{"location":"user_eval_function_docs/compareConstructs/parameters/","title":"Parameters","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p> <p>Parameters are communicated from the web application to the evaluation function in JSON format, which are passed to <code>evaluation_function()</code> as a dictionary.</p> <p>This dictionary has several fields:</p> <ul> <li><code>global_variables_check_list</code>: This is either a list of strings, or a comma-delimited string   containing a list of variable names to check. If this list is present, a   global variable check is performed, and the given variables are compared   between the response and the answer.</li> <li><code>check_func</code>: This is a string containing the name of a function. If this is present,   a function check is performed on this function</li> <li><code>check_names</code>: A boolean. If this is true, all functions and classes in the response must   match with the answer exactly.</li> <li><code>output_inexact</code>: A boolean. If this is true, a tolerance is applied to values printed to the   console. This is useful for comparing numbers, where for example <code>0</code>, <code>0.0</code> and <code>0.000000005</code>   should be considered the same due to floating point precision. Otherwise, string comparisons   are used.</li> </ul> <p>This parameter format is simple, but flawed. It is not possible to specify exactly which checks are run. For example, currently, if <code>check_func</code> is set, the specified function will be tested but the global variables will not be checked, even if <code>global_variables_check_list</code> is given. This is a clear area for improvement.</p>"},{"location":"dev_eval_function_docs/isExactEqual/","title":"IsExactEqual","text":"<p>Edit on GitHub  View Code </p> <p>Could be qualified as the simplest form of evaluation function, testing exact equality. This function will use the default python <code>==</code> test to compare answer and responses. It doesn't infer any types - meaning it requires a <code>params.type</code> to be supplied.</p>"},{"location":"dev_eval_function_docs/isExactEqual/#inputs","title":"Inputs","text":"<p>This function requires a parameter to function properly: <pre><code>{ \n  \"params\": {\n    \"type\": \"&lt;string&gt;\" (any of [\"int\", \"float\", \"str\", \"dict\"])\n  }\n  \"response\": &lt;&gt;,\n  \"answer\": &lt;&gt;\n}\n</code></pre></p> <p>When a student submits a response to a response area the number of previously submitted responses submitted to the same response area byt the same student will be sent to the evaluation function. The following format is used: <pre><code>{\n    \"submission_context\": {\n        \"submissions_per_student_per_response_area\": # non-negative integer that represent the number of previously processed responses\n    }\n}\n</code></pre></p> <p>The total number of submitted responses (i.e. the number of processed response + 1) can be displayed in the feedback by setting adding a field named <code>display_submission_count</code> to <code>params</code> and set its value to true.</p>"},{"location":"dev_eval_function_docs/isExactEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>grade</code> command will feature:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isExactEqual/#examples","title":"Examples","text":""},{"location":"dev_eval_function_docs/isExactEqual/#simple-string-comparison","title":"Simple String Comparison","text":"<pre><code>{\n  \"answer\": \"hydrophobic\",\n  \"response\": \"hydrophobic\",\n  \"params\": {\n    \"type\": \"str\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isExactEqual/#displaying-number-of-submitted-responses-in-the-feedback","title":"Displaying number of submitted responses in the feedback","text":"<pre><code>{\n  \"answer\": 1,\n  \"response\": 1,\n  \"params\": {\n    \"type\": \"int\",\n    \"display_submission_count\": true\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/isExactEqual/","title":"IsExactEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>NUMBER</code></li> <li><code>BOOLEAN</code></li> <li><code>TEXT</code></li> </ul> <p>Use this function to check the exact equality between the student response and answer. This function requires a 'type' parameter which specifies how each of the two inputs should be cast before direct comparison in python.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/","title":"ShortTextAnswer","text":"<p>Edit on GitHub  View Code </p> <p>This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#inputs","title":"Inputs","text":"<p><code>keystrings</code> - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#keystring-object","title":"<code>keystring</code> object","text":"<p>The <code>keystring</code> object contains several fields which affect how it will be interpreted:</p> <ul> <li><code>string</code> - Required. The actual keystring being searched for.</li> <li><code>exact_match</code> - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to <code>false</code></li> <li><code>should_contain</code> - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to <code>true</code>. Setting this flag to false indicates that a correct response will not contain the specified keystring.</li> <li><code>custom_feedback</code> - Optional. A feedback string to be returned if the <code>string</code> was not found (or if it was, in case <code>should_contain</code> was set to <code>false</code>). Defaults to <code>None</code>, in which case a generic response will be generated containing the string searched for.</li> </ul>"},{"location":"dev_eval_function_docs/shortTextAnswer/#outputs","title":"Outputs","text":"<p>The function will return an object with 3 fields of interest. the <code>is_correct</code> and <code>feedback</code> fields are required by LambdaFeedback to present feedback to the user. The <code>result</code> field is only used for development. <pre><code>{\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"result\": {\n        \"response\": \"&lt;string&gt;\",\n        \"processing_time\": \"&lt;double&gt;\",\n    },\n    \"feedback\": \"string\"\n}\n</code></pre></p> <ul> <li><code>response</code> - The student answer. USed for debugging purposes.</li> <li><code>processing_time</code> - The time it took for the function to evaluate</li> </ul> <p>If the function identified a problematic keystring, the result object will have an additional field: * <code>keystring-scores</code> - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer.</p> <p>Otherwise, it will have the additional fields: * <code>method</code> - string. Either \"w2v\" or \"BOW vector similarity\". * <code>similarity_value</code> - double. The similarity value between the response and the answer.</p> <p>If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#initial-setup","title":"Initial SetUp","text":"<p>Follow Docker Image instructions and run  <code>docker build -t &lt;image_name&gt; .</code> in app/</p> <p>Otherwise if setup locally: 1. create a venv 2. in the venv <code>pip install -r app/requirements.txt</code> 3. if errors encountered with nltk packages, follow <code>testing_nltk.py</code> instructions</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","title":"Example simple input, no keystring","text":"<p>Input <pre><code>{\n    \"response\": \"Density, velocity, viscosity, length\",\n    \"answer\": \"Density, speed, Viscosity, Length\",\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': True, \n    'result': {\n        'response': 'Density, speed, Viscosity, Length',\n        'processing_time': 0.022912219000000178, \n        'method': 'w2v', \n        'similarity_value': 0.9326027035713196}, \n    'feedback': 'Confidence: 0.933%'\n}\n</code></pre></p>"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-keystring-input","title":"Example keystring input","text":"<p>Input <pre><code>{\n    \"response\": \"Molecules are made out of atoms\",\n    \"answer\": \"Many atoms form a molecule\",\n    'keystrings': [\n        {'string': 'molecule'}, \n        {'string': 'proton', 'exact_match': True}\n    ]\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': False, \n    'result': {\n        'response': 'Molecules are made out of atoms', \n        'processing_time': 0.30640586500000033, \n        'keystring-scores': [\n            ('molecule', 0.990715997949492), \n            ('proton', 0.9186190596675989) # Searched for with exact match, therefore not a match.\n        ]\n    }, \n    'feedback': \"Cannot determine if the answer is correct. Please provide more information about 'proton'\"}\n</code></pre></p>"},{"location":"user_eval_function_docs/shortTextAnswer/","title":"ShortTextAnswer","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> </ul> <p>This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#inputs","title":"Inputs","text":"<p><code>keystrings</code> - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#keystring-object","title":"<code>keystring</code> object","text":"<p>The <code>keystring</code> object contains several fields which affect how it will be interpreted:</p> <ul> <li><code>string</code> - Required. The actual keystring being searched for.</li> <li><code>exact_match</code> - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to <code>false</code></li> <li><code>should_contain</code> - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to <code>true</code>. Setting this flag to false indicates that a correct response will not contain the specified keystring.</li> <li><code>custom_feedback</code> - Optional. A feedback string to be returned if the <code>string</code> was not found (or if it was, in case <code>should_contain</code> was set to <code>false</code>). Defaults to <code>None</code>, in which case a generic response will be generated containing the string searched for.</li> </ul>"},{"location":"user_eval_function_docs/shortTextAnswer/#outputs","title":"Outputs","text":"<p>The function will return an object with 3 fields of interest. the <code>is_correct</code> and <code>feedback</code> fields are required by LambdaFeedback to present feedback to the user. The <code>result</code> field is only used for development. <pre><code>{\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"result\": {\n        \"response\": \"&lt;string&gt;\",\n        \"processing_time\": \"&lt;double&gt;\",\n    },\n    \"feedback\": \"string\"\n}\n</code></pre></p> <ul> <li><code>response</code> - The student answer. USed for debugging purposes.</li> <li><code>processing_time</code> - The time it took for the function to evaluate</li> </ul> <p>If the function identified a problematic keystring, the result object will have an additional field: * <code>keystring-scores</code> - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer.</p> <p>Otherwise, it will have the additional fields: * <code>method</code> - string. Either \"w2v\" or \"BOW vector similarity\". * <code>similarity_value</code> - double. The similarity value between the response and the answer.</p> <p>If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"user_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","title":"Example simple input, no keystring","text":"<p>Input <pre><code>{\n    \"response\": \"Density, velocity, viscosity, length\",\n    \"answer\": \"Density, speed, Viscosity, Length\",\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': True, \n    'result': {\n        'response': 'Density, speed, Viscosity, Length',\n        'processing_time': 0.022912219000000178, \n        'method': 'w2v', \n        'similarity_value': 0.9326027035713196}, \n    'feedback': 'Confidence: 0.933%'\n}\n</code></pre></p>"},{"location":"user_eval_function_docs/shortTextAnswer/#example-keystring-input","title":"Example keystring input","text":"<p>Input <pre><code>{\n    \"response\": \"Molecules are made out of atoms\",\n    \"answer\": \"Many atoms form a molecule\",\n    'keystrings': [\n        {'string': 'molecule'}, \n        {'string': 'proton', 'exact_match': True}\n    ]\n}\n</code></pre></p> <p>Output <pre><code>{\n    'is_correct': False, \n    'result': {\n        'response': 'Molecules are made out of atoms', \n        'processing_time': 0.30640586500000033, \n        'keystring-scores': [\n            ('molecule', 0.990715997949492), \n            ('proton', 0.9186190596675989) # Searched for with exact match, therefore not a match.\n        ]\n    }, \n    'feedback': \"Cannot determine if the answer is correct. Please provide more information about 'proton'\"}\n</code></pre></p>"},{"location":"dev_eval_function_docs/latexEqual/","title":"LatexEqual","text":"<p>Edit on GitHub  View Code </p> <p>This is an experimental evaluation function for comparing two latex strings. For evaluation, it actually calls the <code>symbolicEqual</code> function using the experimental EvaluationFunctionClient.</p>"},{"location":"dev_eval_function_docs/latexEqual/#inputs","title":"Inputs","text":"<p>This function doesn't have any parameters, both <code>response</code> and <code>answer</code> should be valid LaTeX strings. If you wish, you can pass parameters that are relayed to the <code>symbolicEqual</code> function (refer to it's documentation for more information about those).</p>"},{"location":"dev_eval_function_docs/latexEqual/#outputs","title":"Outputs","text":"<pre><code>{\n  \"is_correct\": \"&lt;bool&gt;\"\n}\n</code></pre>"},{"location":"user_eval_function_docs/latexEqual/","title":"LatexEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is not configured for any Response Area components</p> <p>Use this function to check if a given latex string response is mathematically equivalent to the answer. This is done by converting the latex inputs into mathematical expressions, which are then compared using the symbolicEqual function.</p>"},{"location":"dev_eval_function_docs/compareSets/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Brief description of what this evaluation function does, from the developer perspective</p>"},{"location":"dev_eval_function_docs/compareSets/#inputs","title":"Inputs","text":"<p>Specific input parameters which can be supplied when the <code>eval</code> command is supplied to this function.</p>"},{"location":"dev_eval_function_docs/compareSets/#outputs","title":"Outputs","text":"<p>Output schema/values for this function</p>"},{"location":"dev_eval_function_docs/compareSets/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/compareSets/#simple-evaluation","title":"Simple Evaluation","text":"<pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre> <pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/compareSets/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> </ul> <p>Teacher-facing documentation for this function.</p>"},{"location":"dev_eval_function_docs/isSimilar/","title":"IsSimilar","text":"<p>Edit on GitHub  View Code </p> <p>This simple evaluation function checks if the supplied response is within a tolerance range defined in <code>params</code>. Works exactly like the numpy.isclose function.</p> <p>Valid params include <code>atol</code> and <code>rtol</code>, which can be used in combination, or alone. As the comparison made is the following:</p> <pre><code>is_correct = abs(res - ans) &lt;= (atol + rtol*abs(ans))\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#inputs","title":"Inputs","text":"<pre><code>{\n  \"response\": \"&lt;number&gt;\",\n  \"answer\": \"&lt;number&gt;\",\n  \"params\": {\n    \"atol\": \"&lt;number&gt;\",\n    \"rtol\": \"&lt;number&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#atol","title":"<code>atol</code>","text":"<p>Absolute tolerance parameter</p>"},{"location":"dev_eval_function_docs/isSimilar/#rtol","title":"<code>rtol</code>","text":"<p>Relative tolerance parameter</p>"},{"location":"dev_eval_function_docs/isSimilar/#outputs","title":"Outputs","text":"<pre><code>{\n  \"is_correct\": \"&lt;bool&gt;\",\n  \"real_diff\": \"&lt;number&gt;\",\n  \"allowed_diff\": \"&lt;number&gt;\",\n}\n</code></pre>"},{"location":"dev_eval_function_docs/isSimilar/#real_diff","title":"<code>real_diff</code>","text":"<p>Real difference between the given answer and response</p>"},{"location":"dev_eval_function_docs/isSimilar/#allowed_diff","title":"<code>allowed_diff</code>","text":"<p>Allowed difference between answer and response, calculated using the supplied <code>atol</code> and <code>rtol</code> parameters</p>"},{"location":"dev_eval_function_docs/isSimilar/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/isSimilar/","title":"IsSimilar","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>NUMBER</code></li> <li><code>TEXT</code></li> </ul> <p>Use this evaluation function to check if the student's reponse is within a tolerance range defined in <code>params</code>. Works exactly like the numpy.isclose function. Valid params include <code>atol</code> and <code>rtol</code> (absolute and relative tolerances) which can be used in combination, or alone.</p> <p>Note: If the answer is not a number, all responses will generate an error.</p> <p>Note: If the response is not a number, a feedback message asking the user to submit a number will be returned.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/","title":"SymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Evaluates the equality between two symbolic expressions using the python <code>SymPy</code> package.</p> <p>Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#inputs","title":"Inputs","text":""},{"location":"dev_eval_function_docs/symbolicEqual/#optional-grading-parameters","title":"Optional grading parameters","text":"<p>There are eight optional parameters that can be set: <code>complexNumbers</code>, <code>elementary_functions</code>, <code>specialFunctions</code>, <code>strict_syntax</code>,  <code>symbol_assumptions</code>, <code>multiple_answers_criteria</code>, <code>plus_minus</code> and <code>minus_plus</code>.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"dev_eval_function_docs/symbolicEqual/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumption names can be found in this list:  <code>SymPy Assumption Predicates</code></p>"},{"location":"dev_eval_function_docs/symbolicEqual/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#outputs","title":"Outputs","text":"<p>Outputs to the <code>eval</code> command will feature:</p> <pre><code>{\n  \"command\": \"eval\",\n  \"result\": {\n    \"is_correct\": \"&lt;bool&gt;\",\n    \"response_latex\": \"&lt;str&gt;\",\n    \"response_simplified\": \"&lt;str&gt;\",\n    \"level\": \"&lt;int&gt;\"\n  }\n}\n</code></pre>"},{"location":"dev_eval_function_docs/symbolicEqual/#response_latex","title":"<code>response_latex</code>","text":"<p>This is a latex string, indicating how the user's <code>response</code> was understood by SymPy. It can be used to provide feedback in the front-end.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#level","title":"<code>level</code>","text":"<p>The function tests equality using three levels, of increasing complexity. This parameter indicates the level at which equality was found. It is not present if the result is incorrect.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#response_simplified","title":"<code>response_simplified</code>","text":"<p>This is a math-simplified string of the given response. All mathematically-equivalent expressions will yield identical strings under this field. This can be used by teacher dashboards when aggregating common student errors.</p>"},{"location":"dev_eval_function_docs/symbolicEqual/#examples","title":"Examples","text":""},{"location":"user_eval_function_docs/symbolicEqual/","title":"SymbolicEqual","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> </ul> <p>This function utilises the <code>SymPy</code> to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p> <p>Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equations as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\).</p>"},{"location":"user_eval_function_docs/symbolicEqual/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/symbolicEqual/#optional-grading-parameters","title":"Optional grading parameters","text":"<p>There are eight optional parameters that can be set: <code>complexNumbers</code>, <code>elementary_functions</code>, <code>specialFunctions</code>, <code>strict_syntax</code>,  <code>symbol_assumptions</code>, <code>multiple_answers_criteria</code>, <code>plus_minus</code> and <code>minus_plus</code>.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"user_eval_function_docs/symbolicEqual/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumption names can be found in this list:  <code>SymPy Assumption Predicates</code></p>"},{"location":"user_eval_function_docs/symbolicEqual/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/symbolicEqual/#1-setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","title":"1 Setting input symbols to be assumed positive to avoid issues with fractional powers","text":"<p>In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a &gt; 0\\) and \\(b &gt; 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\). The same is true for other fractional powers.</p> <p>So if expression like these are expected in the answer and/or response then it is a good idea to use the <code>symbol_assumptions</code> parameter to note that \\(a &gt; 0\\) and \\(b &gt; 0\\). This can be done by setting <code>symbol_assumptions</code> to <code>('a','positive') ('b','positive')</code>.</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>SymbolicEqual</code> with answer <code>sqrt(a/b)</code>, <code>strict_syntax</code> set to false and <code>symbol_assumptions</code> set as above. Some examples of expressions that are accepted as correct: <code>sqrt(a)/sqrt(b)</code>, <code>(a/b)**(1/2)</code>, <code>a**(1/2)/b**(1/2)</code>, <code>(a/b)^(0.5)</code>, <code>a^(0.5)/b^(0.5)</code></p>"},{"location":"dev_eval_function_docs/compareExpressions/","title":"YourFunctionName","text":"<p>Edit on GitHub  View Code </p> <p>Brief description of what this evaluation function does, from the developer perspective</p>"},{"location":"dev_eval_function_docs/compareExpressions/#inputs","title":"Inputs","text":"<p>Specific input parameters which can be supplied when the <code>eval</code> command is supplied to this function.</p>"},{"location":"dev_eval_function_docs/compareExpressions/#outputs","title":"Outputs","text":"<p>Output schema/values for this function</p>"},{"location":"dev_eval_function_docs/compareExpressions/#examples","title":"Examples","text":"<p>List of example inputs and outputs for this function, each under a different sub-heading</p>"},{"location":"dev_eval_function_docs/compareExpressions/#simple-evaluation","title":"Simple Evaluation","text":"<pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre> <pre><code>{\n  \"example\": {\n    \"Something\": \"something\"\n  }\n}\n</code></pre>"},{"location":"user_eval_function_docs/compareExpressions/","title":"CompareExpressions","text":"<p>Edit on GitHub  View Code </p> <p>Supported Response Area Types</p> <p>This evaluation function is supported by the following Response Area components:</p> <ul> <li><code>TEXT</code></li> <li><code>EXPRESSION</code></li> <li><code>NUMERIC_UNITS</code></li> <li><code>CODE</code></li> <li><code>ESSAY</code></li> </ul> <p>This function utilises the <code>SymPy</code> to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that <code>pi</code> is a reserved constant and cannot be used as a symbol name.</p> <p>Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equalities as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\).</p>"},{"location":"user_eval_function_docs/compareExpressions/#inputs","title":"Inputs","text":""},{"location":"user_eval_function_docs/compareExpressions/#optional-parameters","title":"Optional parameters","text":"<p>There are nine optional parameters that can be set: <code>complexNumbers</code>, <code>convention</code>, <code>criteria</code>, <code>multiple_answers_criteria</code>, <code>elementary_functions</code>, <code>feedback_for_incorrect_response</code>, <code>physical_quantity</code>, <code>plus_minus</code>/<code>minus_plus</code> <code>specialFunctions</code>, <code>strict_syntax</code>, <code>symbol_assumptions</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#complexnumbers","title":"<code>complexNumbers</code>","text":"<p>If you want to use <code>I</code> for the imaginary constant, set the grading parameter <code>complexNumbers</code> to True.</p>"},{"location":"user_eval_function_docs/compareExpressions/#convention","title":"<code>convention</code>","text":"<p>Changes the implicit multiplication convention. If unset it will default to <code>equal_precedence</code>.</p> <p>If set to <code>implicit_higher_precedence</code> then implicit multiplication will have higher precedence than explicit multiplication, i.e. <code>1/ab</code> will be equal to <code>1/(ab)</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p> <p>If set to <code>equal_precedence</code> then implicit multiplication will have the same precedence than explicit multiplication, i.e. both <code>1/ab</code> and <code>1/a*b</code> will be equal to <code>(1/a)*b</code>.</p>"},{"location":"user_eval_function_docs/compareExpressions/#criteria","title":"<code>criteria</code>","text":"<p>The <code>criteria</code> parameter can be used to customize the comparison performed by the evaluation function. If unset the evaluation function will will default to checking if the answer and response are symbolically equal.</p> <p>The <code>criteria</code> parameter takes a string that defines a set of (comma separated) mathematical statements. If all statements in the list are true the response is considered correct.</p> <p>The <code>criteria</code> parameter reserves <code>response</code> and <code>answer</code> as keywords that will be replaced y the response and answer respectively when the criteria is checked. Setting <code>criteria</code> to <code>answer=response</code> is gives the same behaviour as leaving <code>criteria</code> unset.</p> <p>Note: Currently the <code>criteria</code> parameter is ignored if <code>physical_quantity</code> is set to true.</p> <p>Note: The <code>criteria</code> parameters functionality is currently under development and will rarely produce appropriate feedback and can be quite difficult to debug.</p>"},{"location":"user_eval_function_docs/compareExpressions/#elementary_functions","title":"<code>elementary_functions</code>","text":"<p>When using implicit multiplication function names with multiple characters are sometimes split and not interpreted properly. Setting <code>elementary_functions</code> to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview.</p> <p><code>sin</code>, <code>sinc</code>, <code>csc</code> (alternative <code>cosec</code>), <code>cos</code>, <code>sec</code>, <code>tan</code>, <code>cot</code> (alternative <code>cotan</code>), <code>asin</code> (alternative <code>arcsin</code>), <code>acsc</code> (alternatives <code>arccsc</code>, <code>arccosec</code>), <code>acos</code> (alternative <code>arccos</code>), <code>asec</code> (alternative <code>arcsec</code>), <code>atan</code> (alternative <code>arctan</code>), <code>acot</code> (alternatives <code>arccot</code>, <code>arccotan</code>), <code>atan2</code> (alternative <code>arctan2</code>), <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>csch</code> (alternative <code>cosech</code>), <code>sech</code>, <code>asinh</code> (alternative <code>arcsinh</code>), <code>acosh</code> (alternative <code>arccosh</code>), <code>atanh</code> (alternative <code>arctanh</code>), <code>acsch</code> (alternatives <code>arccsch</code>, <code>arcosech</code>), <code>asech</code> (alternative <code>arcsech</code>), <code>exp</code> (alternative <code>Exp</code>), <code>E</code> (equivalent to <code>exp(1)</code>, alternative <code>e</code>), <code>log</code>, <code>sqrt</code>, <code>sign</code>, <code>Abs</code> (alternative <code>abs</code>), <code>Max</code> (alternative <code>max</code>), <code>Min</code> (alternative <code>min</code>), <code>arg</code>, <code>ceiling</code> (alternative <code>ceil</code>), <code>floor</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#feedback_for_incorrect_response","title":"<code>feedback_for_incorrect_response</code>","text":"<p>All feedback for all incorrect responses will be replaced with the string that this parameter is set to.</p>"},{"location":"user_eval_function_docs/compareExpressions/#multiple_answers_criteria","title":"<code>multiple_answers_criteria</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter <code>multiple_answers_criteria</code> controls this. The default setting, <code>all</code>, is that each answer must have a corresponding answer and vice versa. The setting <code>all_responses</code> check that all responses are valid answers and the setting <code>all_answers</code> checks that all answers are found among the responses.</p>"},{"location":"user_eval_function_docs/compareExpressions/#physical_quantity","title":"<code>physical_quantity</code>","text":"<p>If unset, <code>physical_quantity</code> will default to <code>false</code>.</p> <p>If <code>physical_quantity</code> is set to <code>true</code> the answer and response will interpreted as a physical quantity using units and conventions decided by the <code>strictness</code> and <code>units_string</code> parameters.</p> <p>Remark: Setting <code>physical_quantity</code> to <code>true</code> will also mean that comparisons will be done numerically. If neither the <code>atol</code> nor <code>rtol</code> parameters are set, the evaluation function will choose a relative error based on the number of sigificant digits given in the answer.</p> <p>When <code>physical_quantity</code> the evaluation function will generate feedback based on the flowchart below. Hovering over a criterion node will show a short natural language description of the criterion. Hovering over a result node will show the feedback produced so far.</p> <p>Remark: In some browser it is necessary to right-click and open the image in a separate tab in order for the tooltips to show up on hover.</p> <p></p>"},{"location":"user_eval_function_docs/compareExpressions/#strictness","title":"<code>strictness</code>","text":"<p>Constrols the conventions used when parsing physical quantities.</p> <p>Remark: If <code>physical_quantity</code> is set to <code>false</code>, this parameter will be ignored.</p> <p>There are three possible values: <code>strict</code>, <code>natural</code> and <code>legacy</code>. If <code>strict</code> is chosen then quantities will be parsed according to the conventions described in 5.1, 5.2, 5.3.2, 5.3.3 in https://www.bipm.org/documents/20126/41483022/si_brochure_8.pdf and 5.2, 5.3, 5.4.2 and 5.4.3 in https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf. If <code>natural</code> is chosen then less restrictive conventions are used.</p> <p>Remark: The default setting is <code>natural</code>.</p> <p>Remark: The <code>legacy</code> setting should not be used and is only there to allow compatibility with content designed for use with older versions of the evaluation function. If you encounter a question using the <code>legacy</code> setting is recommended that it is changed to another setting and the answer is redefined to match the chosen conventions.</p>"},{"location":"user_eval_function_docs/compareExpressions/#units_string","title":"<code>units_string</code>","text":"<p>Controls what sets of units are used. There are three values <code>SI</code>, <code>common</code> and <code>imperial</code>.</p> <p>If <code>SI</code> is chosen then only units from the tables <code>Base SI units</code> and <code>Derived SI units</code> (below) are allowed (in combinations with prefixes). If <code>common</code> is chosen then all the units allowed by <code>SI</code> as well as those listed in the tables for <code>Common non-SI units</code>. If <code>imperial</code> is chosen the base SI units and the units listed in the <code>Imperial units</code> table are allowed.</p> <p>Remark: The different settings can also be combine, e.g. <code>SI common imperial</code> will allow all units.</p>"},{"location":"user_eval_function_docs/compareExpressions/#notation-and-definition-of-units","title":"Notation and definition of units","text":""},{"location":"user_eval_function_docs/compareExpressions/#table-base-si-units","title":"Table: Base SI units","text":"<p>SI base units based on Table 2 in https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf</p> <p>Note that gram is used as a base unit instead of kilogram.</p> SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity"},{"location":"user_eval_function_docs/compareExpressions/#table-si-prefixes","title":"Table: SI prefixes","text":"<p>SI prefixes based on Table 7 in https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf</p> SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-derived-si-units","title":"Table: Derived SI units","text":"<p>Derived SI based on Table 4 in https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf</p> <p>Note that the function treats radians and steradians as dimensionless values.</p> Unit name Symbol Expressed in base SI units radian r \\((2\\pi)^{-1}\\) steradian sr \\((4\\pi)^{-1}\\) hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-common-non-si-units","title":"Table: Common non-SI units","text":"<p>Commonly used non-SI units based on Table 8 in https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf and Tables 7 and 8 in https://www.bipm.org/documents/20126/41483022/si_brochure_8.pdf Note that the function treats angles, neper and bel as dimensionless values.</p> <p>Note that only the first table in this section has short form symbols defined, the second table does not, this is done to minimize ambiguities when writing units.</p> Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{1}{360}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)"},{"location":"user_eval_function_docs/compareExpressions/#table-imperial-units","title":"Table: Imperial units","text":"<p>Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units</p> Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)"},{"location":"user_eval_function_docs/compareExpressions/#plus_minus-and-minus_plus","title":"<code>plus_minus</code> and <code>minus_plus</code>","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively.</p> <p>To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p>"},{"location":"user_eval_function_docs/compareExpressions/#specialfunctions","title":"<code>specialFunctions</code>","text":"<p>If you want to use the special functions <code>beta</code> (Euler Beta function), <code>gamma</code> (Gamma function) and <code>zeta</code> (Riemann Zeta function), set the grading parameter <code>specialFunctions</code> to True.</p>"},{"location":"user_eval_function_docs/compareExpressions/#strict_syntax","title":"<code>strict_syntax</code>","text":"<p>If <code>strict_syntax</code> is set to true then the answer and response must have <code>*</code> or <code>/</code> between each part of the expressions and exponentiation must be done using <code>**</code>, e.g. <code>10*x*y/z**2</code> is accepted but <code>10xy/z^2</code> is not.</p> <p>If <code>strict_syntax</code> is set to false, then <code>*</code> can be omitted and <code>^</code> used instead of <code>**</code>. In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols.</p> <p>By default <code>strict_syntax</code> is set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#strictness_1","title":"<code>strictness</code>","text":"<p>This parameter is only used when <code>physical_quantity</code> is set to <code>true</code>. It accepts three possible values: <code>strict</code>, <code>natural</code> and <code>legacy</code>. <code>strict</code>:</p>"},{"location":"user_eval_function_docs/compareExpressions/#symbol_assumptions","title":"<code>symbol_assumptions</code>","text":"<p>This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form <code>('symbol','assumption name')</code> and all pairs concatenated into a single string.</p> <p>The possible assumptions are: <code>constant</code>, <code>function</code> as well as those listed here:  <code>SymPy Assumption Predicates</code></p> <p>Note: Writing a symbol which denotes a function without its arguments, e.g. <code>T</code> instead of <code>T(x,t)</code>, is prone to cause errors.</p>"},{"location":"user_eval_function_docs/compareExpressions/#examples","title":"Examples","text":"<p>Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.</p>"},{"location":"user_eval_function_docs/compareExpressions/#1-setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","title":"1 Setting input symbols to be assumed positive to avoid issues with fractional powers","text":"<p>In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a &gt; 0\\) and \\(b &gt; 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\). The same is true for other fractional powers.</p> <p>So if expressions like these are expected in the answer and/or response then it is a good idea to use the <code>symbol_assumptions</code> parameter to note that \\(a &gt; 0\\) and \\(b &gt; 0\\). This can be done by setting <code>symbol_assumptions</code> to <code>('a','positive') ('b','positive')</code>.</p> <p>The example given in the example problem set uses two EXPRESSION response areas. Both response areas uses <code>compareExpression</code> with answer <code>sqrt(a/b)</code>, <code>strict_syntax</code> set to false, <code>elementary_functions</code> set to true. One response area leaves <code>symbol_assumptions</code> unset and the other sets the parameter as described in the previous paragraph. Some examples of expressions that are accepted as correct when positivity is assumed: <code>sqrt(a)/sqrt(b)</code>, <code>(a/b)**(1/2)</code>, <code>a**(1/2)/b**(1/2)</code>, <code>(a/b)^(0.5)</code>, <code>a^(0.5)/b^(0.5)</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#2-using-plusminus-symbols","title":"2 Using plus/minus symbols","text":"<p>The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in  the answer or response by <code>plus_minus</code> and <code>minus_plus</code> respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters <code>plus_minus</code> and <code>minus_plus</code> to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.</p> <p>It is considered good practice to make sure that the appropriate notation for \\(\\pm\\) and \\(\\mp\\) are added and displayed as input symbols in order to minimize confusion.</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>plus_minus x**2 + minus_plus y**2</code>, <code>strict_syntax</code> set to false and <code>elementary_function</code> set to true. Some examples of expressions that are accepted as correct: <code>plus_minus x**2 + minus_plus y**2</code>, <code>- minus_plus x**2 + minus_plus y**2</code>, <code>- minus_plus x^2 minus_plus y^2</code>, <code>- minus_plus x^2 - plus_minus y^2</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#3-equalities-in-the-answer-and-response","title":"3 Equalities in the answer and response","text":"<p>There is (limited) support for using equalities in the response and answer.</p> <p>The example given in the example problem set uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>x**2-5*y**2-7=0</code>. Some examples of expressions that are accepted as correct: <code>x**2-5*y**2-7=0</code>, <code>x^2 = 5y^2+7</code>, <code>2x^2 = 10y^2+14</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#4-checking-the-value-of-an-expression-or-a-physical-quantity","title":"4 Checking the value of an expression or a physical quantity","text":"<p>If the parameter <code>physical_quantity</code> is set to true, the evaluation function can handle expressions that describe physical quantities. Which units are permitted and how they should be written depends on the <code>units_string</code> and <code>strictness</code> parameters respectively.</p> <p>There are three examples in the example problem set. Each examples uses an EXPRESSION response area that uses <code>compareExpression</code> with answer <code>strict_syntax</code> set to false and <code>physical_quantity</code> set to true.</p>"},{"location":"user_eval_function_docs/compareExpressions/#example-a","title":"Example (a)","text":"<p>Here the answer is <code>2.00 km/h</code>. The parameters <code>strictness</code> and <code>units_string</code> are left unset which is equivalent to setting <code>strictness</code> to <code>natural</code>, and <code>units_string</code> to <code>SI common imperial</code>. Thus this response area accepts a wide range of responses, e.g. <code>2.00 kilometre/hour</code>, <code>2 km/h</code>, <code>2000 meter/hour</code>, <code>2 metre/millihour</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#example-b","title":"Example (b)","text":"<p>Here the answer is <code>2.00 km/h</code>. To restrict the answers to SI units <code>strictness</code> is set to <code>strict</code> and <code>units_string</code> is set to <code>SI</code>. Some examples of accepted responses are: <code>0.556 metre/second</code>, <code>5.56 dm/s</code>, <code>55.6 centimetre second^(-1)</code></p>"},{"location":"user_eval_function_docs/compareExpressions/#example-c","title":"Example (c)","text":"<p>Here the answer is <code>2.00 km/h</code>. To restrict the answers to imperial units <code>strictness</code> is set to <code>strict</code> and <code>units_string</code> is set to <code>imperial common</code>. Accepted response: <code>1.24 mile/hour</code></p>"}]}